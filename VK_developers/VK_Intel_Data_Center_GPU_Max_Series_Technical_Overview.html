<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Document</title>


    <!-- font family -->
    <link href="https://fonts.cdnfonts.com/css/intel-clear" rel="stylesheet">
    <!-- boootstap file -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/css/bootstrap.min.css" rel="stylesheet"
        integrity="sha384-EVSTQN3/azprG1Anm3QDgpJLIm9Nao0Yz1ztcQTwFspd3yD65VohhpuuCOmLASjC" crossorigin="anonymous">
    <!-- costom css file -->
    <link rel="stylesheet" href="/css/vivek.css">
    <link rel="stylesheet" href="/css/yatri.css">

    <!-- all.min file -->
    <link rel="stylesheet" href="/css/all.min.css">

</head>

<body>


    <main>

        <!-- header -->
        <header>
            <div id="navbar"></div>
        </header>


        <!-- poster -->
        <section class=" VK_light_blue">
            <div class="VK_cont">
                <div class="row m-0">
                    <div class="col text-white">
                        <h1 class="VK_py_pre_heading fw-light m-0 py-4 mt-2">
                            Intel® Data Center GPU Max Series Technical Overview
                        </h1>
                        <div
                            class="d-flex pb-4 col-md-4 col-sm-6 col-12 flex-md-row flex-column justify-content-between">
                            <div class="d-flex flex-md-column flex-row">
                                <p class="mb-2 me-3 w-100">
                                    ID
                                </p>
                                <p class="mb-2 me-3 w-100">
                                    766085
                                </p>
                            </div>
                            <div class="d-flex flex-md-column flex-row">
                                <p class="mb-2 me-3 w-100">
                                    Updated
                                </p>
                                <p class="mb-2 me-3 w-100">
                                    4/8/2024
                                </p>
                            </div>
                            <div class="d-flex flex-md-column flex-row">
                                <p class="mb-2 me-3 w-100">
                                    Version
                                </p>
                                <p class="mb-2 me-3 w-100">
                                    Current
                                </p>
                            </div>
                            <div class="d-flex flex-md-column flex-row">
                                <p class="mb-2 me-3 d-none d-md-block">
                                    &nbsp;
                                </p>
                                <p class="mb-2 me-3 w-100">
                                    Public
                                </p>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </section>


        <!--  -->
        <section class="">
            <div class="VK_cont py-5 VK_border_bottom">
                <div class="row m-0">
                    <div class="col-md-4 col-lg-3 p-0">

                    </div>
                    <div class="col-md-8 ps-sm-5">
                        <div class="mb-5">
                            <div class="VK_section_descriptions">
                                <div class="text-end">
                                    <p class="m-0 VK_print_email_font">
                                        <span class="VK_a mx-2 d-inline-block">
                                            <i class="fa-solid fa-print"></i>
                                        </span>
                                        <span class="VK_a mx-2 d-inline-block">
                                            <i class="fa-regular fa-envelope"></i>
                                        </span>
                                    </p>
                                </div>
                            </div>
                        </div>
                        <div class="mt-4">
                            <div class="d-flex justify-content-center">
                                <img src="/img/vivek/VK_206.png" class="w-100" alt="">
                            </div>
                            <p class="py-3">
                                This article discusses the capabilities available in the Intel® Data Center GPU Max
                                Series previously codename Ponte Vecchio and how developers can take advantage of them.
                                This new GPU product is based on the Intel Xe HPC micro-architecture. The GPU uses
                                highly parallelized computing models associated with AI and HPC. It is supported by the
                                oneAPI open ecosystem with the flexibility of Single Instruction Multiple Data (SIMD)
                                and Single Instruction Multiple Threads (SIMT).
                            </p>
                            <div class="d-flex justify-content-center">
                                <img src="/img/vivek/VK_207.png" class="w-100" alt="">
                            </div>
                            <p class="mt-3">
                                Figure 1 – High level Xe HPC Stack Component Overview
                            </p>
                            <p>
                                At the heart of the product is the Xe HPC Stack, which is comprised of various tiles
                                stacked upon each other within a single package. The components of the Xe HPC Stack are
                                comprised of the Xe-core Tile (Compute Tile), L2 Cache Tile, Base Tile (PCIe paths,
                                media engine, copy engine, etc.), High Memory Bandwidth Tile, Xe Link Tile for scale-up
                                and scale-out, and the Embedded Multi-Die Interconnect Bridge (EMIB) for communication
                                between Xe HPC Stacks. These components all reside within a Multi Tile Package.
                            </p>
                        </div>
                        <div class="mt-5">
                            <h3 class="VK_side_heading fw-light">
                                Intel X<sup>e</sup> HPC Micro-Architecture
                            </h3>
                            <p>
                                Over the course of this article, we are going to cover the different functional aspects
                                of the total solution. This includes a discussion of the architecture, multiple
                                execution units are combined into an individual core, multiple cores are combined into a
                                slice, multiple slices are combined into a stack and multiple stacks are incorporated
                                into different form factors to become different product lines.
                            </p>
                            <div class="d-flex justify-content-center">
                                <img src="/img/vivek/VK_208.png" class="w-100" alt="">
                            </div>
                            <p>
                                Figure 2 – Block Diagram Of The different components of the Intel Xe HPC
                                Micro-Architecture
                            </p>
                            <p>
                                Intel Xe is a scalable graphics and compute architecture designed to deliver exceptional
                                performance and functionality. The product includes a discrete PCIe add-in card and an
                                OpenCompute Accelerator Module (OAM) to provide solutions that require increased GPU
                                density within the data center. Intel’s Xe Architecture consists of one of three
                                possible micro-architectures: Xe LP a low-power solution, Xe HPG which is optimized for
                                enthusiasts and gaming, or Xe HPC which is optimized for HPC and AI acceleration. Intel®
                                Data Center GPU Max Series is built upon the Xe HPC micro-architecture with a compute
                                focused, programable and scalable element called the Xe-core. This Xe-core is made up of
                                a combination of Vector Engines, Matrix engines that are referred to in this article as
                                Intel® Xe Matrix Extensions (Intel® XMX), cache and shared local memory. Each Xe-core
                                contains eight 512-bit Vector Engines designed to accelerate traditional graphics,
                                compute, and HPC workloads. Along with eight 4096-bit Intel XMX, engineered to
                                accelerate AI workloads. The Xe-core provides 512KB of L1 cache and shared local memory
                                to support the engines.
                            </p>
                            <div class="d-flex justify-content-center">
                                <div class="col-md-10">
                                    <img src="/img/vivek/VK_209.png" class="w-100" alt="">
                                </div>
                            </div>
                            <p class="mt-4">
                                Figure 3 – Xe-core Block Diagram With Xe HPC Micro-Architecture
                            </p>
                            <p>
                                Developers can optimize their software for use with the Xe HPC micro-architecture by
                                using libraries and compilers that leverage the instruction set architecture (ISA). This
                                will take advantage of the ports previously referred to as the Data Parallel Matrix
                                Engine. Within one of the execution units of the Xe-core can be found three ports that
                                support a variety of programming data types to help maximize application performance.
                                The Xe-core has eight vector engines with a 512-bit wide register supporting all
                                “vector” or non-systolic numerics as follows, FP64 (256 ops per clock), FP32 (256 ops
                                per clock), and FP16 (512 ops per clock). While the Intel XMX systolic array depth has
                                been increased to 8 as compared to an Xe HPG Core providing 4096-bits for each of the
                                units. Intel XMX supports systolic numerics as follows, TF32 (2048 ops per clock), FP16
                                (4096 ops per clock), BF16 (4096 ops per clock) and Int8 (8192 ops per clock). New data
                                types introduced to support specific segments include a 64-bit double precision float
                                (FP64) to benefit HPC workloads utilizing the Vector Engines. A 32-bit tensor float
                                (TF32) data type is provided to benefit AI workloads using Intel XMX. The L1 data cache
                                and shared local memory have been increased to help support the additional flops
                                introduced by these new data types. The load/store unit has also been enhanced for
                                nd-tensor data structures.
                            </p>
                            <div class="d-flex justify-content-center">
                                <img src="/img/vivek/VK_210.png" class="w-100" alt="">
                            </div>
                            <p class="mt-4">
                                Figure 4 – Inside one of the execution units of the Xe-core on the Xe HPC
                                micro-architecture
                            </p>
                            <p>
                                The operations supplied to an execution unit of the Xe HPC micro-architecture will be
                                serviced by one of the three ports. Integer and floating-point operations are separated
                                out into two separate ports to increase throughput by running calculations parallel. The
                                port architecture allows AI applications, matrix operations, element-wise vector
                                operations and address computations to all be executed concurrently. If broader HPC
                                applications require higher precision, the dual-issued double precision pipes deliver
                                FP64 vector compute at 1x rate as compared to FP32.
                            </p>
                            <div class="d-flex justify-content-center">
                                <div class="col">
                                    <img src="/img/vivek/VK_211.png" class="w-100" alt="">
                                </div>
                            </div>
                            <p class="mt-3">
                                Figure 5 – Data Type Comparison
                            </p>
                            <p>
                                Deep Neural Networks have a high tolerance to numerical precisions. Comparing the
                                different data types, higher system performance with AI workloads can be achieved with
                                lower and mixed precision operations. The 32-bit floating-point format (FP32) is the
                                default for deep learning training and inference. When we look at what the other data
                                types offer as compared to FP32 we can see their advantages. The 32-bit tensor float
                                (TF32) data type, which can be enabled with a simple global setting, reduces the
                                precision (only 10 bits mantissa) with the same 8-bit exponent. Because of this the
                                throughput for TF32 is higher than FP32 just based on the amount of work done per clock
                                cycle. The 16-bit floating-point format (FP16) can do more work per clock cycle than
                                TF32 due to a 5-bit exponent, but it requires special handling techniques via loss
                                scaling. The
                                <a href="" class="Vk_text_underline_dots VK_a">
                                    Brain Floating Point Format
                                </a>
                                (BF16) is also a 16-bit floating point data type. It matches the dynamic range of FP32
                                and TF32 with an 8-bit exponent while having a lower precision than either, with only
                                7-bits of mantissa. It doesn’t suffer from the special handling required by FP16 yet is
                                able to match its throughput advantage making it ideal for training as compared to FP32,
                                FP16 or TF32. The 8-bit integer (INT8) data type can be used for post-training
                                quantization for faster inference. This is due to its smaller 8-bit size as compared to
                                FP32, FP16, BF16, or TF32 reducing both the memory and computing requirements. Even
                                though it is faster it lacks the precision of the other data types, which only makes it
                                useful for inference instead of training. Cloud edge devices involved with deep
                                learning, which have smaller memory and bandwidth requirements is an area that can
                                benefit from INT8.
                            </p>
                            <p>
                                Due to the general-purpose nature of the execution unit, it can process both Single
                                Instruction Multiple Data (SIMD) and Single Instruction Multiple Threads (SIMT).
                                Typically, SIMD is used by CPUs while SIMT is used by GPUs. The execution unit provides
                                parallelism for both the thread and data elements. Software developers can benefit from
                                the parallel nature of the data by extending the parallelism of the instructions.
                                Simultaneously executing both SIMT and SIMD provides a unique opportunity for potential
                                performance improvements.
                                <a href="" class="Vk_text_underline_dots VK_a">
                                    OneAPI
                                </a>
                                presents an opportunity for ease of development with this approach.
                            </p>
                        </div>
                        <div class="mt-5">
                            <h3 class="VK_side_heading fw-light">
                                X<sup>e</sup> HPC Slice
                            </h3>
                            <p>
                                Building up from the Xe-cores to create a functional SoC (System on a Chip)
                                infrastructure we start with the Xe HPC Slice. It is comprised of sixteen Xe-cores
                                combined with sixteen ray tracing units along with a hardware context. The ray tracing
                                units provide fixed-function computation for Ray Traversal Bounding Box Intersection,
                                and Triangle Intersection. The hardware context enables execution of multiple
                                applications concurrently without expensive software-based context switches.
                            </p>
                            <div class="d-flex justify-content-center">
                                <img src="/img/vivek/VK_212.png" class="w-100" alt="">
                            </div>
                            <p>
                                Figure 6 – Xe HPC Slice
                            </p>
                        </div>
                        <div class="mt-5">
                            <h3 class="VK_side_heading fw-light">
                                X<sup>e</sup> HPC Stack
                            </h3>
                            <p>
                                Four Xe HPC Slices are then combined with an L2 cache, one media engine, Gen 5 PCI
                                Express interconnect, a memory controller, and eight Xe Links to form an Xe HPC Stack.
                                The L2 cache can be up to a size of 204MB, the media engine performs decode only
                                functions for AVC, AVI and HEVC. In a two-stack configuration the communication occurs
                                via the Embedded Multi-Die Interconnect Bridge (EMIB) interface with up to 230 GB/s in
                                both directions. The Xe Links provide a high-speed coherent unified fabric for
                                communications between GPUs to support scale up and scale out.
                            </p>
                            <div class="d-flex justify-content-center">
                                <img src="/img/vivek/VK_213.png" class="w-100" alt="">
                            </div>
                            <p>
                                Figure 7 – Xe HPC Stack
                            </p>
                            <p>
                                From a perspective of form factor a single Xe HPC Stack can be used to create a PCIe
                                add-in card. While more than one Xe HPC Stack can be combined within a physical package
                                also known as an OpenCompute Accelerator Module (OAM). The OAM’s are placed onto a
                                carrier baseboard for scalability and the sled can be plugged into a server via PCI
                                Express.
                            </p>
                        </div>
                        <div class="mt-5">
                            <h3 class="VK_side_heading fw-light">
                                X<sup>e</sup> Link
                            </h3>
                            <p>
                                The Xe Link is designed to provide a high-speed coherent unified fabric for
                                communications between the Xe HPC Stacks. This includes both the internal communication
                                within an OAM as well as across multiple OAMs. It supports load and store operations,
                                bulk data transfers and synchronization of semantics. The technology is a scalability
                                feature that can provide communications for up to eight Xe HPC Stacks (x8 Glueless) via
                                an embedded switch. Each Xe Link is capable of up to 26.5 GB/s of bandwidth in each
                                direction.
                            </p>
                            <div class="d-flex justify-content-center">
                                <img src="/img/vivek/VK_214.png" class="w-100" alt="">
                            </div>
                            <p>
                                Figure 8 - Xe Link Configuration Options
                            </p>
                        </div>
                        <div class="mt-5">
                            <h3 class="VK_side_heading fw-light">
                                Scale Up
                            </h3>
                            <p>
                                The Xe Link is integral to scale up and scale out solutions. The expansion of the number
                                of Xe HPC Stacks instances within a single system is known as a scale up solution. This
                                can take on different configurations depending on whether PCI express cards are used or
                                OAMs. A PCI Express card has a single Xe HPC Stack and six Xe Links available for
                                scaling up. The supported PCI Express card configurations would be a two-card
                                configuration with six Xe Links between each Xe HPC Stack. The second supported
                                configuration would be four PCI Express cards with two Xe Links between each Xe HPC
                                Stack. With a bandwidth of 26.6 GB/s in each direction per Xe Link this means that in a
                                two-card configuration the bandwidth would be 159 GB/s in each direction and in a four
                                card configuration the bandwidth would be 53 GB/s in each direction. Figure 8 shows
                                varying levels of Xe Link connectivity that is possible with different amounts of PCI
                                Express cards and OAMs. Ease of use for developers on scale up solutions is provided by
                                the OS kernel. The kernel provides access to all the execution units without any needing
                                any code changes.
                            </p>
                        </div>
                        <div class="mt-5">
                            <h3 class="VK_side_heading fw-light">
                                Scale Out
                            </h3>
                            <p>
                                A scale out solution is when you need to connect multiple nodes together to create a
                                larger cluster of execution units. Up to a maximum of sixty-four interconnected OAMs can
                                be linked together using a single form of external communication between the nodes. The
                                inter-node communication can be a NIC-based solution using an InfiniBand fabric.
                                Alternatively, an Xe Link Glueless solution using directly connected passive copper
                                cables can be utilized to achieve linking together sixty-four OAMs. A need for scaling
                                out beyond sixty-four OAMs can be accomplished by combining Xe Link Glueless along with
                                a NIC-based InfiniBand fabric. This combination can provide up to 512 inter-connected
                                OAMs.
                            </p>
                            <p>
                                Intel’s initial offering will be a carrier base board that contains four OAMs to create
                                a half width sled. This configuration codenamed Tuscany can be plugged into an existing
                                server via the Gen 5 PCI Express bus. It provides a high-density solution for use with
                                HPC workloads. There will be options for a 450 watt and a 600 watt versions that can
                                support liquid or air cooled systems on single or dual processor systems. PCI Express
                                add in cards will be available in addition to the OAMs. The table below gives an
                                overview of the different product specifications.
                            </p>
                        </div>
                        <div class="mt-5">
                            <h3 class="VK_side_heading fw-light">
                                Micro-Architecture Overview
                            </h3>
                            <p>
                                <b>
                                    Table 1. Intel® Data Center GPU Max Series Micro-Architecture Overview
                                </b>
                            </p>
                            <div class="overflow-auto">
                                <table>
                                    <thead>
                                        <tr>
                                            <td class="VK_bg_e6 ps-2 py-2 pe-3 border-white border-2">
                                                &nbsp;
                                            </td>
                                            <td class="VK_bg_e6 ps-2 py-2 pe-3 border-white border-2" colspan="2">
                                                Specifications below are per OAM. Note that in the case of Intel’s
                                                product offering on a half width sled, codenamed Tuscany there will be
                                                four OAMs.
                                            </td>
                                            <td class="VK_bg_e6 ps-2 py-2 pe-3 border-white border-2">
                                                PCI Express Card
                                            </td>
                                        </tr>
                                    </thead>
                                    <tbody class="VK_theme_tbody">
                                        <tr>
                                            <td>
                                                SKU
                                            </td>
                                            <td>
                                                OAM SKU Based on a Thermal Design Power of 600 Watts
                                            </td>
                                            <td>
                                                OAM SKU Based on a Thermal Design Power of 450 Watts
                                            </td>
                                            <td>
                                                PCI Express Card with a Thermal Design Power of 300 Watts
                                            </td>
                                        </tr>
                                        <tr>
                                            <td>
                                                Thermal Design Power
                                            </td>
                                            <td>
                                                600 watts – Liquid cooling only
                                            </td>
                                            <td>
                                                450 watts – Liquid or Air cooling
                                            </td>
                                            <td>
                                                300 watts – Air cooling only
                                            </td>
                                        </tr>
                                        <tr>
                                            <td>
                                                Silicon
                                            </td>
                                            <td>
                                                Xe-core (Codename Ponte Vecchio)
                                            </td>
                                            <td>
                                                Xe-core (Codename Ponte Vecchio)
                                            </td>
                                            <td>
                                                Xe-core (Codename Ponte Vecchio)
                                            </td>
                                        </tr>
                                        <tr>
                                            <td>
                                                Xe HPC Stacks
                                            </td>
                                            <td>
                                                Two Xe HPC Stacks
                                            </td>
                                            <td>
                                                Two Xe HPC Stacks
                                            </td>
                                            <td>
                                                One Xe HPC Stack
                                            </td>
                                        </tr>
                                        <tr>
                                            <td>
                                                Xe HPC Micro-Architecture
                                            </td>
                                            <td>
                                                128 Xe-cores with a total of 1024 Vector Engines and 1024 Matrix Engines
                                            </td>
                                            <td>
                                                96 Xe-cores with a total of 896 Vector Engines and 896 Matrix Engines
                                            </td>
                                            <td>
                                                56 Xe-cores with a total of 448 Vector Engines and 448 Matrix Engines
                                            </td>
                                        </tr>
                                        <tr>
                                            <td>
                                                L1 Cache
                                            </td>
                                            <td>
                                                64MB total or 512KB per Xe-core
                                            </td>
                                            <td>
                                                48MB total or 512KB per Xe-core
                                            </td>
                                            <td>
                                                28MB total or 512KB per Xe-core
                                            </td>
                                        </tr>
                                        <tr>
                                            <td>
                                                L2 Cache
                                            </td>
                                            <td>
                                                408MB total or 204MB per Xe HPC Stack (ECC support is this associated
                                                with L2 or something else?)
                                            </td>
                                            <td>
                                                216MB total or 108MB per Xe HPC Stack (ECC support is this associated
                                                with L2 or something else?)
                                            </td>
                                            <td>
                                                108MB total (ECC support is this associated with L2 or something else?)
                                            </td>
                                        </tr>
                                        <tr>
                                            <td>
                                                Memory
                                            </td>
                                            <td>
                                                128GB HBM2e total or 64GB per Xe HPC Stack with an aggregate bandwidth
                                                of 12.8 TB/s
                                            </td>
                                            <td>
                                                96GB HBM2e total or 48GB per Xe HPC Stack with an aggregate bandwidth of
                                                12.8 TB/s
                                            </td>
                                            <td>
                                                48MB total or 512KB per Xe-core with an aggregate bandwidth of 12.8 TB/s
                                            </td>
                                        </tr>
                                        <tr>
                                            <td>
                                                Embedded Multi-Die Interconnect Bridge (EMIB) Interface
                                            </td>
                                            <td>
                                                230 GB/s between Xe HPC Stacks 1 and 2 within the same OAM
                                            </td>
                                            <td>
                                                230 GB/s between Xe HPC Stacks 1 and 2 within the same OAM
                                            </td>
                                            <td>
                                                N/A
                                            </td>
                                        </tr>
                                        <tr>
                                            <td>
                                                Xe Links
                                            </td>
                                            <td>
                                                16 total or 8 per Xe HPC Stack
                                                <br>
                                                <br>
                                                Each link is capable of 26.5 GB/s of bandwidth in each direction
                                            </td>
                                            <td>
                                                16 total or 8 per Xe HPC Stack
                                                <br><br>
                                                Each link is capable of 26.5 GB/s of bandwidth in each direction
                                            </td>
                                            <td>
                                                3 total supporting x2 or x4 bridge
                                                <br><br>
                                                Each link is capable of 26.5 GB/s of bandwidth in each direction
                                            </td>
                                        </tr>
                                        <tr>
                                            <td>
                                                HBM
                                            </td>
                                            <td>
                                                Up to 4x HBM2e 3.2GT/s
                                            </td>
                                            <td>
                                                Up to 4x HBM2e 3.2GT/s
                                            </td>
                                            <td>
                                                Up to 4x HBM2e 3.2GT/s
                                            </td>
                                        </tr>
                                        <tr>
                                            <td>
                                                Memory Capacity
                                            </td>
                                            <td>
                                                16G-128G (1T to 2T) Dedicated
                                            </td>
                                            <td>
                                                &nbsp;
                                            </td>
                                            <td>
                                                &nbsp;
                                            </td>
                                        </tr>
                                        <tr>
                                            <td>
                                                PCI Express
                                            </td>
                                            <td>
                                                PCI Express
                                            </td>
                                            <td>
                                                Gen5
                                            </td>
                                            <td>
                                                Gen5
                                            </td>
                                        </tr>
                                        <tr>
                                            <td>
                                                PCIe
                                            </td>
                                            <td>
                                                X16 PCIe Gen5 per tile
                                            </td>
                                            <td>
                                                &nbsp;
                                            </td>
                                            <td>
                                                &nbsp;
                                            </td>
                                        </tr>
                                        <tr>
                                            <td>
                                                Media Codec Support
                                            </td>
                                            <td>
                                                Decode only functions for AVC, AVI and HEVC
                                            </td>
                                            <td>
                                                Decode only functions for AVC, AVI and HEVC
                                            </td>
                                            <td>
                                                Decode only functions for AVC, AVI and HEVC
                                            </td>
                                        </tr>
                                        <tr>
                                            <td>
                                                FP64 PEAK FLOPS
                                            </td>
                                            <td>
                                                52 TFLOPS
                                            </td>
                                            <td>
                                                44 TFLOPS
                                            </td>
                                            <td>
                                                22 TFLOPS
                                            </td>
                                        </tr>
                                        <tr>
                                            <td>
                                                FP32 PEAK FLOPS
                                            </td>
                                            <td>
                                                52 TFLOPS
                                            </td>
                                            <td>
                                                44 TFLOPS
                                            </td>
                                            <td>
                                                22 TFLOPS
                                            </td>
                                        </tr>
                                        <tr>
                                            <td>
                                                BF16 PEAK FLOPS
                                            </td>
                                            <td>
                                                832 TFLOPS
                                            </td>
                                            <td>
                                                704 TFLOPS
                                            </td>
                                            <td>
                                                352 TFLOPS
                                            </td>
                                        </tr>
                                        <tr>
                                            <td>
                                                INT8 PEAK FLOPS
                                            </td>
                                            <td>
                                                1664 TFLOPS
                                            </td>
                                            <td>
                                                1408 TFLOPS
                                            </td>
                                            <td>
                                                704 TFLOPS
                                            </td>
                                        </tr>
                                    </tbody>
                                </table>
                            </div>
                        </div>
                        <div class="mt-5">
                            <h3 class="VK_side_heading fw-light">
                                Virtualization
                            </h3>
                            <p>
                                The Xe HPC micro-architecture supports virtualization built upon Single Root I/O
                                Virtualization (SR-IOV). Sixty-three virtual functions (VFs) are possible with a two Xe
                                HPC Stack implementation to help with the scalability of virtualization. To assist with
                                management VF profiles can be dynamically modified without taking the system offline.
                                Additionally, there is flexibility to administer both heterogeneous or homogeneous
                                profiles across the device or tile. Security of the VFs is done through both hardware
                                isolation of the device and at the Xe HPC Stack. Isolating the data within the memory
                                associated with device and at the Xe HPC Stack provides an additional level of security.
                            </p>
                            <p>
                                Virtualization can be implemented at the OAM level (i.e. multiple Xe HPC Stacks) or at
                                the individual Xe HPC Stack level. The entire OAM’s processing and memory capabilities
                                can be dedicated to a single virtual function or each Xe HPC Stack within the OAM can be
                                associated with a separate VF. Alternatively, VFs can share the entire OAM or an Xe HPC
                                Stack via an allotted a slice of time. These modes of operation are managed by the
                                hypervisor and provide full memory isolation between the VFs.
                            </p>
                        </div>
                        <div class="mt-5">
                            <h3 class="VK_side_heading fw-light">
                                Software
                            </h3>
                            <p>
                                Intel oneAPI is provided for ease of adoption for software developers and includes a
                                complete set of advanced compliers, libraries, and a
                                <a href="" class="Vk_text_underline_dots VK_a">
                                    code migration with SYCL
                                </a>
                                along with analysis and debugger tools. Developers can be confident that existing
                                applications will work seamlessly with oneAPI due to its interoperability with existing
                                programming models and code bases (C++, Fortran, Python, OpenMP, etc.). The transition
                                to CPUs, GPUs, FPGAs, and AI accelerators using a single code base in Data Parallel C++
                                helps to optimize a developer’s time. A good place to begin for application developers
                                is the
                                <a href="" class="Vk_text_underline_dots VK_a">
                                    workflow guide
                                </a>
                                focused on the GPU and the
                                <a href="" class="Vk_text_underline_dots VK_a">
                                    Intel® oneAPI Base Toolkit.
                                </a>
                            </p>
                            <p>
                                The
                                <a href="" class="Vk_text_underline_dots VK_a">
                                    Intel oneAPI Base Toolkit
                                </a>
                                provides a set of high-performance tools for building C++ and Data Parallel C++
                                applications. While domain specific toolkits are available for specific workloads.
                                <a href="" class="Vk_text_underline_dots VK_a">
                                    Intel oneAPI Tools for HPC
                                </a>
                                provides support for scalability of Fortran, OpenMP and MPI applications.
                                <a href="" class="Vk_text_underline_dots VK_a">
                                    Intel oneAPI Tools for IoT
                                </a>
                                is designed to help developers that are designing solutions at the edge of the network.
                                <a href="" class="Vk_text_underline_dots VK_a">
                                    Intel oneAPI AI Analytics Toolkit
                                </a>
                                provides support for optimization of Deep Learning frameworks and Python libraries to
                                assist with machine learning and data science pipelines.
                                <a href="" class="Vk_text_underline_dots VK_a">
                                    Intel oneAPI Rendering Toolkit
                                </a>
                                focuses on visualization applications. Intel Distribution of OpenVINO Toolkit assists
                                developers working with inference and applications from the edge to the cloud. Lastly
                                for guidance on code optimization see the
                                <a href="" class="Vk_text_underline_dots VK_a">
                                    oneAPI GPU Optimization Guide.
                                </a>
                            </p>
                            <p>
                                If you are focused on AI and work with PyTorch or the Tensor Flow Frameworks, the latest
                                versions are compatible with the Intel® Data Center GPU Max Series.
                            </p>
                        </div>
                        <div class="mt-5">
                            <p>
                                <b>
                                    Additional Resources:
                                </b>
                            </p>
                            <div class="mt-4">
                                <p>
                                    <a href="" class="Vk_text_underline_dots VK_a">
                                        Technical Overview Of The Intel® Xeon® Scalable processor Max Series
                                    </a>
                                </p>
                                <p>
                                    <a href="" class="Vk_text_underline_dots VK_a">
                                        Technical Overview Of The 4th Gen Intel® Xeon® Scalable processor family
                                    </a>
                                </p>
                                <p>
                                    <a href="" class="Vk_text_underline_dots VK_a">
                                        Code Sample: Intel® Advanced Matrix Extensions (Intel® AMX) - Intrinsics
                                        Functions
                                    </a>
                                </p>
                                <p>
                                    <a href="" class="Vk_text_underline_dots VK_a">
                                        Intel® In-Memory Analytics Accelerator (Intel® IAA) Enabling Guide
                                    </a>
                                </p>
                                <p>
                                    <a href="" class="Vk_text_underline_dots VK_a">
                                        Proof Points of Intel® Dynamic Load Balancer (Intel® DLB)
                                    </a>
                                </p>
                            </div>
                            <div class="mt-4">
                                <p>
                                    <b>
                                        The Author:
                                    </b>
                                    David Mulnix is a software engineer and has been with Intel Corporation for over 20
                                    years. His areas of focus include technical content development and training,
                                    software automation, server performance and power analysis with
                                    <a href="" class="Vk_text_underline_dots VK_a">
                                        SPECPower,
                                    </a>
                                    and he supported the development effort for the
                                    <a href="" class="Vk_text_underline_dots VK_a">
                                        Server Efficiency Rating ToolTM.
                                    </a>
                                </p>
                            </div>
                            <div class="mt-4">
                                <p>
                                    <b>Notices/Disclaimers</b>
                                </p>
                                <p>
                                    Performance varies by use, configuration and other factors. Learn more on the
                                    Performance Index site.
                                </p>
                                <p>
                                    No product or component can be absolutely secure.
                                </p>
                                <p>
                                    Code names are used by Intel to identify products, technologies, or services that
                                    are in development and not publicly available. These a​​re not "commercial" names
                                    and not intended to function as trademarks
                                </p>
                                <p>
                                    Intel disclaims all express and implied warranties, including without limitation,
                                    the implied warranties of merchantability, fitness for a particular purpose, and
                                    non-infringement, as well as any warranty arising from course of performance, course
                                    of dealing, or usage in trade
                                </p>
                                <p>
                                    Your costs and results may vary.
                                </p>
                                <p>
                                    Intel technologies may require enabled hardware, software or service activation.
                                </p>
                                <p>
                                    © Intel Corporation. Intel, the Intel logo, and other Intel marks are trademarks of
                                    Intel Corporation or its subsidiaries. Other names and brands may be claimed as the
                                    property of others.
                                </p>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </section>


        <!--  -->
        <section class="py-5">
            <div class="VK_cont">
                <div class="row m-0">
                    <div class="col">
                        <p>
                            Product and Performance Information
                        </p>
                        <p class="VK_font14">
                            Performance varies by use, configuration and other factors. Learn more at
                            <a href="" class="Vk_text_underline_dots VK_a">
                                www.Intel.com/PerformanceIndex.
                            </a>
                        </p>
                    </div>
                </div>
            </div>
        </section>


        <!-- footer -->
        <footer>
            <div id="footer"></div>
        </footer>

    </main>



    <!---------------- Javascript Files ---------------->

    <script>
        // navbar include
        fetch('/y_index/y_navbar.html')
            .then(response => response.text())
            .then(data => {
                document.getElementById('navbar').innerHTML = data;
            });
        // footer include 
        fetch('/y_index/y_footer.html')
            .then(response => response.text())
            .then(data => {
                document.getElementById('footer').innerHTML = data;
            });
    </script>

    <!-- jquery -->
    <script src="/js/jquery-3.7.1.js"></script>


    <!-- bootstrap js file -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/js/bootstrap.bundle.min.js"
        integrity="sha384-MrcW6ZMFYlzcLA8Nl+NtUVF0sA7MsXsP1UyJoMp4YLEuNSfAP+JcXn/tWtIaxVXM"
        crossorigin="anonymous"></script>

    <!-- costom js file -->
    <script src="/js/vivek.js"></script>

    <!-- all min -->
    <!-- <script src="/js/all.min.js"></script> -->

</body>

</html>