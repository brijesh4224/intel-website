<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Intel® Software News Updates</title>

    <link rel="stylesheet"
        href="/css/dk_developer_edge_iot_&_5G_optimize_fine_tuning_and_deployment_of_LLMs_on_an_ai_pc.css">

    <!-- header footer -->
    <link rel="stylesheet" href='/css/yatri.css'>

    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/css/bootstrap.min.css" rel="stylesheet"
        integrity="sha384-EVSTQN3/azprG1Anm3QDgpJLIm9Nao0Yz1ztcQTwFspd3yD65VohhpuuCOmLASjC" crossorigin="anonymous">
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/js/bootstrap.bundle.min.js"
        integrity="sha384-MrcW6ZMFYlzcLA8Nl+NtUVF0sA7MsXsP1UyJoMp4YLEuNSfAP+JcXn/tWtIaxVXM"
        crossorigin="anonymous"></script>
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.6.0/css/all.min.css" rel="stylesheet">
</head>

<style>
    .VK_ai_navigation{
        display: block !important;
    }
</style>

<body>

    <!-- header -->
    <div id="navbar"></div>

    <!-- Optimize Fine-Tuning and Deployment of LLMs on an AI PC -->
    <section>
        <div class="mv_intel_amx_bg_color">
            <div class="container">
                <div class="row mv_intel_amx_content">
                    <div class="col-md-12 col-sm-12 mv_intel_amx_item">
                        <div class="mv_intel_amx">
                            <h1><a style="color: #fff; font-weight: 300; font-size: 2.25rem;" href="">Intel® Software News Updates</a></h1>
                            <p style="margin-bottom: 20px; font-size: 1.25rem;">Get the latest info on products and services that meet your needs.</p>
                            <a class="dk_learn" href="">Sign Up</a>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="container py-5">
        <div class="row">
            <div class="d-flex justify-content-end col-xl-10 py-3 d-md-none">
                <i class="fa-solid fa-print fs-4 px-2 " style="color:#0068B5"></i>
                <i class="fa-regular fa-envelope fs-4 px-2" style="color:#0068B5"></i>
            </div>
            <div class="col-lg-3 col-md-4">
                <!-- nav -->
                <div class="VK_client_app_navigation VK_ai_navigation">
                    <div class="justify-content-center align-items-center overflow-hidden flex-nowrap mb-4">
                        <ul class="VK_ai_nav_bar list-unstyled m-0">
                            <li>
                                <a href="#dk_november_2024" class="text-dark text-decoration-none d-block">
                                    November 2024
                                </a>
                            </li>
                            <li>
                                <a href="#dk_october_2024" class="text-dark text-decoration-none d-block VK_ai_nav_link">
                                    October 2024
                                </a>
                            </li>
                            <li>
                                <a href="#dk_september_2024"
                                    class="text-dark text-decoration-none d-block VK_ai_nav_link">
                                    September 2024
                                </a>
                            </li>
                            <li>
                                <a href="#dk_August_2024" class="text-dark text-decoration-none d-block VK_ai_nav_link">
                                    August 2024
                                </a>
                            </li>
                            <li>
                                <a href="#dk_june_2024" class="text-dark text-decoration-none d-block VK_ai_nav_link">
                                    June 2024
                                </a>
                            </li>
                            <li>
                                <a href="#dk_may_2024" class="text-dark text-decoration-none d-block VK_ai_nav_link">
                                    May 2024
                                </a>
                            </li>
                            <li>
                                <a href="#dk_april_2024" class="text-dark text-decoration-none d-block VK_ai_nav_link">
                                    April 2024
                                </a>
                            </li>
                            <li>
                                <a href="#dk_march_2024" class="text-dark text-decoration-none d-block VK_ai_nav_link">
                                    March 2024
                                </a>
                            </li>
                            <li>
                                <a href="#dk_february_2024" class="text-dark text-decoration-none d-block VK_ai_nav_link">
                                    February 2024
                                </a>
                            </li>
                            <li>
                                <a href="#dk_january_2024" class="text-dark text-decoration-none d-block VK_ai_nav_link">
                                    January 2024
                                </a>
                            </li>
                            <li>
                                <a href="#dk_december_2023" class="text-dark text-decoration-none d-block VK_ai_nav_link">
                                    December 2023
                                </a>
                            </li>
                            <li>
                                <a href="#dk_november_2023" class="text-dark text-decoration-none d-block VK_ai_nav_link">
                                    November 2023 
                                </a>
                            </li>
                        </ul>
                    </div>
                </div>
                <div class="dk_things_code_main">
                    <div class="dk_things_code">
                        <h6><b>What's New</b></h6>
                        <ul class="ps-3">   
                            <li><p>The 2025.0 Intel® Software Development Tools Are Here: Marking the 5th Anniversary of oneAPI
                                   <a class="b_special_a2" href="">Read</a></p>
                            </li>
                            <li><p>Announcing General Availability of Object Storage on Intel® Tiber™ AI Cloud
                                   <a class="b_special_a2" href="">Read</a></p>
                            </li>
                            <li><p>Inflection AI Launches Enterprise AI Running on Intel® Gaudi® 3 and Intel® Tiber™ AI Cloud
                                   <a class="b_special_a2" href="">Read</a></p>
                            </li>
                            <li><p>Intel Launches Xeon 6 and Gaudi 3, Enabling the Next-Generation of AI Solutions 
                                   <a class="b_special_a2" href="">Read</a></p>
                            </li>
                            <li><p>Deliver AI Faster on Next-Gen Intel® Core™ Ultra AI PCs
                                   <a class="b_special_a2" href="">Read</a></p>
                            </li>
                        </ul>
                        <a class="dk_button" href="">Software News on Intel Newsroom</a>
                    </div>
                </div>
            </div>
            <div class="col-lg-9 col-md-8">
                <div class="d-md-flex justify-content-end col-xl-10 py-3 d-none">
                    <i class="fa-solid fa-print fs-4 px-2 " style="color:#0068B5"></i>
                    <i class="fa-regular fa-envelope fs-4 px-2" style="color:#0068B5"></i>
                </div>
                <div class="col-xl-10">
                    <p>&nbsp;</p>
                    <div style="padding-bottom: 16px;">
                    <p>&nbsp;</p>
                    <section id="dk_november_2024">
                        <h3 style="font-weight: 300;">The 2025.0 Intel® Software Development Tools Are Here, Marking the 5th Anniversary of oneAPI</h3>
                        <p class="mt-3">November 17, 2024 | <a style="color: blue;" href="">Intel® Software Development Tools</a></p>
                        <p>Today, Intel released its 2025.0 developer tools—all powered by oneAPI—marking the <a class="b_special_a1" href="">5<sup>th</sup> anniversary of the oneAPI programming model&nbsp;</a> with expanded performance optimizations and open-standards coverage to support the latest innovations in multiarchitecture, hardware-agnostic software development and deployment, edge to cloud.</p>
                        <p style="text-align:center"><a style="padding: 5px 20px;" class="dk_button" href="">Explore &amp; download</a></p>
                        <h4>3 Key Benefits</h4>
                        <ul>
                            <li><strong>More Performance on Intel Platforms</strong> – Achieve up to 3x higher GenAI performance on 6<sup>th</sup> Gen Intel® Xeon® processors (P-cores) with <a href="" style="color:blue; text-decoration:none !important">oneDNN</a>, Intel-optimized AI frameworks, and Intel® AMX<sup>1</sup>; achieve up to 2.5x better HPCG performance with MRDIMM<sup>2</sup> and <a href="" style="color:blue; text-decoration:none !important">oneMKL</a>; develop high-performance AI on the PC—including LLM development—with optimized tools to unlock the power of Intel® Core™ Ultra processors (Series 2); and improve security and encryption with <a href="" style="color:blue; text-decoration:none !important">Intel® Cryptography Primitives Library</a>.</li>
                            <li><strong>More Access to Industry-Standard Tools</strong> – Get even more from your existing development workflows using industry-leading AI frameworks and performance libraries with even more built-in Intel optimizations, including native support for PyTorch 2.5 on CPUs and GPUs; achieve optimal performance across CPU, GPU, and AI accelerators from the latest LLMs—Llama 3.2, Qwen2, Phi-3, and more—with <a href="" style="color:blue; text-decoration:none !important">Intel AI tools</a>; and streamline your software setup with our <a href="\" style="color:blue; text-decoration:none !important">toolkit selector</a> to install full kits or right-sized sub-bundles.</li>
                            <li><strong>More Hardware Choices </strong>– Enjoy increased multi-vendor, multiarchitecture support, including faster CUDA*-to-SYCL* migration with the <a href="" style="color:blue; text-decoration:none !important">Intel® DPC++ Compatibility Tool</a> that auto-migrates over 100 APIs used by popular AI, HPC, and rendering apps; achieve near-native performance on CPU and GPU for numeric compute with Intel® Distribution for Python; get 4x speedup of GPU kernels for algorithms with <a href="" style="color:blue; text-decoration:none !important">oneDPL</a>; and gain future system flexibility and prevent lock-in through cross-hardware AI-acceleration libraries, including Triton, JAX, and OpenXLA*.</li>
                        </ul>

                        <h4>The Nuts & Bolts</h4>
                        <p>Here's the collection for those interested in diving into the component-level details.</p>
                        <p><strong>Compilers</strong></p>
                        <ul>
                            <li><a href="" style="color:blue; text-decoration:none !important">Intel oneAPI DPC++/C++ Compiler</a> adds optimizations tailored for Intel® Xeon® 6 processors and Intel® Core™ Ultra processors, enables dynamic execution and flexible programming for Intel GPUs with new SYCL Bindless Textures support, streamlines development with new LLVM sanitizers to detect and troubleshoot device code issues, and enhances OpenMP standards conformance for 5.x and 6.0 plus add a more user-friendly optimization report that includes OpenMP offloading details.</li>
                            <li><a href="" style="color:blue; text-decoration:none !important">Intel® Fortran Compiler</a> adds several enhancements, including Fortran 2023 standard features such as the AT Edit Descriptor for cleaner output, conditional TEAMS construct execution with the new IF clause for OpenMP 6.0, and support for arrays of co-arrays and “standard-semantics” option to precisely control application standards compliance; updates Fortran Developer Guide and reference documentation with refreshed code samples and added support for Fortran 2018 and 2023 Fortran language features.</li>
                        </ul>
                        <p><strong>Performance Libraries</strong></p>
                        <ul>
                            <li><a href="" style="color:blue; text-decoration:none 
                            !important">Intel® oneAPI Math Kernel Library</a> (oneMKL) introduces performance optimizations across multiple domains—BLAS, LAPACK, FFT, &nbsp;and others—for developers targeting Xeon 6 processors with P-cores. It also adds significant improvements for HPC workload execution using single-precision 3D real in-place FFT on Intel® Data Center GPU Max Series and makes available new distribution models and data types for RNG using SYCL device API.</li>
                            <li><a href="" style="color:blue; text-decoration:none 
                            !important">Intel® oneAPI Data Analytics Library</a> (oneDAL) enables calculation of SHAP (SHapley Additive exPlanations) values for binary classification models, which are required for explainability random forest (RF) algorithms.</li>
                            <li><a href="" style="color:blue; text-decoration:none 
                            !important">Intel® oneAPI Deep Neural Network Library</a> (oneDNN) maximizes efficiency and performance with tailored optimizations for the latest Intel® platforms—spanning server, desktop, and mobile—including significantly faster performance for large language models (LLMs) and Scaled Dot-Product Attention subgraphs.</li>
                            <li><a href="" style="color:blue; text-decoration:none 
                            !important">Intel® oneAPI Threading Building Blocks</a> (oneTBB) improves scalability for task_group, flow_graph, and parallel_for_each so multi-threaded applications run faster; introduces try_put_and_wait experimental API for faster results using oneTBB flow graph to process overlapping messages on a shared graph.</li>
                            <li><a href="" style="color:blue; text-decoration:none 
                            !important">Intel® oneAPI Collective Communications Library</a> (oneCCL) improves workload performance and scalability with enhancements to Key-Value store, which allows workloads to scale up to an even larger number of nodes, and performance improvements to key collectives such as Allgather, Allreduce, and Reduce-scatter.</li>
                            <li><a href="" style="color:blue; text-decoration:none 
                            !important">Intel® MPI Library</a> offers a full MPI 4.0 implementation, including partitioned communication, improved error handling, and Fortran 2008 support; and improves scale-out/scale-up performance on both Xeon 6 processors with P-core pinning and Intel GPUs via optimizations for MPI_Allreduce.</li>
                            <li><a href="" style="color:blue; text-decoration:none 
                            !important">Intel® oneAPI DPC++ Library</a> (oneDPL) accelerates GPU kernels up to 4x<sup>3</sup> for algorithms including reduce, scan and many other functions. Range-based algorithms with over 20 new C++20 standard ranges and views accelerate highly parallel code execution on multiarchitecture devices.</li>
                            <li><a href="" style="color:blue; text-decoration:none 
                            !important">Intel® Integrated Performance Primitives</a> (Intel® IPP) adds CET-enabled protection (Control-flow Enforcement Technology), cutting-edge, hardware-enforced security measures that safeguard software against attacks and exploitation risks.</li>
                            <li><a href="" style="color:blue; text-decoration:none 
                            !important">Intel® Cryptography Primitives Library</a> (formerly Intel® IPP) enables developers to dispatch on Xeon 6 processors, turbocharging RSA encryption (2k, 3k, 4k) with multi-buffer capabilities and hashing with an enhanced SM3 algorithm.</li>
                            </ul>
                      <p><strong>Analyzers &amp; Debuggers</strong></p>
                      <ul>
                        <li><a href="" style="color:blue; text-decoration:none !important">Intel® DPC++ Compatibility Tool</a> saves time and effort when migrating CUDA code and CMake build script to SYCL via auto-migration of more APIs used by popular AI, HPC, and rendering applications; migrated code is easy to comprehend with SYCLcompat, easy to debug with CodePin, and runs performantly on NVIDIA GPUs.</li>
                        <li><a href="" style="color:blue; text-decoration:none !important">Intel® VTune™ Profiler</a> adds support for Intel Xeon 6 processors with P-cores and Core Ultra processors (Series 2), plus profiling support for Python 3.11, improving productivity with the ability to focus Python profiling to areas of interest and control performance data collection with <a href="" style="color:blue; text-decoration:none !important">Intel® ITT APIs</a>.</li>
                        <li><a href="" style="color:blue; text-decoration:none !important">Intel® Advisor</a> increases developers’ ability to identify bottlenecks, optimize code, and achieve peak performance on the latest Intel platforms; introduces a more adaptable kernel-matching mechanism—flexible kernel matching and XCG integration—to identify and analyze code regions relevant to specific optimization goals.</li>
                        <li><a href="" style="color:blue; text-decoration:none !important">Intel® Distribution for GDB*</a> rebases to GDB 15, staying current and aligned with the latest enhancements supporting effective application debug; adds support for Core Ultra processors (Series 2) on Windows*; and enhances developer experience, both on the command line and when using Microsoft* Visual Studio and Visual Studio Code*, by boosting the debugger performance and refining the user interface.</li>
                    </ul>
                    <p><strong>AI &amp; ML Tools, Frameworks, and Accelerated Python</strong></p>
                    <ul>
                        <li><a href="" style="color:blue; text-decoration:none !important">Intel® Distribution for Python*</a> provides drop-in, near-native performance on CPU and GPU for numeric compute; Data Parallel Extension for Python (dpnp) and Data Parallel Control (dpctl) expand compatibility, adding NumPy 2.0 support in the runtime and providing asynchronous execution of offloaded operations.</li>
                        <li><a href="" style="color:blue; text-decoration:none !important">Intel AI Tools</a> latest release ensures current and future GenAI foundation models—Llama 3.2, Qwen2, Phi-3 family, and more—perform optimally across Intel CPUs, GPUs, and AI accelerators.</li>
                        <li><a href="" style="color:blue; text-decoration:none !important">Triton</a> (open source GPU programming for neural networks) enables developers to achieve peak performance and kernel efficiency on Intel GPUs thanks to it being fully optimized for Intel Core Ultra and Data Center GPU Max Series processors and available upstream in stock PyTorch.</li>
                        <li><a href="" style="color:blue; text-decoration:none !important">Native Support for PyTorch 2.5</a> is accessible on Intel’s Data Center GPUs, Core Ultra processors, and client GPUs, where it can be used to develop on Windows with out-of-the-box support for Intel® Arc™ Graphics and Intel® Iris® XE Graphics GPUs.</li>
                        <li>Simplify enterprise GenAI adoption and reduce the time to production of hardened, trusted solutions by adopting the open platform project, <a href="" style="color:blue; text-decoration:none !important">OPEA</a>, part of LF AI &amp; Data. Now at release 1.0, OPEA continues to gain momentum with over 40 partners, including AMD, BONC, ByteDance, MongoDB, and Rivos.</li>
                        <li>Seamlessly run&nbsp;<a href="" style="color:blue; text-decoration:none !important">JAX</a>&nbsp;models on Intel® Data Center GPU Max and Flex with Intel® Extension for <a href="" style="color:blue; text-decoration:none !important">OpenXLA*</a>, an Intel-optimized PyPI package based on&nbsp;<a href="" style="color:blue; text-decoration:none !important">PJRT</a>&nbsp;plugin mechanism.</li>
                    </ul>
                    <p style="text-align:center"><a style="padding: 5px 20px;" class="dk_button" href="">Explore &amp; download the tools</a>&nbsp;</p>

                    <p><strong>Footnotes</strong></p>
                    <p><sup>1</sup> See [9A2] at intel.com/processorclaims: Intel® Xeon® 6. Results may vary.</p>
                    <p><sup>2</sup> See [9H10] at intel.com/processorclaims: Intel® Xeon® 6. Results may vary.</p>
                    <p><sup>3</sup> See <a href="" style="color:blue; text-decoration:none !important" title="">oneDPL product page</a></p>

                    <p>&nbsp;</p>
                    <h3 style="font-weight: 350; margin: 1rem 0 11px;">oneAPI Turns 5!</h3>
                    <p>November 17, 2024 | <a href="" style="color:blue; text-decoration:none !important">What is oneAPI?</a>, <a href="" style="color:blue; text-decoration:none !important">oneAPI Developer Page</a></p>
                    <h4>Happy 5<sup>th</sup> Anniversary to the open, standards-based, multiarchitecture programming initiative for accelerator architectures</h4>

                    <p><img alt="" height="141" src="/img/darshit_image/logo-oneapi.png" style="float:left" width="250"></p>
                    <p>Launched at Supercomputing 2019, the oneAPI initiative not only fostered permanent change in how the global developer ecosystem approaches heterogeneous programming, it’s become the foundation for building, optimizing, and deploying high-performance software that can run on any vendor architecture. &nbsp;</p>
                    <p>With hundreds of contributors, over 4.3 million installations, and 6.8 million developers using it via Intel® Software and AI Tools (explore the <a href="" style="color:blue; text-decoration:none !important">2025.0 release</a>), oneAPI is arguably one of the most eminent programming standards, a point further underscored by its adoption in 2023 by the Unified Acceleration (UXL) Foundation, hosted by Linux Foundation. UXL’s mission: &nbsp;to deliver an open-standard accelerator programming model that simplifies development of performant, cross-platform applications. It marks yet another critical step in driving innovation, with oneAPI as a key component.</p>
                    <p>All that in just 5 years. (Imagine what the next 5 will bring.)</p>
                    <p>If you haven’t tried oneAPI, you can get the gist of it <a href="" style="color:blue; text-decoration:none !important">here</a> and download the 2025.0 tools <a href="" style="color:blue; text-decoration:none !important">here</a>.</p>
                    <h4><strong>Celebrating oneAPI’s 5<sup>th</sup> Anniversary – What the Ecosystem is Saying</strong></h4>
                    <p>&nbsp;</p>

                    <div class="d-flex">
                        <div class="">
                            <i class="fa-solid fa-quote-left" style="font-size: 65px; color: #E5E6E6; padding-right: 15px;"></i>
                        </div>
                        <div class="">
                            <div class="mv_together_content">
                                <p class="mb-2" style="font-weight: 300;"><i>The Performance improvements achieved with Intel’s latest Xeon platform will support our efforts in wider adoption of HE, particularly in improving data analysis and insights, as well as product innovation around areas such as anti-financial crime.</i></p>
                                <p>Nikolai Larbalestier, Senior Vice President, Enterprise Architecture, NASDAQ</p>
                            </div>
                        </div>
                    </div>

                    <p>&nbsp;</p>
                    <p>&nbsp;</p>

                    <div class="d-flex">
                        <div class="">
                            <i class="fa-solid fa-quote-left" style="font-size: 65px; color: #E5E6E6; padding-right: 15px;"></i>
                        </div>
                        <div class="">
                            <div class="mv_together_content">
                                <p class="mb-2" style="font-weight: 300;"><i>oneAPI has revolutionized the way we approach heterogeneous computing by enabling seamless development across architectures. Its open, unified programming model has accelerated innovation in fields from AI to HPC, unlocking new potential for researchers and developers alike. Happy 5th Anniversary to oneAPI!</i></p>
                                <p>– Dr. Gal Oren, Asst Professor, Dept. of Computer Science, Israel Institute of Technology</p>
                            </div>
                        </div>
                    </div>

                    <p>&nbsp;</p>
                    <p>&nbsp;</p>

                    <div class="d-flex">
                        <div class="">
                            <i class="fa-solid fa-quote-left" style="font-size: 65px; color: #E5E6E6; padding-right: 15px;"></i>
                        </div>
                        <div class="">
                            <div class="mv_together_content">
                                <p class="mb-2" style="font-weight: 300;"><i>Intel's commitment to their oneAPI software stack is a testament to their developer-focused, open-standards commitment. As oneAPI celebrates its 5th Anniversary, it provides comprehensive and performant implementations of OpenMP and SYCL for CPUs and GPUs, bolstered by an ecosystem of library and tools to make the most of Intel processors.</i></p>
                                <p>– Dr. Tom Deakin, Sr. Lecturer, Head of Advanced HPC Research Group, University of Bristol</p>
                            </div>
                        </div>
                    </div>

                    <p>&nbsp;</p>
                    <p>&nbsp;</p>

                    <div class="d-flex">
                        <div class="">
                            <i class="fa-solid fa-quote-left" style="font-size: 65px; color: #E5E6E6; padding-right: 15px;"></i>
                        </div>
                        <div class="">
                            <div class="mv_together_content">
                                <p class="mb-2" style="font-weight: 300;"><i>Celebrating 5 years of oneAPI. In ExaHyPE, oneAPI has been instrumental in implementing the numerical compute kernels for hyperbolic equation systems, making a huge different in performance with SYCL providing the ideal abstraction and agnosticism for exploring these variations. This versatility enabled our team, together with Intel engineers, to publish three distinct design paradigms for our kernels.</i></p>
                                <p>– Dr. Tobias Weinzierl, Director, Institute for Data Science, Durham University</p>
                            </div>
                        </div>
                    </div>

                    <p>&nbsp;</p>
                    <p>&nbsp;</p>

                    <div class="d-flex">
                        <div class="">
                            <i class="fa-solid fa-quote-left" style="font-size: 65px; color: #E5E6E6; padding-right: 15px;"></i>
                        </div>
                        <div class="">
                            <div class="mv_together_content">
                                <p class="mb-2" style="font-weight: 300;"><i>Happy 5th Anniversary, oneAPI! We’ve been partners since the private beta program in 2019. We are currently exploring energy-efficient solutions for simulations in material science and data analysis in bioinformatics with different accelerators. For that, the components of oneAPI, its compilers with backends for various GPUs and FPGAs, oneMKL, and the performance tools VTune Profiler and Advisor, are absolutely critical.</i></p>
                                <p>– Dr. Thomas Steinke, Head of Supercomputing Department, Zuse Institute Berlin</p>
                            </div>
                        </div>
                    </div>

                    <p>&nbsp;</p>
                    <p>&nbsp;</p>

                    <div class="d-flex">
                        <div class="">
                            <i class="fa-solid fa-quote-left" style="font-size: 65px; color: #E5E6E6; padding-right: 15px;"></i>
                        </div>
                        <div class="">
                            <div class="mv_together_content">
                                <p class="mb-2" style="font-weight: 300;"><i>GROMACS was an early adopter of SYCL as a performance-portability backend, leveraging it to run on multi-vendor GPUs. Over the years, we’ve observed significant improvements in the SYCL standard and the growth of its community. This underscores the importance of open standards in computational research to drive innovation and collaboration. We look forward to continued SYCL development, which will enable enhancements in software performance and increase programmer productivity.</i></p>
                                <p>– Andrey Alekseenko, Researcher, Department of Applied Physics, KTH Royal Institute of Technology</p>
                            </div>
                        </div>
                    </div>

                    <p>&nbsp;</p>
                    <p>&nbsp;</p>

                    <div class="d-flex">
                        <div class="">
                            <i class="fa-solid fa-quote-left" style="font-size: 65px; color: #E5E6E6; padding-right: 15px;"></i>
                        </div>
                        <div class="">
                            <div class="mv_together_content">
                                <p class="mb-2" style="font-weight: 300;"><i>Using the Intel® oneAPI Base Toolkit, [GE HealthCare] successfully migrated code, which requires heavy processing and is extensively used in ultrasound diagnostics solutions, to SYCL. This is a big step forward on the way to a single, open, standards-based programming model for heterogeneous computing. The migrated code efficiently runs on different GPU platforms and achieves competitive performance.</i></p>
                                <p>– Arcady Kempinsky, Sr. Lead Software Architect, GE HealthCare</p>
                            </div>
                        </div>
                    </div>

                    <p>&nbsp;</p>
                    <p>&nbsp;</p>

                    <div class="d-flex">
                        <div class="">
                            <i class="fa-solid fa-quote-left" style="font-size: 65px; color: #E5E6E6; padding-right: 15px;"></i>
                        </div>
                        <div class="">
                            <div class="mv_together_content">
                                <p class="mb-2" style="font-weight: 300;"><i>Using Intel® oneAPI Base Toolkit, we have successfully implemented GE HealthCare's proprietary TrueFidelity DL, a deep learning image reconstruction algorithm available across much of the company's CT portfolio. The open source SYCL compiler provides near entitlement AI/DL inferencing performance for several NVIDIA GPU devices. Based on GE Healthcare experience with OpenCL software, code portability is crucial to protect our software development investment and reuse the software across different platforms and vendors.</i></p>
                                <p>– Mark Valkenburgh, Sr. Director, Global & Strategic Alliances, GE</p>
                            </div>
                        </div>
                    </div>

                    <p>&nbsp;</p>
                    <p class="mb-0">&nbsp;</p>

                    <p class="mb-0">See other testimonials:</p>
                    <ul>
                        <li><a href="" style="color:blue; text-decoration:none !important">AI &amp; Machine Learning Ecosystem Developer Resources</a></li>
                        <li><a href="" style="color:blue; text-decoration:none !important">HPC Ecosystem Developer Resources</a></li>
                    </ul>

                    </section>
                    </div>
                    <p>&nbsp;</p>

                    <section id="dk_october_2024">
                        <h4 style="font-weight: 300;">Announcing General Availability of Object Storage on Intel® Tiber™ AI Cloud</h4>
                        <p>October 17, 2024 | <a class="b_special_a1" href="">Intel® Tiber™ AI Cloud</a></p>
                        <p>Today Intel announced the availability of a new object storage service on its AI Cloud, providing scalable, durable, and cost-effective data storage that meets the demanding requirements of modern data and AI workloads.</p>
                        <p>It’s built on the powerful and open source MinIO platform, which is compatible with the S3 API (AWS’ Simple Storage Service), ensuring easy integration with existing applications and tools.</p>
                        <p style="text-align:center"><a style="padding: 5px 20px;" class="dk_button" href="">Learn more</a></p>
                        <p class="mb-0">Customer benefits include:</p>
                        <ul>
                            <li><strong>Scalability &amp; flexibility</strong> – Can handle massive data storage needs, whether gigabytes or petabytes, to ensure your storage infrastructure grows with your business.</li>
                            <li><strong>Performance</strong> – Optimized for fast data access and retrieval, ensuring data is always accessible and can be processed quickly, including AI/ML workloads.</li>
                            <li><strong>Cost-effective storage</strong> – Enables businesses of all sizes to store vast amounts of data without breaking the bank.</li>
                            <li><strong>Robust security </strong>– Incorporates encryption at rest and in transit and includes robust access controls.</li>
                            <li><strong>Easy integration</strong> – Is purpose-built to integrate seamlessly with your existing workflows and applications spanning backup and recovery, data archiving, data lake use, and more.</li>
                            <li><strong>Enhanced data management</strong> – Manage your data efficiently with features like versioning, lifecycle policies, and metadata management.</li>
                        </ul>
                        <hr>
                        <p>&nbsp;</p>
                        <p>&nbsp;</p>
                        <h3 style="font-weight: 300;">Inflection AI Launches Enterprise AI Running on Intel® Gaudi® 3 and Intel® Tiber™ AI Cloud</h3>
                        <p>October 7, 2024 | <a class="b_special_a1" href="">Inflection AI-Intel collaboration</a>, <a class="b_special_a1" href="" class="">Intel® Tiber™ AI Cloud</a></p>
                        <p><strong>New collaboration delivers turnkey AI-powered platform to drive high-impact results for enterprises</strong></p>
                        <p>Today Inflection AI and Intel announced a collaboration to accelerate the adoption and impact of AI for the world’s largest enterprises. Inflection AI is launching Inflection 3.0, an industry-first, enterprise-grade AI platform, delivering empathetic, conversational and employee-friendly AI capabilities—powered by Intel® Gaudi® 3 accelerators on Intel® Tiber™ AI Cloud—that provides the control, customization, and scalability required for complex, large-scale deployments.</p>
                        <p style="text-align:center"><a style="padding: 5px 20px;" class="dk_button" href="">Learn more</a></p>
                        
                        <p>&nbsp;</p>

                    <div class="d-flex">
                        <div class="">
                            <i class="fa-solid fa-quote-left" style="font-size: 65px; color: #E5E6E6; padding-right: 15px;"></i>
                        </div>
                        <div class="">
                            <div class="mv_together_content">
                                <p class="mb-2" style="font-weight: 300;"><i>“Together, we’re giving enterprise customers ultimate control over their AI,” said Markus Flierl, CVP of Intel Tiber Cloud Services. “By integrating Inflection AI with Intel Tiber AI Cloud and Gaudi 3, we are providing an open ecosystem of software, price and performance, and scalability,  unlocking the critical roadblocks to enterprise AI adoption and the secure, purpose-built, employee-specific, and culture-oriented AI tools customers need.”</i></p>
                                <p>Markus Flierl, CVP of Intel Tiber Cloud Services
                                </p>
                            </div>
                        </div>
                    </div>
                    <p>&nbsp;</p>
                    <p><strong>Why it matters</strong></p>
                    <p>Building an AI platform is complex, requiring extensive infrastructure; time to develop, train, and fine-tune models; and a multitude of engineers, data scientists, and application developers.</p>
                    <p>With Inflection 3.0, enterprise customers now have access to a complete AI platform that supercharges their employees with a virtual AI co-worker trained on their company data, policies and culture. And running it on Gaudi 3 in the Intel Tiber AI cloud offers high performance, robust software and efficiency, ultimately delivering industry-leading performance, speed and scalability in a cost-effective way for high-impact results.</p>
                    
                    <div class="d-flex justify-content-center">
                        <img class="w-75" src="/img/darshit_image/logo-tiber-ai-cloud-inflection-ai-rwd.png" alt="">    
                        <hr>
                    </div>
                    </section>

                    <section id="dk_september_2024">
                        <h4 style="font-weight: 350; margin: 2.5rem 0 11px;">Intel Launches Xeon 6 and Gaudi 3, Enabling the Next-Generation of AI Solutions</h3>
                            <p>September 24, 2024 | <a class="b_special_a1" href="">Xeon 6 with P-Cores</a>, <a class="b_special_a1" href="">Gaudi 3 AI Accelerator</a></p>
                            <p>Today, Intel launched Intel® Xeon® 6 processors with Performance cores (P-cores) and Intel® Gaudi® 3 AI accelerators, bolstering the company’s commitment to deliver powerful AI systems with optimal performance-per-watt and lower TCO.</p>
                            <p>Highlights of these two major updates to Intel’s AI-focused data center portfolio include:<br>
                                &nbsp;</p>

                                <ul>
                                    <li><strong>Intel Xeon 6 with P-cores </strong>is designed to handle compute-intensive workloads with exceptional efficiency, delivering twice the performance of its predecessor<sup>1</sup>. It features increased core count, double the memory bandwidth, and AI acceleration capabilities embedded in every core.<br>
                                    &nbsp;</li>
                                    <li><strong>Intel Gaudi 3 AI Accelerator </strong>is specifically optimized for large-scale generative AI, boasting 64 Tensor processor cores and 8 matrix multiplications engines to accelerate deep neural network computations. It includes 128 Gigabytes of HBM2e memory for training and inference and 24 200-Gigabit Ethernet ports for scalable networking, and it offers up to 20% more throughput and 2x price/performance vs NVIDIA H100 for inference of Llama 2 70B<sup>2</sup>.</li>
                                </ul>
                                <p class="mb-0">&nbsp;</p>
                                <p style="text-align:center"><a style="padding: 5px 20px;" class="dk_button" href="">Get the details</a></p>

                                <p style="text-align:center"><img alt="" height="163" src="/img/darshit_image/newsroom-intel-xeon-6-e-cores-1.jpg" width="290"></p>

                                <p style="text-align:center"><img alt="" height="340" src="/img/darshit_image/gaudi3-reference-board-wordmark-dm.png" width="340"></p>
                                <hr>
                                <p>&nbsp;</p>

                        <p class="mb-0">&nbsp;</p>
                    </section>
                    <section id="dk_sdditional">
                        <h3 style="font-weight: 350; margin: 2.5rem 0 11px;">Additional Resources</h3>
                        <p><a class="b_special_a1" href="">Intel Gaudi 2 AI Accelerator</a><br>
                            <a class="b_special_a1" href="">OpenVINO™ Toolkit on GitHub</a><br>
                            <a class="b_special_a1" href="">AI PCs from Intel</a><br>
                            <a class="b_special_a1" href="">Intel Arc Graphics</a><br>
                            <a class="b_special_a1" href="">Optimum for Intel Library</a><br>
                            <a class="b_special_a1" href="">Neural Network Compression Framework (NNCF)</a><br>
                            <a class="b_special_a1" href="">Hugging Face Transformers</a><br>
                            <a class="b_special_a1" href="">Hugging Face Diffusers</a></p>
                    </section>
                    <section id="dk_notices">
                        <h3 style="font-weight: 350; margin: 2.5rem 0 11px;">Notices and Disclaimers </h3>
                        <p>Use of the pretrained Llama 2 model is subject to compliance with third-party licenses, including the <em>Llama 2 Community License Agreement</em> (LLAMAV2). For guidance on the intended use of the Llama 2 model, what will be considered misuse and out-of-scope uses, who are the intended users, and additional terms, review and read the <a href="https://ai.meta.com/llama/license/">license instructions</a>.&nbsp;</p>
                        <p>&nbsp;</p>
                        <p>&nbsp;</p>
                    </section>
                </div>
            </div>
            <p class="mb-0">&nbsp;</p>
        </div>
    </section>

    <section style="border-top: 1px solid #d7d7d7;" class="container py-5">
        <h4 class="h6">Product and Performance Information</h4>
        <div class="disclaimer" style="font-size: 12px;"><sup>1</sup> Performance varies by use, configuration and other
            factors. Learn more at <a class="b_special_a1" href="">www.Intel.com/PerformanceIndex</a>.</div>
        </div>
    </section>

    <!-- footer -->
    <div id="footer"></div>

    <!-- script header and footer -->
    <script>
        // navbar include  
        fetch('/y_index/y_navbar.html')
            .then(response => response.text())
            .then(data => {
                document.getElementById('navbar').innerHTML = data;
            });
        // footer include 
        fetch('/y_index/y_footer.html')
            .then(response => response.text())
            .then(data => {
                document.getElementById('footer').innerHTML = data;
            });
    </script>

    <!-- nav script -->
    <script>
        document.addEventListener('DOMContentLoaded', function () {
            const nav = document.querySelector('.VK_client_app_navigation');
            const navLinks = document.querySelectorAll('.VK_ai_nav_bar a');
            const sections = document.querySelectorAll('section[id]');
            let navOffset = nav.offsetTop;

            // Add smooth scrolling to all links
            navLinks.forEach(link => {
                link.addEventListener('click', function (e) {
                    e.preventDefault();
                    document.querySelector(this.getAttribute('href')).scrollIntoView({
                        behavior: 'smooth'
                    });
                });
            });

            // Sticky Navigation
            window.addEventListener('scroll', () => {
                if (window.pageYOffset >= navOffset) {
                    nav.classList.add('VK_sticky_nav_bar');
                } else {
                    nav.classList.remove('VK_sticky_nav_bar');
                }
                // Section highlighting
                sections.forEach(section => {
                    const sectionTop = section.offsetTop - nav.clientHeight;
                    const sectionHeight = section.clientHeight;
                    console.log(sectionTop);
                    console.log(sectionHeight);
                    if (window.pageYOffset >= sectionTop && window.pageYOffset <= sectionTop + sectionHeight) {
                        navLinks.forEach(link => {
                            link.classList.remove('active');
                            if (link.getAttribute('href') === `#${section.id}`) {
                                link.classList.add('active');

                                // Ensure the active link is visible in the nav bar
                                const navBar = document.querySelector('.VK_ai_nav_bar');
                                const activeLink = document.querySelector('.VK_ai_nav_bar a.active');
                                const linkRect = activeLink.getBoundingClientRect();
                                const navBarRect = navBar.getBoundingClientRect();

                                if (linkRect.left < navBarRect.left || linkRect.right > navBarRect.right) {
                                    activeLink.scrollIntoView({ inline: 'center', behavior: 'smooth' });
                                }
                            }
                        });
                    }
                });
            });
        });
    </script>

    <!-- copy script -->
    <script>
        document.addEventListener("DOMContentLoaded", () => {
            document.querySelectorAll(".mv_copy_icon").forEach((icon) => {
                icon.addEventListener("click", async function () {
                    try {
                        const toolbar = this.closest(".mv_code_toolbar");
                        const codeBlock = toolbar.querySelector(".code-content");
                        const codeContent = codeBlock.innerText;

                        // Use Clipboard API to copy text
                        await navigator.clipboard.writeText(codeContent);

                        // Hide the copy icon and show the "Copied!" message
                        const message = toolbar.querySelector(".mv_copy_message");
                        this.classList.add("hidden"); // Hide the copy icon
                        message.classList.remove("hidden"); // Show the "Copied!" message

                        // Hide the message and show the icon again after 2 seconds
                        setTimeout(() => {
                            message.classList.add("hidden");
                            this.classList.remove("hidden");
                        }, 2000); // Adjust delay as needed

                    } catch (err) {
                        console.error('Failed to copy text: ', err);
                    }
                });
            });
        });
    </script>

</body>

</html>


