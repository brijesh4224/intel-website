<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Intel® Software News Updates</title>

    <link rel="stylesheet"
        href="/css/dk_developer_edge_iot_&_5G_optimize_fine_tuning_and_deployment_of_LLMs_on_an_ai_pc.css">

    <!-- header footer -->
    <link rel="stylesheet" href='/css/yatri.css'>

    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/css/bootstrap.min.css" rel="stylesheet"
        integrity="sha384-EVSTQN3/azprG1Anm3QDgpJLIm9Nao0Yz1ztcQTwFspd3yD65VohhpuuCOmLASjC" crossorigin="anonymous">
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/js/bootstrap.bundle.min.js"
        integrity="sha384-MrcW6ZMFYlzcLA8Nl+NtUVF0sA7MsXsP1UyJoMp4YLEuNSfAP+JcXn/tWtIaxVXM"
        crossorigin="anonymous"></script>
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.6.0/css/all.min.css" rel="stylesheet">
</head>

<style>
    .VK_ai_navigation{
        display: block !important;
    }
</style>

<body>

    <!-- header -->
    <div id="navbar"></div>

    <!-- Optimize Fine-Tuning and Deployment of LLMs on an AI PC -->
    <section>
        <div class="mv_intel_amx_bg_color">
            <div class="container">
                <div class="row mv_intel_amx_content">
                    <div class="col-md-12 col-sm-12 mv_intel_amx_item">
                        <div class="mv_intel_amx">
                            <h1><a style="color: #fff; font-weight: 300; font-size: 2.25rem;" href="">Intel® Software News Updates</a></h1>
                            <p style="margin-bottom: 20px; font-size: 1.25rem;">Get the latest info on products and services that meet your needs.</p>
                            <a class="dk_learn" href="">Sign Up</a>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="container py-5">
        <div class="row">
            <div class="d-flex justify-content-end col-xl-10 py-3 d-md-none">
                <i class="fa-solid fa-print fs-4 px-2 " style="color:#0068B5"></i>
                <i class="fa-regular fa-envelope fs-4 px-2" style="color:#0068B5"></i>
            </div>
            <div class="col-lg-3 col-md-4">
                <!-- nav -->
                <div class="VK_client_app_navigation VK_ai_navigation">
                    <div class="justify-content-center align-items-center overflow-hidden flex-nowrap mb-4">
                        <ul class="VK_ai_nav_bar list-unstyled m-0">
                            <li>
                                <a href="#dk_november_2024" class="text-dark text-decoration-none d-block">
                                    November 2024
                                </a>
                            </li>
                            <li>
                                <a href="#dk_october_2024" class="text-dark text-decoration-none d-block VK_ai_nav_link">
                                    October 2024
                                </a>
                            </li>
                            <li>
                                <a href="#dk_september_2024"
                                    class="text-dark text-decoration-none d-block VK_ai_nav_link">
                                    September 2024
                                </a>
                            </li>
                            <li>
                                <a href="#dk_August_2024" class="text-dark text-decoration-none d-block VK_ai_nav_link">
                                    August 2024
                                </a>
                            </li>
                            <li>
                                <a href="#dk_june_2024" class="text-dark text-decoration-none d-block VK_ai_nav_link">
                                    June 2024
                                </a>
                            </li>
                            <li>
                                <a href="#dk_may_2024" class="text-dark text-decoration-none d-block VK_ai_nav_link">
                                    May 2024
                                </a>
                            </li>
                            <li>
                                <a href="#dk_april_2024" class="text-dark text-decoration-none d-block VK_ai_nav_link">
                                    April 2024
                                </a>
                            </li>
                            <li>
                                <a href="#dk_march_2024" class="text-dark text-decoration-none d-block VK_ai_nav_link">
                                    March 2024
                                </a>
                            </li>
                            <li>
                                <a href="#dk_february_2024" class="text-dark text-decoration-none d-block VK_ai_nav_link">
                                    February 2024
                                </a>
                            </li>
                            <li>
                                <a href="#dk_january_2024" class="text-dark text-decoration-none d-block VK_ai_nav_link">
                                    January 2024
                                </a>
                            </li>
                            <li>
                                <a href="#dk_december_2023" class="text-dark text-decoration-none d-block VK_ai_nav_link">
                                    December 2023
                                </a>
                            </li>
                            <li>
                                <a href="#dk_november_2023" class="text-dark text-decoration-none d-block VK_ai_nav_link">
                                    November 2023 
                                </a>
                            </li>
                        </ul>
                    </div>
                </div>
                <div class="dk_things_code_main">
                    <div class="dk_things_code">
                        <h6><b>What's New</b></h6>
                        <ul class="ps-3">   
                            <li><p>The 2025.0 Intel® Software Development Tools Are Here: Marking the 5th Anniversary of oneAPI
                                   <a class="b_special_a2" href="">Read</a></p>
                            </li>
                            <li><p>Announcing General Availability of Object Storage on Intel® Tiber™ AI Cloud
                                   <a class="b_special_a2" href="">Read</a></p>
                            </li>
                            <li><p>Inflection AI Launches Enterprise AI Running on Intel® Gaudi® 3 and Intel® Tiber™ AI Cloud
                                   <a class="b_special_a2" href="">Read</a></p>
                            </li>
                            <li><p>Intel Launches Xeon 6 and Gaudi 3, Enabling the Next-Generation of AI Solutions 
                                   <a class="b_special_a2" href="">Read</a></p>
                            </li>
                            <li><p>Deliver AI Faster on Next-Gen Intel® Core™ Ultra AI PCs
                                   <a class="b_special_a2" href="">Read</a></p>
                            </li>
                        </ul>
                        <a class="dk_button" href="">Software News on Intel Newsroom</a>
                    </div>
                </div>
            </div>
            <div class="col-lg-9 col-md-8">
                <div class="d-md-flex justify-content-end col-xl-10 py-3 d-none">
                    <i class="fa-solid fa-print fs-4 px-2 " style="color:#0068B5"></i>
                    <i class="fa-regular fa-envelope fs-4 px-2" style="color:#0068B5"></i>
                </div>
                <div class="col-xl-10">
                    <p>&nbsp;</p>
                    <div style="padding-bottom: 16px;">
                    <p>&nbsp;</p>
                    <section id="dk_november_2024">
                        <h3 style="font-weight: 300;">The 2025.0 Intel® Software Development Tools Are Here, Marking the 5th Anniversary of oneAPI</h3>
                        <p class="mt-3">November 17, 2024 | <a style="color: blue;" href="">Intel® Software Development Tools</a></p>
                        <p>Today, Intel released its 2025.0 developer tools—all powered by oneAPI—marking the <a class="b_special_a1" href="">5<sup>th</sup> anniversary of the oneAPI programming model&nbsp;</a> with expanded performance optimizations and open-standards coverage to support the latest innovations in multiarchitecture, hardware-agnostic software development and deployment, edge to cloud.</p>
                        <p style="text-align:center"><a style="padding: 5px 20px;" class="dk_button" href="">Explore &amp; download</a></p>
                        <h4>3 Key Benefits</h4>
                        <ul>
                            <li><strong>More Performance on Intel Platforms</strong> – Achieve up to 3x higher GenAI performance on 6<sup>th</sup> Gen Intel® Xeon® processors (P-cores) with <a href="" style="color:blue; text-decoration:none !important">oneDNN</a>, Intel-optimized AI frameworks, and Intel® AMX<sup>1</sup>; achieve up to 2.5x better HPCG performance with MRDIMM<sup>2</sup> and <a href="" style="color:blue; text-decoration:none !important">oneMKL</a>; develop high-performance AI on the PC—including LLM development—with optimized tools to unlock the power of Intel® Core™ Ultra processors (Series 2); and improve security and encryption with <a href="" style="color:blue; text-decoration:none !important">Intel® Cryptography Primitives Library</a>.</li>
                            <li><strong>More Access to Industry-Standard Tools</strong> – Get even more from your existing development workflows using industry-leading AI frameworks and performance libraries with even more built-in Intel optimizations, including native support for PyTorch 2.5 on CPUs and GPUs; achieve optimal performance across CPU, GPU, and AI accelerators from the latest LLMs—Llama 3.2, Qwen2, Phi-3, and more—with <a href="" style="color:blue; text-decoration:none !important">Intel AI tools</a>; and streamline your software setup with our <a href="\" style="color:blue; text-decoration:none !important">toolkit selector</a> to install full kits or right-sized sub-bundles.</li>
                            <li><strong>More Hardware Choices </strong>– Enjoy increased multi-vendor, multiarchitecture support, including faster CUDA*-to-SYCL* migration with the <a href="" style="color:blue; text-decoration:none !important">Intel® DPC++ Compatibility Tool</a> that auto-migrates over 100 APIs used by popular AI, HPC, and rendering apps; achieve near-native performance on CPU and GPU for numeric compute with Intel® Distribution for Python; get 4x speedup of GPU kernels for algorithms with <a href="" style="color:blue; text-decoration:none !important">oneDPL</a>; and gain future system flexibility and prevent lock-in through cross-hardware AI-acceleration libraries, including Triton, JAX, and OpenXLA*.</li>
                        </ul>

                        <h4>The Nuts & Bolts</h4>
                        <p>Here's the collection for those interested in diving into the component-level details.</p>
                        <p><strong>Compilers</strong></p>
                        <ul>
                            <li><a href="" style="color:blue; text-decoration:none !important">Intel oneAPI DPC++/C++ Compiler</a> adds optimizations tailored for Intel® Xeon® 6 processors and Intel® Core™ Ultra processors, enables dynamic execution and flexible programming for Intel GPUs with new SYCL Bindless Textures support, streamlines development with new LLVM sanitizers to detect and troubleshoot device code issues, and enhances OpenMP standards conformance for 5.x and 6.0 plus add a more user-friendly optimization report that includes OpenMP offloading details.</li>
                            <li><a href="" style="color:blue; text-decoration:none !important">Intel® Fortran Compiler</a> adds several enhancements, including Fortran 2023 standard features such as the AT Edit Descriptor for cleaner output, conditional TEAMS construct execution with the new IF clause for OpenMP 6.0, and support for arrays of co-arrays and “standard-semantics” option to precisely control application standards compliance; updates Fortran Developer Guide and reference documentation with refreshed code samples and added support for Fortran 2018 and 2023 Fortran language features.</li>
                        </ul>
                        <p><strong>Performance Libraries</strong></p>
                        <ul>
                            <li><a href="" style="color:blue; text-decoration:none 
                            !important">Intel® oneAPI Math Kernel Library</a> (oneMKL) introduces performance optimizations across multiple domains—BLAS, LAPACK, FFT, &nbsp;and others—for developers targeting Xeon 6 processors with P-cores. It also adds significant improvements for HPC workload execution using single-precision 3D real in-place FFT on Intel® Data Center GPU Max Series and makes available new distribution models and data types for RNG using SYCL device API.</li>
                            <li><a href="" style="color:blue; text-decoration:none 
                            !important">Intel® oneAPI Data Analytics Library</a> (oneDAL) enables calculation of SHAP (SHapley Additive exPlanations) values for binary classification models, which are required for explainability random forest (RF) algorithms.</li>
                            <li><a href="" style="color:blue; text-decoration:none 
                            !important">Intel® oneAPI Deep Neural Network Library</a> (oneDNN) maximizes efficiency and performance with tailored optimizations for the latest Intel® platforms—spanning server, desktop, and mobile—including significantly faster performance for large language models (LLMs) and Scaled Dot-Product Attention subgraphs.</li>
                            <li><a href="" style="color:blue; text-decoration:none 
                            !important">Intel® oneAPI Threading Building Blocks</a> (oneTBB) improves scalability for task_group, flow_graph, and parallel_for_each so multi-threaded applications run faster; introduces try_put_and_wait experimental API for faster results using oneTBB flow graph to process overlapping messages on a shared graph.</li>
                            <li><a href="" style="color:blue; text-decoration:none 
                            !important">Intel® oneAPI Collective Communications Library</a> (oneCCL) improves workload performance and scalability with enhancements to Key-Value store, which allows workloads to scale up to an even larger number of nodes, and performance improvements to key collectives such as Allgather, Allreduce, and Reduce-scatter.</li>
                            <li><a href="" style="color:blue; text-decoration:none 
                            !important">Intel® MPI Library</a> offers a full MPI 4.0 implementation, including partitioned communication, improved error handling, and Fortran 2008 support; and improves scale-out/scale-up performance on both Xeon 6 processors with P-core pinning and Intel GPUs via optimizations for MPI_Allreduce.</li>
                            <li><a href="" style="color:blue; text-decoration:none 
                            !important">Intel® oneAPI DPC++ Library</a> (oneDPL) accelerates GPU kernels up to 4x<sup>3</sup> for algorithms including reduce, scan and many other functions. Range-based algorithms with over 20 new C++20 standard ranges and views accelerate highly parallel code execution on multiarchitecture devices.</li>
                            <li><a href="" style="color:blue; text-decoration:none 
                            !important">Intel® Integrated Performance Primitives</a> (Intel® IPP) adds CET-enabled protection (Control-flow Enforcement Technology), cutting-edge, hardware-enforced security measures that safeguard software against attacks and exploitation risks.</li>
                            <li><a href="" style="color:blue; text-decoration:none 
                            !important">Intel® Cryptography Primitives Library</a> (formerly Intel® IPP) enables developers to dispatch on Xeon 6 processors, turbocharging RSA encryption (2k, 3k, 4k) with multi-buffer capabilities and hashing with an enhanced SM3 algorithm.</li>
                            </ul>
                      <p><strong>Analyzers &amp; Debuggers</strong></p>
                      <ul>
                        <li><a href="" style="color:blue; text-decoration:none !important">Intel® DPC++ Compatibility Tool</a> saves time and effort when migrating CUDA code and CMake build script to SYCL via auto-migration of more APIs used by popular AI, HPC, and rendering applications; migrated code is easy to comprehend with SYCLcompat, easy to debug with CodePin, and runs performantly on NVIDIA GPUs.</li>
                        <li><a href="" style="color:blue; text-decoration:none !important">Intel® VTune™ Profiler</a> adds support for Intel Xeon 6 processors with P-cores and Core Ultra processors (Series 2), plus profiling support for Python 3.11, improving productivity with the ability to focus Python profiling to areas of interest and control performance data collection with <a href="" style="color:blue; text-decoration:none !important">Intel® ITT APIs</a>.</li>
                        <li><a href="" style="color:blue; text-decoration:none !important">Intel® Advisor</a> increases developers’ ability to identify bottlenecks, optimize code, and achieve peak performance on the latest Intel platforms; introduces a more adaptable kernel-matching mechanism—flexible kernel matching and XCG integration—to identify and analyze code regions relevant to specific optimization goals.</li>
                        <li><a href="" style="color:blue; text-decoration:none !important">Intel® Distribution for GDB*</a> rebases to GDB 15, staying current and aligned with the latest enhancements supporting effective application debug; adds support for Core Ultra processors (Series 2) on Windows*; and enhances developer experience, both on the command line and when using Microsoft* Visual Studio and Visual Studio Code*, by boosting the debugger performance and refining the user interface.</li>
                    </ul>
                    <p><strong>AI &amp; ML Tools, Frameworks, and Accelerated Python</strong></p>
                    <ul>
                        <li><a href="" style="color:blue; text-decoration:none !important">Intel® Distribution for Python*</a> provides drop-in, near-native performance on CPU and GPU for numeric compute; Data Parallel Extension for Python (dpnp) and Data Parallel Control (dpctl) expand compatibility, adding NumPy 2.0 support in the runtime and providing asynchronous execution of offloaded operations.</li>
                        <li><a href="" style="color:blue; text-decoration:none !important">Intel AI Tools</a> latest release ensures current and future GenAI foundation models—Llama 3.2, Qwen2, Phi-3 family, and more—perform optimally across Intel CPUs, GPUs, and AI accelerators.</li>
                        <li><a href="" style="color:blue; text-decoration:none !important">Triton</a> (open source GPU programming for neural networks) enables developers to achieve peak performance and kernel efficiency on Intel GPUs thanks to it being fully optimized for Intel Core Ultra and Data Center GPU Max Series processors and available upstream in stock PyTorch.</li>
                        <li><a href="" style="color:blue; text-decoration:none !important">Native Support for PyTorch 2.5</a> is accessible on Intel’s Data Center GPUs, Core Ultra processors, and client GPUs, where it can be used to develop on Windows with out-of-the-box support for Intel® Arc™ Graphics and Intel® Iris® XE Graphics GPUs.</li>
                        <li>Simplify enterprise GenAI adoption and reduce the time to production of hardened, trusted solutions by adopting the open platform project, <a href="" style="color:blue; text-decoration:none !important">OPEA</a>, part of LF AI &amp; Data. Now at release 1.0, OPEA continues to gain momentum with over 40 partners, including AMD, BONC, ByteDance, MongoDB, and Rivos.</li>
                        <li>Seamlessly run&nbsp;<a href="" style="color:blue; text-decoration:none !important">JAX</a>&nbsp;models on Intel® Data Center GPU Max and Flex with Intel® Extension for <a href="" style="color:blue; text-decoration:none !important">OpenXLA*</a>, an Intel-optimized PyPI package based on&nbsp;<a href="" style="color:blue; text-decoration:none !important">PJRT</a>&nbsp;plugin mechanism.</li>
                    </ul>
                    <p style="text-align:center"><a style="padding: 5px 20px;" class="dk_button" href="">Explore &amp; download the tools</a>&nbsp;</p>

                    <p><strong>Footnotes</strong></p>
                    <p><sup>1</sup> See [9A2] at intel.com/processorclaims: Intel® Xeon® 6. Results may vary.</p>
                    <p><sup>2</sup> See [9H10] at intel.com/processorclaims: Intel® Xeon® 6. Results may vary.</p>
                    <p><sup>3</sup> See <a href="" style="color:blue; text-decoration:none !important" title="">oneDPL product page</a></p>

                    <p>&nbsp;</p>
                    <h3 style="font-weight: 350; margin: 1rem 0 11px;">oneAPI Turns 5!</h3>
                    <p>November 17, 2024 | <a href="" style="color:blue; text-decoration:none !important">What is oneAPI?</a>, <a href="" style="color:blue; text-decoration:none !important">oneAPI Developer Page</a></p>
                    <h4>Happy 5<sup>th</sup> Anniversary to the open, standards-based, multiarchitecture programming initiative for accelerator architectures</h4>

                    <p><img alt="" height="141" src="/img/darshit_image/logo-oneapi.png" style="float:left" width="250"></p>
                    <p>Launched at Supercomputing 2019, the oneAPI initiative not only fostered permanent change in how the global developer ecosystem approaches heterogeneous programming, it’s become the foundation for building, optimizing, and deploying high-performance software that can run on any vendor architecture. &nbsp;</p>
                    <p>With hundreds of contributors, over 4.3 million installations, and 6.8 million developers using it via Intel® Software and AI Tools (explore the <a href="" style="color:blue; text-decoration:none !important">2025.0 release</a>), oneAPI is arguably one of the most eminent programming standards, a point further underscored by its adoption in 2023 by the Unified Acceleration (UXL) Foundation, hosted by Linux Foundation. UXL’s mission: &nbsp;to deliver an open-standard accelerator programming model that simplifies development of performant, cross-platform applications. It marks yet another critical step in driving innovation, with oneAPI as a key component.</p>
                    <p>All that in just 5 years. (Imagine what the next 5 will bring.)</p>
                    <p>If you haven’t tried oneAPI, you can get the gist of it <a href="" style="color:blue; text-decoration:none !important">here</a> and download the 2025.0 tools <a href="" style="color:blue; text-decoration:none !important">here</a>.</p>
                    <h4><strong>Celebrating oneAPI’s 5<sup>th</sup> Anniversary – What the Ecosystem is Saying</strong></h4>
                    <p>&nbsp;</p>

                    <div class="d-flex">
                        <div class="">
                            <i class="fa-solid fa-quote-left" style="font-size: 65px; color: #E5E6E6; padding-right: 15px;"></i>
                        </div>
                        <div class="">
                            <div class="mv_together_content">
                                <p class="mb-2" style="font-weight: 300;"><i>The Performance improvements achieved with Intel’s latest Xeon platform will support our efforts in wider adoption of HE, particularly in improving data analysis and insights, as well as product innovation around areas such as anti-financial crime.</i></p>
                                <p>Nikolai Larbalestier, Senior Vice President, Enterprise Architecture, NASDAQ</p>
                            </div>
                        </div>
                    </div>

                    <p>&nbsp;</p>
                    <p>&nbsp;</p>

                    <div class="d-flex">
                        <div class="">
                            <i class="fa-solid fa-quote-left" style="font-size: 65px; color: #E5E6E6; padding-right: 15px;"></i>
                        </div>
                        <div class="">
                            <div class="mv_together_content">
                                <p class="mb-2" style="font-weight: 300;"><i>oneAPI has revolutionized the way we approach heterogeneous computing by enabling seamless development across architectures. Its open, unified programming model has accelerated innovation in fields from AI to HPC, unlocking new potential for researchers and developers alike. Happy 5th Anniversary to oneAPI!</i></p>
                                <p>– Dr. Gal Oren, Asst Professor, Dept. of Computer Science, Israel Institute of Technology</p>
                            </div>
                        </div>
                    </div>

                    <p>&nbsp;</p>
                    <p>&nbsp;</p>

                    <div class="d-flex">
                        <div class="">
                            <i class="fa-solid fa-quote-left" style="font-size: 65px; color: #E5E6E6; padding-right: 15px;"></i>
                        </div>
                        <div class="">
                            <div class="mv_together_content">
                                <p class="mb-2" style="font-weight: 300;"><i>Intel's commitment to their oneAPI software stack is a testament to their developer-focused, open-standards commitment. As oneAPI celebrates its 5th Anniversary, it provides comprehensive and performant implementations of OpenMP and SYCL for CPUs and GPUs, bolstered by an ecosystem of library and tools to make the most of Intel processors.</i></p>
                                <p>– Dr. Tom Deakin, Sr. Lecturer, Head of Advanced HPC Research Group, University of Bristol</p>
                            </div>
                        </div>
                    </div>

                    <p>&nbsp;</p>
                    <p>&nbsp;</p>

                    <div class="d-flex">
                        <div class="">
                            <i class="fa-solid fa-quote-left" style="font-size: 65px; color: #E5E6E6; padding-right: 15px;"></i>
                        </div>
                        <div class="">
                            <div class="mv_together_content">
                                <p class="mb-2" style="font-weight: 300;"><i>Celebrating 5 years of oneAPI. In ExaHyPE, oneAPI has been instrumental in implementing the numerical compute kernels for hyperbolic equation systems, making a huge different in performance with SYCL providing the ideal abstraction and agnosticism for exploring these variations. This versatility enabled our team, together with Intel engineers, to publish three distinct design paradigms for our kernels.</i></p>
                                <p>– Dr. Tobias Weinzierl, Director, Institute for Data Science, Durham University</p>
                            </div>
                        </div>
                    </div>

                    <p>&nbsp;</p>
                    <p>&nbsp;</p>

                    <div class="d-flex">
                        <div class="">
                            <i class="fa-solid fa-quote-left" style="font-size: 65px; color: #E5E6E6; padding-right: 15px;"></i>
                        </div>
                        <div class="">
                            <div class="mv_together_content">
                                <p class="mb-2" style="font-weight: 300;"><i>Happy 5th Anniversary, oneAPI! We’ve been partners since the private beta program in 2019. We are currently exploring energy-efficient solutions for simulations in material science and data analysis in bioinformatics with different accelerators. For that, the components of oneAPI, its compilers with backends for various GPUs and FPGAs, oneMKL, and the performance tools VTune Profiler and Advisor, are absolutely critical.</i></p>
                                <p>– Dr. Thomas Steinke, Head of Supercomputing Department, Zuse Institute Berlin</p>
                            </div>
                        </div>
                    </div>

                    <p>&nbsp;</p>
                    <p>&nbsp;</p>

                    <div class="d-flex">
                        <div class="">
                            <i class="fa-solid fa-quote-left" style="font-size: 65px; color: #E5E6E6; padding-right: 15px;"></i>
                        </div>
                        <div class="">
                            <div class="mv_together_content">
                                <p class="mb-2" style="font-weight: 300;"><i>GROMACS was an early adopter of SYCL as a performance-portability backend, leveraging it to run on multi-vendor GPUs. Over the years, we’ve observed significant improvements in the SYCL standard and the growth of its community. This underscores the importance of open standards in computational research to drive innovation and collaboration. We look forward to continued SYCL development, which will enable enhancements in software performance and increase programmer productivity.</i></p>
                                <p>– Andrey Alekseenko, Researcher, Department of Applied Physics, KTH Royal Institute of Technology</p>
                            </div>
                        </div>
                    </div>

                    <p>&nbsp;</p>
                    <p>&nbsp;</p>

                    <div class="d-flex">
                        <div class="">
                            <i class="fa-solid fa-quote-left" style="font-size: 65px; color: #E5E6E6; padding-right: 15px;"></i>
                        </div>
                        <div class="">
                            <div class="mv_together_content">
                                <p class="mb-2" style="font-weight: 300;"><i>Using the Intel® oneAPI Base Toolkit, [GE HealthCare] successfully migrated code, which requires heavy processing and is extensively used in ultrasound diagnostics solutions, to SYCL. This is a big step forward on the way to a single, open, standards-based programming model for heterogeneous computing. The migrated code efficiently runs on different GPU platforms and achieves competitive performance.</i></p>
                                <p>– Arcady Kempinsky, Sr. Lead Software Architect, GE HealthCare</p>
                            </div>
                        </div>
                    </div>

                    <p>&nbsp;</p>
                    <p>&nbsp;</p>

                    <div class="d-flex">
                        <div class="">
                            <i class="fa-solid fa-quote-left" style="font-size: 65px; color: #E5E6E6; padding-right: 15px;"></i>
                        </div>
                        <div class="">
                            <div class="mv_together_content">
                                <p class="mb-2" style="font-weight: 300;"><i>Using Intel® oneAPI Base Toolkit, we have successfully implemented GE HealthCare's proprietary TrueFidelity DL, a deep learning image reconstruction algorithm available across much of the company's CT portfolio. The open source SYCL compiler provides near entitlement AI/DL inferencing performance for several NVIDIA GPU devices. Based on GE Healthcare experience with OpenCL software, code portability is crucial to protect our software development investment and reuse the software across different platforms and vendors.</i></p>
                                <p>– Mark Valkenburgh, Sr. Director, Global & Strategic Alliances, GE</p>
                            </div>
                        </div>
                    </div>

                    <p>&nbsp;</p>
                    <p class="mb-0">&nbsp;</p>

                    <p class="mb-0">See other testimonials:</p>
                    <ul>
                        <li><a href="" style="color:blue; text-decoration:none !important">AI &amp; Machine Learning Ecosystem Developer Resources</a></li>
                        <li><a href="" style="color:blue; text-decoration:none !important">HPC Ecosystem Developer Resources</a></li>
                    </ul>

                    </section>
                    </div>
                    <p>&nbsp;</p>

                    <section id="dk_october_2024">
                        <h4 style="font-weight: 300;">Announcing General Availability of Object Storage on Intel® Tiber™ AI Cloud</h4>
                        <p>October 17, 2024 | <a class="b_special_a1" href="">Intel® Tiber™ AI Cloud</a></p>
                        <p>Today Intel announced the availability of a new object storage service on its AI Cloud, providing scalable, durable, and cost-effective data storage that meets the demanding requirements of modern data and AI workloads.</p>
                        <p>It’s built on the powerful and open source MinIO platform, which is compatible with the S3 API (AWS’ Simple Storage Service), ensuring easy integration with existing applications and tools.</p>
                        <p style="text-align:center"><a style="padding: 5px 20px;" class="dk_button" href="">Learn more</a></p>
                        <p class="mb-0">Customer benefits include:</p>
                        <ul>
                            <li><strong>Scalability &amp; flexibility</strong> – Can handle massive data storage needs, whether gigabytes or petabytes, to ensure your storage infrastructure grows with your business.</li>
                            <li><strong>Performance</strong> – Optimized for fast data access and retrieval, ensuring data is always accessible and can be processed quickly, including AI/ML workloads.</li>
                            <li><strong>Cost-effective storage</strong> – Enables businesses of all sizes to store vast amounts of data without breaking the bank.</li>
                            <li><strong>Robust security </strong>– Incorporates encryption at rest and in transit and includes robust access controls.</li>
                            <li><strong>Easy integration</strong> – Is purpose-built to integrate seamlessly with your existing workflows and applications spanning backup and recovery, data archiving, data lake use, and more.</li>
                            <li><strong>Enhanced data management</strong> – Manage your data efficiently with features like versioning, lifecycle policies, and metadata management.</li>
                        </ul>
                        <hr>
                        <p>&nbsp;</p>
                        <p>&nbsp;</p>
                        <h3 style="font-weight: 300;">Inflection AI Launches Enterprise AI Running on Intel® Gaudi® 3 and Intel® Tiber™ AI Cloud</h3>
                        <p>October 7, 2024 | <a class="b_special_a1" href="">Inflection AI-Intel collaboration</a>, <a class="b_special_a1" href="" class="">Intel® Tiber™ AI Cloud</a></p>
                        <p><strong>New collaboration delivers turnkey AI-powered platform to drive high-impact results for enterprises</strong></p>
                        <p>Today Inflection AI and Intel announced a collaboration to accelerate the adoption and impact of AI for the world’s largest enterprises. Inflection AI is launching Inflection 3.0, an industry-first, enterprise-grade AI platform, delivering empathetic, conversational and employee-friendly AI capabilities—powered by Intel® Gaudi® 3 accelerators on Intel® Tiber™ AI Cloud—that provides the control, customization, and scalability required for complex, large-scale deployments.</p>
                        <p style="text-align:center"><a style="padding: 5px 20px;" class="dk_button" href="">Learn more</a></p>
                        
                        <p>&nbsp;</p>

                    <div class="d-flex">
                        <div class="">
                            <i class="fa-solid fa-quote-left" style="font-size: 65px; color: #E5E6E6; padding-right: 15px;"></i>
                        </div>
                        <div class="">
                            <div class="mv_together_content">
                                <p class="mb-2" style="font-weight: 300;"><i>“Together, we’re giving enterprise customers ultimate control over their AI,” said Markus Flierl, CVP of Intel Tiber Cloud Services. “By integrating Inflection AI with Intel Tiber AI Cloud and Gaudi 3, we are providing an open ecosystem of software, price and performance, and scalability,  unlocking the critical roadblocks to enterprise AI adoption and the secure, purpose-built, employee-specific, and culture-oriented AI tools customers need.”</i></p>
                                <p>Markus Flierl, CVP of Intel Tiber Cloud Services
                                </p>
                            </div>
                        </div>
                    </div>
                    <p>&nbsp;</p>
                    <p><strong>Why it matters</strong></p>
                    <p>Building an AI platform is complex, requiring extensive infrastructure; time to develop, train, and fine-tune models; and a multitude of engineers, data scientists, and application developers.</p>
                    <p>With Inflection 3.0, enterprise customers now have access to a complete AI platform that supercharges their employees with a virtual AI co-worker trained on their company data, policies and culture. And running it on Gaudi 3 in the Intel Tiber AI cloud offers high performance, robust software and efficiency, ultimately delivering industry-leading performance, speed and scalability in a cost-effective way for high-impact results.</p>
                    
                    <div class="d-flex justify-content-center">
                        <img class="w-75" src="/img/darshit_image/logo-tiber-ai-cloud-inflection-ai-rwd.png" alt="">    
                        <hr>
                    </div>
                    </section>

                    <section id="dk_september_2024">
                        <h4 style="font-weight: 350; margin: 2.5rem 0 11px;">Intel Launches Xeon 6 and Gaudi 3, Enabling the Next-Generation of AI Solutions</h3>
                            <p>September 24, 2024 | <a class="b_special_a1" href="">Xeon 6 with P-Cores</a>, <a class="b_special_a1" href="">Gaudi 3 AI Accelerator</a></p>
                            <p>Today, Intel launched Intel® Xeon® 6 processors with Performance cores (P-cores) and Intel® Gaudi® 3 AI accelerators, bolstering the company’s commitment to deliver powerful AI systems with optimal performance-per-watt and lower TCO.</p>
                            <p>Highlights of these two major updates to Intel’s AI-focused data center portfolio include:<br>
                                &nbsp;</p>

                                <ul>
                                    <li><strong>Intel Xeon 6 with P-cores </strong>is designed to handle compute-intensive workloads with exceptional efficiency, delivering twice the performance of its predecessor<sup>1</sup>. It features increased core count, double the memory bandwidth, and AI acceleration capabilities embedded in every core.<br>
                                    &nbsp;</li>
                                    <li><strong>Intel Gaudi 3 AI Accelerator </strong>is specifically optimized for large-scale generative AI, boasting 64 Tensor processor cores and 8 matrix multiplications engines to accelerate deep neural network computations. It includes 128 Gigabytes of HBM2e memory for training and inference and 24 200-Gigabit Ethernet ports for scalable networking, and it offers up to 20% more throughput and 2x price/performance vs NVIDIA H100 for inference of Llama 2 70B<sup>2</sup>.</li>
                                </ul>
                                <p class="mb-0">&nbsp;</p>
                                <p style="text-align:center"><a style="padding: 5px 20px;" class="dk_button" href="">Get the details</a></p>

                                <p style="text-align:center"><img alt="" height="163" src="/img/darshit_image/newsroom-intel-xeon-6-e-cores-1.jpg" width="290"></p>

                                <p style="text-align:center"><img alt="" height="340" src="/img/darshit_image/gaudi3-reference-board-wordmark-dm.png" width="340"></p>
                                <hr>
                                <p>&nbsp;</p>
                                <p class="mb-0">&nbsp;</p>

                                <h3 style="font-weight: 350; margin: 2.5rem 0 11px;">Seekr Launches Self-Service AI Enterprise Platform on Intel</h3>
                                <p>September 4, 2024 | <a class="b_special_a1" href="">SeekrFlow</a>, <a class="b_special_a1" href="">Intel® Tiber Developer Cloud</a></p>
                                <p><strong>Deploy trusted AI with Seekr at a superior price-performance running on Intel® Tiber™ Developer Cloud</strong></p>
                                <p><img alt="" height="113" src="/img/darshit_image/logo-seekr-rwd.png" style="float:left" width="200">Today Seekr announced its enterprise-ready platform, SeekrFlow, is now available in the Intel® Tiber™ Developer Cloud, running on high-performance, cost-efficient Intel® Gaudi® AI accelerators.</p>
                                <p>SeekrFlow is a complete end-to-end platform for training, validating, deploying, and scaling trusted enterprise AI applications, reducing the cost and complexity of AI adoption and lessening hallucinations.</p>
                                <p style="text-align:center"><a style="padding: 5px 20px;" class="dk_button" href="">Learn more</a></p>
                                <p><strong>Why it matters</strong></p>
                                <p>In short, customer advantage.</p>
                                <p>By using Intel’s cloud for developing and deploying AI at scale while also leveraging the power of SeekrFlow to run Trusted AI—and doing this all in one place—customers gain excellent price-performance, access to Intel CPUs, GPUs and AI accelerators, and flexibility with an open AI software stack.</p>
                                <hr>
                                <p>&nbsp;</p>
                                <p class="mb-0">&nbsp;</p>
                                <h3 style="font-weight: 350; margin: 2.5rem 0 11px;">Deliver AI Faster on Next-Gen Intel® Core™ Ultra AI PCs</h3>
                                <p>September 3, 2024 | <a class="b_special_a1" href="">Jumpstart AI Development</a>, <a class="b_special_a1" href="">Develop for the AI PC</a></p>
                                <p><img alt="" height="113" src="/img/darshit_image/newsroom-intel-core-ultra-processor-badge.jpg" style="float:left" width="200">Today Intel introduced next-gen Intel® Core™ Ultra processors (code-named Lunar Lake), revealing breakthroughs in efficiency, compute, and AI performance in the latest AI PCs.</p>
                                <p>ISVs, developers, AI engineers, and data scientists can take advantage of the client platform’s AI horsepower for their work—AI PCs are great for developing and optimizing models, applications, and solutions.</p>
                                <ul>
                                    <li>Simplify and accelerate AI training and inference using open source foundational models, optimized frameworks like PyTorch and TensorFlow, and <a class="b_special_a1" href="">Intel® OpenVINO™ toolkit</a>.</li>
                                    <li>Tap into the AI PC’s cutting-edge capabilities such as Intel® AVX-512 and Intel® AI Boost by leveraging <a class="b_special_a1" href="">Intel® Software Development Tools</a> to gain performance and development productivity.</li>
                                    <li>Port your existing CPU/GPU code using <a class="b_special_a1" href="">oneAPI</a> heterogeneous programming and optimize it to run faster while drawing up to 40% less power.</li>
                                </ul>
                                <p>Before the end of 2024, Intel Core Ultra processor-based platforms with integrated software development kits (SDKs) will also be available in <a class="b_special_a1" href="">Intel® Tiber Developer Cloud</a>.</p>
                                <p style="text-align:center"><a style="padding: 5px 20px;" class="dk_button" href="">Get the details</a></p>
                                <hr>
                            </section>
                            <p>&nbsp;</p>
                            <p class="mb-0">&nbsp;</p>

                    <section id="dk_August_2024">
                        <h3 style="font-weight: 350; margin: 2.5rem 0 11px;">AI Everywhere: 2024.2 Intel® Software Development & AI Tools Are Here</h3>
                        <p>Aug. 9 , 2024 | <a class="b_special_a1" href="" rel="noreferrer noopener">Intel® Software Development Tools</a>, <a class="b_special_a1"    href="">Intel® Tiber™ Developer Cloud</a>&nbsp;</p>
                        <h3 style="font-weight: 400; margin: 2.5rem 0 11px;">The fast path to performant, production-ready AI </h3>
                        <p>The latest release of Intel’s oneAPI and oneAPI-powered AI tools are tuned to help developers more easily deliver high-performance AI applications (and HPC, too) with faster time-to-solution, increased hardware choice, and improved reliability. And for building and deploying AI in a production cloud environment, check out new hardware and services in Intel® Tiber™ Developer Cloud.&nbsp;&nbsp;</p>
                        <p style="text-align:center"><a style="padding: 5px 20px;" class="dk_button" href="" rel="noreferrer noopener">Explore &amp; download</a>&nbsp;</p>
                        <h3>3 Key Benefits&nbsp;</h3>
                        <ul>
                            <li><strong>Faster, More Responsive AI </strong>– Achieve up to 2x higher GenAI performance on upcoming Intel® Xeon® 6 processors (P-cores) with <a class="b_special_a1" href="" rel="noreferrer noopener">oneDNN</a>, Intel-optimized AI frameworks, and Intel® AMX<sup>3</sup>&nbsp;and up to 1.6 better performance for workloads including analytics and media (with Xeon 6 E-Cores)<sup>4</sup>. Experience improved LLM inference throughput and scalability on AI PCs – including upcoming client processors (codenamed <a class="b_special_a1" href="" rel="noreferrer noopener">Lunar Lake</a>) for unmatched future-ready AI compute, and <a class="b_special_a1" href="" rel="noreferrer noopener">3.5x AI throughput</a> over the previous generation<sup>5</sup>. The tools support <a class="b_special_a1" href="" rel="noreferrer noopener">500+ models</a> such as Llama 3.1 and Phi-3. Deploy and scale production AI on a managed, cost-efficient infrastructure with Intel Tiber Developer Cloud.&nbsp;</li>
                            <li><strong>Greater Choice &amp; Control </strong>– Maximize performance for AI and HPC workloads on all Intel CPUs and GPUs through continued upstream optimizations to industry-standard AI frameworks. Run and deploy <a class="b_special_a1" href="" rel="noreferrer noopener">PyTorch 2.4 on Intel </a><a class="b_special_a1" href="" rel="noreferrer noopener">GPU</a><a class="b_special_a1" href="" rel="noreferrer noopener">s</a><a class="b_special_a1" href="" rel="noreferrer noopener"> </a>with minimal coding efforts for easier deployment on ubiquitous hardware. Increase application efficiency and control through optimizations in oneMKL, oneTBB, and oneDPL and enhanced SYCL* Graph capabilities in Intel® oneAPI DPC++/C++ Compiler. This release introduces broad tools support for Xeon 6 (E-cores and upcoming P-cores) and Lunar Lake processors for accelerating AI, technical, enterprise, and graphics compute workloads.&nbsp;</li>
                            <li><strong>Simplified Code Optimization </strong>– Speed up AI training and inference performance with Intel® VTune™ Profiler’s platform-aware optimizations, wider framework, and new hardware codename Grand Ridge processors. For easier CUDA* code porting to SYCL*, automatically migrate 100+ more CUDA APIs with the Intel® DPC++ Compatibility Tool; and pinpoint inconsistencies in CUDA-to-SYCL code migration using CodePin instrumentation.&nbsp;</li>
                        </ul>
                        <h3 style="font-weight: 350; margin: 2.5rem 0 11px;">The Nuts & Bolts </h3>
                        <p>For those interested in diving into component-level details, here’s the collection. Foundational tools are bundled in the <a class="b_special_a1" href="" rel="noreferrer noopener">Intel® oneAPI Base Toolkit</a> and <a class="b_special_a1" href="" rel="noreferrer noopener">Intel® HPC Toolkit</a>. For AI tools get just what you need in a <a class="b_special_a1" href="" rel="noreferrer noopener">selector tool</a>.&nbsp;&nbsp;</p>
                        <p>Compilers&nbsp;</p>
                        <ul>
                            <li><a class="b_special_a1" href="" rel="noreferrer noopener">Intel oneAPI DPC++/C++ Compiler</a> includes enhanced SYCL Graph capabilities featuring pause/resume support for better control and increased performance tuning; delivers more SYCL performance on Windows* with default context enabled; and introduces SPIR-V support and OpenCL™ query support with the latest release of the kernel compiler for greater compute kernel flexibility and optimization.&nbsp;</li>
                            <li><a class="b_special_a1" href="" rel="noreferrer noopener">Intel® Fortran Compiler</a> adds integer overflow control options (-fstrict-overflow, Qstrict-overflow[-], and -fnostrict-overflow) to ensure correct functionality; expands conformance enhancements for the latest OpenMP standards, including 5.x and 6.0, for increased thread-usage control and more powerful loop optimizations; and adds OpenMP runtime library extensions for memory management, performance, and efficiency.&nbsp;</li>
                        </ul>
                        <p>Libraries&nbsp;</p>
                        <ul>
                            <li><a class="b_special_a1" href="" rel="noreferrer noopener">Intel® Distribution for Python*</a> adds sorting and summing functions to the Data Parallel Control Library for improved productivity; and provides a new family of cumulative and improved linear algebra functions to Data Parallel Extension for NumPy* for increased performance.&nbsp;&nbsp;</li>
                            <li><a class="b_special_a1" href="" rel="noreferrer noopener">Intel® oneAPI Deep Neural Network Library</a> (oneDNN) delivers production-quality optimizations that increase performance on Intel’s AI-enhanced client processors and server platforms, and boosts AI workload efficiency with support for int8 and int4 weight decompression in matmul, which accelerates LLMs for faster insights and results.&nbsp;</li>
                            <li><a class="b_special_a1" href="" rel="noreferrer noopener">Intel® oneAPI Math Kernel Library</a> (oneMKL) introduces enhanced performance of 2D and 3D real and complex FFT targeted for Intel® Max Series GPUs.&nbsp;</li>
                            <li><a class="b_special_a1" href="" rel="noreferrer noopener">Intel® oneAPI Data Analytics Library</a> (oneDAL) extends sparsity functions across its algorithms by adding DPC++ sparse gemm and gemy primitives and sparsity support for the logloss function primitive.&nbsp;</li>
                            <li><a class="b_special_a1" href="" rel="noreferrer noopener">Intel® oneAPI DPC++ Library</a> (oneDPL) adds new C++ Standard Template Library inclusive_scan algorithm extension, which enables developers to write parallel programs for multiarchitecture devices and improves existing algorithms on Intel and other vendor GPUs.&nbsp;</li>
                            <li><a class="b_special_a1" href="" rel="noreferrer noopener">Intel® oneAPI Collective Communications Library</a> (oneCCL) introduces multiple enhancements that improve system resources utilization such as memory and I/O for even better performance.&nbsp;</li>
                            <li><a class="b_special_a1" href="" rel="noreferrer noopener">Intel® oneAPI Threading Building Blocks</a> (oneTBB) optimizes thread and multi-thread synchronization, which reduces startup latency on 5th Gen Intel Xeon processors and speeds OpenVINO™ toolkit performance up to 4x on ARM CPUs, including Apple Mac*; enhanced parallel_reduce improves data movement to avoid extra copying.&nbsp;</li>
                            <li><a class="b_special_a1" href="" rel="noreferrer noopener">Intel® Integrated Performance Primitives</a> (Intel® IPP) adds optimization patch for zlip 1.3.1 to improve compression ratio and throughput in data-compression tasks, and adds accelerated image-processing capabilities on select color-conversion functions using Intel® AVX-512 VNNI on Intel GPUs.&nbsp;</li>
                            <li><a class="b_special_a1" href="" rel="noreferrer noopener">Intel® IPP Cryptography</a> expands security across government agencies and the private sector, including NIST FIPS 140-3 compliance, and enhances data protection with optimized LMS post-quantum crypto algorithm for single buffer implementation. It also optimizes AES-GCM performance on Intel Xeon and Intel® Core™ Ultra processors via a simplified new code sample, and streamlines development with Clang 16.0 compiler support for Linux*.&nbsp;</li>
                            <li><a class="b_special_a1" href="">Intel® MPI Library</a> increases application performance on machines with multiple Network Interface Cards by enabling developers to pin specific threads to individual NICs; and adds optimizations for GPU-aware broadcasts, RMA peer-to-peer device-initiated communications, intranode thread-splits, and Infiniband* tuning for 5th Gen Intel Xeon processors.&nbsp;</li>
                        </ul>
                        <p>AI &amp; ML Tools &amp; Frameworks&nbsp;</p>
                        <ul>
                            <li>PyTorch* 2.4 now provides <a class="b_special_a1" href="" rel="noreferrer noopener">initial support for Intel® Max Series GPUs</a>, which brings Intel GPUs and the SYCL* software stack into the official PyTorch stack to help further accelerate AI workloads.&nbsp;&nbsp;</li>
                            <li><a class="b_special_a1" href="" rel="noreferrer noopener">Intel Extension for PyTorch*</a> provides better tuning for CPU performance for Bert_Large, Stable Diffusion using FP16 optimizations in eager mode. Popular&nbsp; <a class="b_special_a1" href="" rel="noreferrer noopener">LLM models </a>are optimized for Intel GPUs using <a class="b_special_a1" href="" rel="noreferrer noopener">weight-only quantization (WOQ)</a> to reduce the amount of memory access without losing accuracy while still improving performance.&nbsp;&nbsp;</li>
                            <li><a class="b_special_a1" href="" rel="noreferrer noopener">Intel Neural Compressor</a> improves INT8 and INT4 LLM model performance using SmoothQuant and WOQ algorithms in more than 15+ popular LLM quantization&nbsp; <a class="b_special_a1" href="" rel="noreferrer noopener">recipes</a>. Take advantage of in-place mode in WOQ to reduce memory footprint when running the quantization process. Improve model accuracy&nbsp; with <a class="b_special_a1" href="" rel="noreferrer noopener">AutoRound</a>, a low-bit quantization method for LLM inference to fine-tune rounding values and minmax values of weights in fewer steps. New Wanda and DSNOT pruning algorithms for PyTorch LLM help improve performance during AI inferencing while the SNIP algorithm enables scaling models on multi-card or multi-nodes (CPU).&nbsp;</li>
                        </ul>
                        <p>Analysis, Debug and Code Migration Tools&nbsp;</p>
                        <ul>
                            <li><a class="b_special_a1" href="" rel="noreferrer noopener">Intel® VTune™ Profiler</a> enables deeper insights into sub-optimal oneCCL communication, adds support for .NET8, and supports upcoming codename Grand Ridge processors. A technical preview feature allows developers to get a high-level view of potential bottlenecks in software performance analysis before exploring top-down microarchitecture metrics for deeper analysis.&nbsp;</li>
                            <li><a class="b_special_a1" href="" rel="noreferrer noopener">Intel® DPC++ Compatibility Tool</a> accelerates visual AI and imaging applications on multivendor GPUs via option-enabled migration to SYCL* image API extension; auto-compares kernel run logs and reports differences for migrated SYCL code; and can migrate 126 commonly-used CUDA APIs.&nbsp;&nbsp;</li>
                            <li><a class="b_special_a1" href="" rel="noreferrer noopener">Intel® Distribution for GDB*</a> supports Core Ultra processors on Windows*; adds Land Variable Watch Window to monitor and analyze variables and enhance application stability faster and more efficiently in VS Code*; and expands Control-flow Enforcement Technology (CET) to strengthen application security.&nbsp;</li>
                        </ul>
                        <p style="text-align:center"><a style="padding: 5px 20px;" class="dk_button" href="">Explore &amp; download the tools&nbsp;</a></p>
                        <p>Get deeper details with a developer’s perspective on new features <a class="b_special_a1" href="" rel="noreferrer noopener">in this blog</a> and in <a class="b_special_a1" href="" rel="noreferrer noopener">tools release notes</a>.&nbsp;</p>
                        <h4>Build &amp; Deploy AI Solutions at Scale in Intel Tiber Developer Cloud&nbsp;</h4>
                        <p>Develop and deploy AI models, applications, and production workloads on the latest Intel architecture using an open software stack that’s built on oneAPI and includes popular foundational models and optimized tools and frameworks.&nbsp;</p>
                        <p class="mb-0">New hardware and services—access:&nbsp;</p>
                        <ul>
                            <li>Virtual machines with Intel® Max Series GPUs&nbsp;</li>
                            <li>GenAI Jupyter notebooks with Intel® Gaudi® 2 accelerators&nbsp;</li>
                            <li>Intel® Kubernetes Service with container deployment via K8s APIs&nbsp;</li>
                            <li>Intel Xeon 6 preproduction systems in the preview environment&nbsp;</li>
                        </ul>
                        <p style="text-align:center"><a style="padding: 5px 20px;" class="dk_button" href="" rel="noreferrer noopener">Learn more &amp; get started</a>&nbsp;</p>
                        <hr>
                        <p>&nbsp;</p>
                        <p class="mb-0">&nbsp;</p>
                    </section>
                    <section id="dk_june_2024">
                        <h3 style="font-weight: 350; margin: 2.5rem 0 11px;">Intel® Gaudi® 2 Enables a Lower Cost Alternative for AI Compute and GenAI</h3>
                        <p>June 12, 2024 | <a class="b_special_a1" href="">Intel® Gaudi® 2 AI Accelerator</a>, <a class="b_special_a1"  href="">Intel® Tiber™ Developer Cloud</a></p>
                        <p><img alt="" height="141" src="/img/darshit_image/newsroom-gaudi2.jpg" style="float:left" width="250">Today, MLCommons published results of its industry AI performance benchmark: MLPerf Training v4.0. Intel’s results illustrate the choice Intel Gaudi 2 AI accelerators offer to enterprises and customers.</p>
                        <p>Intel submitted results on a large Gaudi 2 system (1,024 Gaudi 2 accelerators) trained in the Intel Tiber Developer Cloud to demonstrate the AI accelerator’s performance and scalability—it can handily train 70B-175B parameter LLMs—as well as Tiber Developer Cloud’s capacity for efficiently training MLPerf’s GPT-3 175B<sup>1</sup> parameter benchmark model.</p>
                        <p><strong>Results</strong></p>
                        <p>Gaudi 2 continues to be the only MLPerf-benchmarked alternative for AI compute to the Nvidia H100. Trained in the Tiber Developer Cloud, Intel’s GPT-3 results for time-to-train (TTT) of 66.9 minutes on an AI system of 1,024 Gaudi accelerators proves strong Gaudi 2 scaling performance on ultra-large LLMs within a developer cloud environment<sup>1</sup>.</p>
                        <p>The benchmark suite also featured a new measurement: fine-tuning the Llama 2 70B parameter model using LoRA (Low-Rank Adaptation, a fine-tuning method for large language and diffusion models). Intel’s submission achieved TTT of 78.1 minutes on eight Gaudi 2 accelerators.</p>
                        <p style="text-align:center"><a style="padding: 5px 20px;" class="dk_button" href="">Get the details</a></p>
                        <p><strong>How Gaudi provides AI value to customers</strong></p>
                        <p>High costs have priced too many enterprises out of the market. Intel Gaudi is starting to change that. At Computex, Intel announced that a standard AI kit including eight Intel Gaudi 2 accelerators with a universal baseboard (UBB) offered to system providers at $65,000 is estimated to be one-third the cost of comparable competitive platforms. A kit including eight Intel Gaudi 3 accelerators with a UBB lists at $125,000, estimated to be two-thirds the cost of comparable competitive platforms<sup>2</sup>.</p>
                        <p><strong>The value of Intel Tiber Developer Cloud</strong></p>
                        <p>Intel’s cloud provides enterprise customers a unique, managed, and cost-efficient platform to develop and deploy AI models, applications, and solutions—from single nodes to large cluster-level compute capacity. This platform increases access to Gaudi for AI compute needs—in the Tiber Developer Cloud, Intel makes its accelerators, CPUs, GPUs, an open AI software stack, and other services are easily accessible. <a class="b_special_a1" href="">Learn more</a>.</p>
                        <p class="mb-0"><strong>More resources</strong></p>
                        <ul>
                            <li><a class="b_special_a1" href="">MLCommons</a></li>
                            <li><a class="b_special_a1" href="">MLPerf 4.0 Training benchmark results</a></li>
                            <li><a class="b_special_a1" href="">Llama 2 70B: An MLPerf Inference Benchmark for LLMs</a></li>
                        </ul>
                        <p style="color: #bbb;"><sup>1</sup> MLPerf's GPT-3 measurement is conducted on a 1% representative slice of the entire model as determined by the participating companies who collectively devise the MLCommons benchmark.</p>
                        <p style="color: #bbb;"><sup>2</sup> Pricing guidance for cards and systems is for modeling purposes only. Please consult your original equipment manufacturer (OEM) of choice for final pricing. Results may vary based upon volumes and lead times.</p>
                        <p style="color: #bbb;">For workloads and configurations, visit MLCommons.org. Results may vary.</p>
                        <hr>
                        <p>&nbsp;</p>
                        <p class="mb-0">&nbsp;</p>
                    </section>

                    <section id="dk_may_2024">
                        <h3 style="font-weight: 350; margin: 2.5rem 0 11px;">More than 500 AI Models Run Optimized on Intel® Core™ Ultra Processors</h3>
                        <p>May 1, 2024 | &nbsp;<a class="b_special_a1" href="">Intel® Core™ Ultra Processor family</a></p>
                        <p><strong>Intel builds the PC industry’s most robust AI PC toolchain</strong></p>
                        <p><img alt="" height="141" src="/img/darshit_image/newsrsoom-intel-core-ultra-wafer.jpg" style="float:left" width="250">Today, Intel announced it has surpassed 500 pre-trained AI models running optimized on new Intel® Core™ Ultra processors, the industry’s premier AI PC processor available in the market.</p>
                        <p>The models span more than 20 categories of local AI inferencing: large language, diffusion, super resolution, object detection, image classification and segmentation, and computer vision, among others. They include Phi-2, Mistral, Llama, BERT, Whisper, and Stable Diffusion 1.5.</p>
                        <p>This is a landmark moment for Intel’s efforts to nurture and support the AI PC transformation—the Intel Core Ultra processor is the fastest growing AI PC processor to date; it feature new AI experiences, immersive graphics, and optimal battery life; and it’s the most robust platform for AI PC development, with more AI models, frameworks, and runtimes enabled than any other processor vendor.</p>
                        <p>All 500 models can be deployed across CPU, GPU, and NPU. They are available across popular industry sources such as OpenVINO Model Zoo, Hugging Face, ONNX Model Zoo, and PyTorch.</p>
                        <p style="text-align:center"><a style="padding: 5px 20px;" class="dk_button" href="">Get the details</a></p>
                        <p><strong>Additional resources</strong></p>
                        <ul>
                            <li><a class="b_special_a1" href="">Software development tools for Intel® Core™ Ultra Processors</a></li>
                            <li><a class="b_special_a1" href="">Get started with AI PC development</a></li>
                        </ul>
                        <hr>
                        <p>&nbsp;</p>
                        <p class="mb-0">&nbsp;</p>
                    </section>

                    
                    <section id="dk_april_2024">
                        <h3 style="font-weight: 350; margin: 2.5rem 0 11px;">Canonical Ubuntu* 24.04 LTS Release Optimized by Intel® Technology</h3>
                        <p>April 25, 2024  | &nbsp;<a class="b_special_a1" href=""> Ubuntu 24.04 LTS, Intel® QAT, </a>
                            <a class="b_special_a1" href=""> Intel® TDX</a></p>
                        <p><strong>Intel builds the PC industry’s most robust AI PC toolchain</strong></p>
                        <p><img alt="" height="141" src="/img/darshit_image/logo-canonical-color-transparent.png" style="float:left" width="250">Today, Canonical announced the release of Ubuntu* 24.04 LTS (codenamed Noble Numbat). This 10<sup>th</sup> Long Term Supported release merges advancements in performance engineering and confidential computing, including integration of Intel® QuickAssist Technology (Intel® QAT) for workload acceleration on CPU and support for Intel® Trust Domain Extensions (Intel® TDX) to strengthen confidential computing in private data centers.</p>
                        <p class="mb-0">&nbsp;</p>
                        <p>&nbsp;</p>
                        <div class="d-flex">
                            <div class="">
                                <i class="fa-solid fa-quote-left" style="font-size: 65px; color: #E5E6E6; padding-right: 15px;"></i>
                            </div>
                            <div class="">
                                <div class="mv_together_content">
                                    <p class="mb-2" style="font-weight: 300;"><i>“Ubuntu is a natural fit to enable the most advanced Intel features. Canonical and Intel have a shared philosophy of enabling performance and security at scale across platforms.”</i></p>
                                    <p>– Mark Skarpness, VP and GM of System Software Engineering, Intel Corporation
                                    </p>
                                </div>
                            </div>
                        </div>
                        <p class="">&nbsp;</p>
                        <p class="">&nbsp;</p>
                        <p class="mb-0"><strong>Release Highlights</strong><br>
                            &nbsp;</p>
                            <ul>
                                <li><strong>Performance-engineering tools </strong>– Includes the latest Linux* 6.8 kernel with improved syscall performance, nested KVM support on ppc64el, features to reduce kernel task scheduling delays, and frame pointers enabled by default on all 64-bit architectures for more complete CPU and off-CPU profiling.</li>
                                <li><strong>Intel® QAT integration </strong>– Enables accelerated encryption and compression, reduce CPU utilization, and improve networking and storage application performance on 4<sup>th</sup> Gen and new Intel® Xeon® Scalable processors.</li>
                                <li><strong>Intel® TDX support </strong>– The release seamlessly supports the extensions on both the host and guest sides, with no changes required to the application layer, greatly simplifying the porting and migration of existing workloads to a confidential computing environment.</li>
                                <li><strong>Increased developer productivity </strong>– Includes Python* 3.12, Ruby 3.2, PHP 8.3, and Go 1.22, with additional focus dedicated to the developer experience for .NET, Java, and Rust.</li>
                            </ul>  
                            <p style="text-align: center;"><a style="padding: 5px 20px;" class="dk_button" href="">Learn more</a><br>
                                <a class="b_special_a1" href="">Download Ubuntu 24.04 LTS</a><br>
                                <a class="b_special_a1" href="">Noble Numbat Deep Dive</a></p>  
                                <p><strong>About Canonical</strong></p>
                                <p><a href="http://canonical.com">Canonical</a>, the publisher of Ubuntu, provides open source security, support, and services. Its portfolio covers critical systems, from the smallest devices to the largest clouds, from the kernel to containers, from databases to AI.&nbsp;</p>
                                <hr>
                                <p>&nbsp;</p>
                                <p class="mb-0">&nbsp;</p>
                        <h3 style="font-weight: 350; margin: 2.5rem 0 11px;">Seekr Grows AI Business with Big Cost Savings on Intel® Tiber™ Developer Cloud</h3>
                        <p>April 10, 2024 | &nbsp;<a class="b_special_a1" href="">Intel® Tiber® Developer Cloud</a></p>
                        <p><strong>Trustworthy AI for content evaluation and generation at reduced costs</strong></p>
                        <p>&nbsp;</p>
                        <p><img alt="" height="98" src="/img/darshit_image/logo-seekr-rwd (1).png" style="float:left" width="175">Named one of the most innovative companies of 2024 by Fast Company, <a class="b_special_a1" href="">Seekr</a> is using the Intel® Tiber™ Developer Cloud<sup>1</sup> to build, train, and deploy advanced LLMs on cost-effective clusters running on the latest Intel hardware and software, including Intel® Gaudi® 2 AI accelerators. This strategic collaboration to accelerate AI helps Seekr meet the enormous demand for compute capacity while reducing its cloud costs and increasing workload performance.</p>
                        <p><strong>Solution overview at a glance</strong></p>
                        <p>Two of Seekr’s popular products, Flow and Align, help customers leverage AI to deploy and optimize their content and advertising strategies and to train, build, and manage the entire LLM pipeline using scalable and composable workflows.</p>
                        <p>This takes immense compute capacity which, historically, would require a significant infrastructure investment and considerable cloud costs.</p>
                        <p>By moving their production workloads from on-premise to Intel Tiber Developer Cloud, Seekr is now able to employ the power and capacity of Intel hardware and software technologies—including thousands of Intel Gaudi 2 cards—to build its LLMs, and do so at a <strong>fraction of the price</strong> and with <strong>exceptionally high performance</strong>.</p>
                        <p style="text-align:center"><a style="padding: 5px 20px;" class="dk_button" href="">Learn more →</a></p>
                        <p style="text-align:center"><a class="b_special_a1" href="">Read the case study</a> (includes benchmarks)</p>
                        <p><strong>About Seekr</strong></p>
                        <p>Seekr builds large language models (LLMs) that identify, score, and generate reliable content at scale; the company’s goal is to make the Internet safer and more valuable to use while solving their customers’ need for brand trust. Its customers include Moderna, SimpliSafe, Babbel, Constant Contact, and Indeed.</p>
                        <p style="color: #bbb;"><sup>1</sup> Formerly “Intel® Developer Cloud”; now part of the Intel® Tiber™ portfolio of enterprise business solutions.</p>
                        <hr>
                        <p class="mb-0">&nbsp;</p>
                        <p class="mb-0">&nbsp;</p>

                        <h3 style="font-weight: 350; margin: 2.5rem 0 11px;">Intel Vision 2024 Unveils Depth & Breadth of Open, Secure, Enterprise AI</h3>
                        <p>April 9, 2024</p>
                        <p>At <a class="b_special_a1" href="">Intel Vision 2024</a>, Intel CEO Pat Gelsinger introduced new strategies, next-gen products and portfolios, customers, and collaborations spanning the AI continuum.</p>
                        <p>Topping the list is Intel® Tiber™, a rich portfolio of complementary business solutions to streamline deployment of enterprise software and services across AI, cloud, edge, and trust and security; and the Intel® Gaudi® 3 accelerator, bringing more performance, openness, and choice to enterprise GenAI.</p>
                        <p>More than 20 customers showcased their leading AI solutions running on Intel® architecture, with LLM/LVM platform providers Landing.ai, Roboflow, and Seekr demonstrating how they use Intel Gaudi 2 accelerators on the Intel® Tiber™ Developer Cloud to develop, fine-tune, and deploy their production-level solutions.</p>
                        <p>Specific to collaborations, Intel announced them with Google Cloud, Thales, and Cohesivity, each of whom is leveraging Intel’s confidential computing capabilities—including Intel® Trust Domain Extensions (Intel® TDX), Intel® Software Guard Extensions (Intel® SGX), and Intel® Tiber™ Trust Services<sup>1</sup> attestation service—in their cloud instances.</p>
                        <p>A lot more was revealed, including formation of the Open Platform for Enterprise AI and Intel’s expanded AI roadmap inclusive of 6<sup>th</sup> Gen Intel® Xeon® processors with E- and P-cores and silicon for client, edge, and connectivity.</p>
                        <p style="text-align:center"><a style="padding: 5px 20px;" class="dk_button" href="">Read the press release</a>&nbsp;</p>
                        <p class="mb-0">&nbsp;</p>
                        <div class="d-flex">
                            <div class="">
                                <i class="fa-solid fa-quote-left" style="font-size: 65px; color: #E5E6E6; padding-right: 15px;"></i>
                            </div>
                            <div class="">
                                <div class="mv_together_content">
                                    <p class="mb-2" style="font-weight: 300;"><i>“We’re seeing incredible customer momentum and demonstrating how Intel’s open, scalable systems, powered by Intel Gaudi, Xeon, Core Ultra processors, Ethernet-enabled networking, and open software, unleash AI today and tomorrow, bringing AI everywhere for enterprises.”</i></p>
                                    <p>– Pat Gelsinger, CEO, Intel
                                    </p>
                                </div>
                            </div>
                        </div>
                        <p class="mb-0">&nbsp;</p>
                        <p class="mb-0">&nbsp;</p>
                        <p><strong>Highlights</strong></p>
                        <p><strong><img alt="" height="113" src="/img/darshit_image/newsroom-intel-tiber-logo.jpg" style="float:left" width="200">Intel Tiber portfolio of business solutions</strong> simplifies the deployment of enterprise software and services, including for AI, making it easier for customers to find complementary solutions that fit their needs, accelerate innovation, and unlock greater value without compromising on security, compliance, or performance. Full rollout is planned in the 3<sup>rd</sup> quarter of 2024.&nbsp;<a class="b_special_a1" href="">Explore Intel Tiber now</a>.</p>
                        <p><strong>Intel Gaudi 3 AI accelerator</strong> promises 4x more compute and 1.5x increase in memory bandwidth over Gaudi 2 and is projected to outperform NVIDIA H100 by an average of 50% on inference and 60% on power efficiency for LLaMa 7B and 70B and Falcon 180B LLMs. It will be available the 2<sup>nd</sup> quarter of 2024, including in the Intel Developer Cloud.</p>
                        <p><strong>Intel Tiber Developer Cloud’s latest release</strong> includes new hardware and services that boost compute capacity, including bare metal as a service (BMaaS) options that host large-scale clusters of Gaudi 2 accelerators and Intel® Max Series GPUs, VMs running on Gaudi 2, storage as a service (StaaS) including file storage, and Intel® Kubernetes Service for cloud-native AI workloads.</p>
                        <p>Find out how <a class="b_special_a1" href="">Seekr</a> used Intel Developer Cloud to deploy a trustworthy LLM for content generation and evaluation at scale.</p>
                        <p style="text-align:center"><a style="padding: 5px 20px;" class="dk_button" href="">Case Study</a>&nbsp;<a style="padding: 5px 20px;" class="dk_button" href="">Blog</a></p>
                        <p><strong>Confidential computing collaborations</strong> with Thales and Cohesity increase trust and security and decrease risk for enterprise customers.<br>
                            &nbsp;</p>
                            <ul>
                                <li><a class="b_special_a1" href="">Thales</a>, a leading global tech and security provider, announced a data security solution comprised of its own CipherTrust Data Security Platform on Google Cloud Platform for end-to-end data protection and Intel Tiber Trust Services for confidential computing and trusted cloud-independent attestation. This will give enterprises additional controls to protect data at rest, in transit, and in use.</li>
                            </ul>
                            <p style="text-align:center"><a class="dk_button" style="padding: 5px 20px;" href="">Press Release</a> <a class="dk_button" style="padding: 5px 20px;" href="">Blog</a><br>
                                &nbsp;</p>
                                <ul>
                                    <li><a class="b_special_a1" href="">Cohesity</a>, a leader in AI-powered data security and management, announced the addition of confidential computing capabilities to Cohesity Data Cloud. The solution leverages its Fort Knox cyber vault service for data-in-use encryption, in tandem with Intel SGX and Intel Tiber Trust Services to reduce the risk posed by bad actors accessing data while it’s being processed in main memory. This is critical for regulated industries such as financial services, healthcare, and government.</li>
                                </ul>
                                <p style="text-align:center"><a class="dk_button"
                                    style="padding: 5px 20px;" href="">Press Release</a>&nbsp;<a class="dk_button"
                                    style="padding: 5px 20px;" href="">Blog</a></p>

                                    <p><strong>Explore more</strong></p>
                                    <ul>
                                        <li><a class="b_special_a1" href="">Intel’s Enterprise Software Portfolio</a></li>
                                        <li><a class="b_special_a1" href="">Intel Tiber Developer Cloud</a></li>
                                        <li><a class="b_special_a1" href="">Intel® Confidential Computing Solutions</a></li>
                                        <li><a class="b_special_a1" href="">Intel TDX</a></li>
                                        <li><a class="b_special_a1" href="">Intel SGX</a></li>
                                    </ul>
                                    <p style="color: #bbb;"><sup>1</sup> Formerly Intel® Trust Authority</p>
                                    <hr>
                                    <p class="mb-0">&nbsp;</p>
                                    <p class="mb-0">&nbsp;</p>
                    </section>

                    <section id="dk_march_2024">
                        <h3 style="font-weight: 350; margin: 2.5rem 0 11px;">Just Released: Intel® Software Development Tools 2024.1</h3>
                        <p>March 28, 2024 | <a class="b_special_a1" href="">Intel® Software Development Tools</a></p>
                        <p><strong>Accelerate code with confidence on the world’s first SYCL 2020-conformant toolchain</strong></p>
                        <p>The 2024.1 Intel® Software Development Tools are now available and include <strong>a major milestone</strong> for accelerated computing:&nbsp; <a class="b_special_a1" href="">Intel® oneAPI DPC++/C++ Compiler</a> has become <a class="b_special_a1" href=""><strong>the</strong> <strong>first compiler</strong></a> to adopt the full <a class="b_special_a1" href="">SYCL 2020 specification</a>.</p>
                        <p>Why is this important?</p>
                        <p>Having a SYCL 2020-conformant compiler means developers can have confidence that their code is future-proof—it’s portable and reliably performant across the diversity of existing and future-emergent architectures and hardware targets, including GPUs.</p>

                        <p class="">&nbsp;</p>
                        <div class="d-flex">
                            <div class="">
                                <i class="fa-solid fa-quote-left" style="font-size: 65px; color: #E5E6E6; padding-right: 15px;"></i>
                            </div>
                            <div class="">
                                <div class="mv_together_content">
                                    <p class="mb-2" style="font-weight: 300;"><i>“SYCL 2020 enables productive heterogeneous computing today, providing the necessary controls to write high-performance parallel software for the complex reality of today’s software and hardware. Intel’s commitment to supporting open standards is again showcased as they become a SYCL 2020 Khronos Adopter.”</i></p>
                                    <p>– Dr. Tom Deakin, Lecturer in Advanced Computer Systems, University of Bristol and Chair of the SYCL Working Group for Khronos
                                    </p>
                                </div>
                            </div>
                        </div>
                        <p class="mb-0">&nbsp;</p>
                        <p class="mb-0">&nbsp;</p>
                        <p style="text-align:center"><a style="padding: 5px 20px;" class="dk_button" href="">Explore &amp; download the tools</a></p>
                        <p><strong>Key Benefits</strong><br>
                            &nbsp;</p>
                            <ol>
                                <li><strong>Code with Confidence &amp; Build Faster </strong>– Optimize parallelization for higher performance and productivity in modern C++ code via the Intel oneAPI DPC++/C++ Compiler, now with full SYCL 2020 conformance; explore new multiarchitecture features across AI, HPC, and distributed computing; and access relevant AI Tools faster and more easily with an expanded set of <a class="b_special_a1" href="">web-based selector</a> options.</li>
                                <li><strong>Accelerate AI Workloads &amp; Lower Compute Costs </strong>– Achieve performance improvements on new Intel CPUs and GPUs, including up to 14x with oneDNN on 5<sup>th</sup> Gen Intel® Xeon® Scalable processors<sup>1</sup>; 10x to 100x out-of-the-box acceleration of popular deep learning frameworks and libraries such as PyTorch* and TensorFlow*<sup>2</sup>; and faster gradient boosting inference across XGBoost, LightGBM, and CatBoost. Perform parallel computations at reduced cost with Intel® Extension for Scikit-learn* algorithms.</li>
                                <li><strong>Increase Innovation &amp; Expand Deployment</strong> – Tune once and deploy universally with more efficient code offload using SYCL Graph, now available on multiple SYCL backends in the Intel oneAPI DPC++/C++ Compiler; ease CUDA-to-SYCL migration of more CUDA APIs in the Intel® DPC++ Compatibility Tool; and explore time savings in a CodePin Tech Preview (new SYCLomatic feature) to auto-capture test vectors and start validation immediately after migration. Codeplay adds new support and capabilities to its oneAPI plugins for NVIDIA and AMD GPUs.</li>
                            </ol> 
                            <p>&nbsp;</p>
                            <p><strong>The Nuts &amp; Bolts</strong></p>
                            <p>For those of you interested in diving into the component-level deets, here’s the collection.</p>
                            <p class="mb-0">Compilers</p>
                            <ul>
                                <li><a class="b_special_a1" href="">Intel oneAPI DPC++/C++ Compiler</a> is the first compiler to achieve SYCL 2020 conformance, giving developers confidence that their SYCL code is portable and reliably performs on the diversity of current and emergent GPUs. Enhanced SYCL Graph allows for seamless integration of multi-threaded work and thread-safe functions with applications and is now available on multiple SYCL backends, enabling tune-once-deploy-anywhere capability. Expanded conformance to OpenMP 5.0, 5.1, 5.2, and TR12 language standards enables increased performance.</li>
                                <li><a class="b_special_a1" href="">Intel® Fortran Compiler</a> adds more Fortran 2023 language features including improved compatibility and interoperability between C and Fortran code, simplified trigonometric calculations, and predefined data types to improve code portability and ensure consistent behavior; makes OpenMP offload programming more productive; and increases compiler stability.</li>
                            </ul>    
                            
                            <p class="mb-0">Performance Libraries</p>
                            <ul>
                                <li><a class="b_special_a1" href="">Intel® oneAPI Math Kernel Library</a> (oneMKL) introduces new optimizations and functionalities to reduce the data transfer between Intel GPUs and the host CPU, enables the ability to reproduce results of BLAS level 3 operations on Intel GPUs from run-to-run through CNR, and streamlines CUDA-to-SYCL porting via the addition of CUDA-equivalent functions.</li>
                                <li><a class="b_special_a1" href="">Intel® oneAPI Data Analytics Library</a> (oneDAL) enables gradient boosting inference acceleration across XGBoost*, LightGBM*, and CatBoost* without sacrificing accuracy; improves clustering by adding spare K-Means support to automatically identify a subset of the features used in clustering observations.</li>
                                <li><a class="b_special_a1" href="">Intel® oneAPI Deep Neural Network Library</a> (oneDNN) adds support for GPT-Q to improve LLM performance, fp8 data type in primitives and Graph API, fp16 and bf16 scale and shift arguments for layer normalization, and opt-in deterministic mode to guarantee results are bitwise identical between runs in a fixed environment.</li>
                                <li><a class="b_special_a1" href="">Intel® oneAPI DPC++ Library</a> (oneDPL) adds a specialized sort algorithm to improve app performance on Intel GPUs, adds transform_if variant with mask input for stencil computation needs, and extends C++ STL style programming with histogram algorithms to accelerate AI and scientific computing.</li>
                                <li><a class="b_special_a1" href="">Intel® oneAPI Collective Communications Library</a> (oneCCL) optimizes all key communication patterns to speed up message passing in a memory-efficient manner and improve inference performance.</li>
                                <li><a class="b_special_a1" href="">Intel® Integrated Performance Primitives</a> expands features and support for quantum computing, cybersecurity, and data compression, including XMSS post-quantum hash-based cryptographic algorithm (tech preview), FIPS 140-3 compliance, and updated LZ4 lossless data compression algorithm for faster data transfer and reduced storage requirements in large data-intensive applications. &nbsp;</li>
                                <li><a class="b_special_a1" href="">Intel® MPI Library</a> adds new features to improve application performance and programming productivity, including GPU RMA for more efficient access to remote memory and MPI 4.0 support for Persistent Collectives and Large Counts.</li>
                            </ul>
                            <p class="mb-0">AI &amp; ML Tools &amp; Frameworks</p>
                            <ul>
                                <li><a class="b_special_a1" href="">Intel® Distribution for Python*</a> expands the ability to develop more future-proof code, including Data Parallel Control (dpctl) library’s 100% conformance to the Python Array API standard and support for NVIDIA devices; Data Parallel Extension for NumPy* enhancements for linear algebra, data manipulation, statistics, data types, plus extended support for keyword arguments; and Data Parallel Extension for Numba* improvements to kernel launch times.</li>
                                <li><a class="b_special_a1" href="">Intel Extension for Scikit-learn</a> reduces the computational costs on GPUs by making computations only on changed dataset pieces with Incremental Covariance and performing parallel GPU computations using SPMD interfaces. &nbsp;&nbsp;</li>
                                <li><a class="b_special_a1" href="">Intel® Distribution of Modin</a>* delivers significant enhancements in security and performance, including a robust security solution that ensures proactive identification and remediation of data asset vulnerabilities, and performance fixes to optimize asynchronous execution. (Note: in the 2024.2 release, developers will be able to access Modin through upstream channels.)</li>
                            </ul>
                            <p class="mb-0">Analyzers &amp; Debuggers</p>
                            <ul>
                                <li><a class="b_special_a1" href="">Intel® VTune™ Profiler</a> expands the ability to identify and understand the reasons of implicit USM data movements between Host and GPU causing performance inefficiencies in SYCL applications; adds support for .NET 8, Ubuntu* 23.10, and FreeBSD* 14.0.</li>
                                <li><a class="b_special_a1" href="">Intel® Distribution for GDB*</a> rebases to GDB 14, staying current and aligned with the latest application debug enhancements; enables the ability to monitor and troubleshoot memory access issues in real time; and adds large General Purpose Register File debug mode support for more comprehensive debugging and optimization of GPU-accelerated applications.</li>
                            </ul>
                            <p class="mb-0">Rendering &amp; Ray Tracing</p>
                            <ul>
                                <li><a class="b_special_a1" href="">Intel® Embree</a> adds enhanced error reporting for SYCL platform and driver to smooth the transition of cross-architecture code; improves stability, security, and performance capabilities.</li>
                                <li><a class="b_special_a1" href="">Intel® Open Image Denoise</a> fully supports multi-vendor denoising across all platforms: x86 and ARM CPUs (including ARM support on Windows*, Linux*, and macOS*) and Intel, NVIDIA, AMD, and Apple GPUs.</li>
                            </ul>
                            <p style="text-align:center"><a style="padding: 5px 20px;" class="dk_button" href="">Explore &amp; download the tools</a></p>
                            <p>&nbsp;</p>
                            <p class="mb-0"><strong>More Resources</strong></p>
                            <ul>
                                <li><a class="b_special_a1" href="">Intel Compiler First to Achieve SYCL 2020 Conformance</a></li>
                                <li><a class="b_special_a1" href="">A Dev's Take on the 2024.1 Release</a></li>
                                <li>Download Codeplay oneAPI plugins: <a class="b_special_a1" href="">NVIDIA GPUs</a> | <a class="b_special_a1" href="">AMD GPUs</a></li>
                            </ul>
                            <p style="color:#bbb;"><strong>Footnotes</strong></p>
                            <p style="color:#bbb;">1 <a href="" style="color:blue; text-decoration:underline 
                            !important">Performance Index:&nbsp; 5th Gen Intel Xeon Scalable Processors</a><br>
                                2 <a href="" style="color:blue; text-decoration:underline 
                                !important">Software AI accelerators: AI performance boost for free</a></p>
                            <hr>
                            <p class="mb-0">&nbsp;</p>
                            <p class="mb-0">&nbsp;</p>

                        <h3 style="font-weight: 350; margin: 2.5rem 0 11px;">Gaudi and Xeon Advance Inference Performance for Generative AI</h3>
                        <p>March 27, 2024 | <a href="" style="color:blue; text-decoration:underline 
                        !important">Intel® Developer Cloud</a>, <a href="" style="color:blue; text-decoration:underline 
                        !important">MLCommons</a></p>
                        <p><strong>Newest MLPerf results for Intel® Gaudi 2 accelerators and 5<sup>th</sup> Gen Intel® Xeon® processors demonstrate Intel is raising the bar for GenAI performance.</strong></p>
                        <p>Today, MLCommons published results of the industry standard MLPerf v4.0 benchmark for inference, inclusive of Intel’s submissions for its Gaudi 2 accelerators and 5<sup>th</sup> Gen intel Xeon Scalable processors with Intel® AMX.</p>
                        <p>As the only benchmarked alternative to NVIDIA H100* for large language and multi-model models, Gaudi 2 offers compelling price/performance, important when gauging the total cost of ownership. On the CPU side, Intel remains the only server CPU vendor to submit MLPerf results (and Xeon is the host CPU for many accelerator submissions).</p>
                        <p>Get the details and results <a class="b_special_a1" href="">here</a>.</p>
                        <p><strong>Try them in the Intel® Developer Cloud</strong></p>
                        <p>You can evaluate 5<sup>th</sup> Gen Xeon and Gaudi 2 in the Intel Developer Cloud, including running small- and large-scale training (LLM or generative AI) and inference production workloads at scale and managing AI compute resources. Explore the subscription options and sign up for an account <a class="b_special_a1" href="">here</a>.</p>
                        <hr>
                        <p class="mb-0">&nbsp;</p>
                        <p class="mb-0">&nbsp;</p>

                        <h3 style="font-weight: 350; margin: 2.5rem 0 11px;">Intel Open Sources Continuous Profiler Solution, Automating Always-On CPU Performance Analysis</h3>
                        <p>March 11, 2024 | <a href="" style="color:blue; text-decoration:underline !important">Intel® Granulate™ Cloud Optimization Software</a></p>
                        <p><strong>A continuous, autonomous way to find runtime efficiencies and simplify code optimization.</strong></p>
                        <p>Today, Intel has released to open source the Continuous Profiler optimization agent, serving as another example of the company’s open ecosystem approach to catalyze innovation and boost productivity for developers.</p>
                        <p>As its name indicates, Continuous Profiler keeps perpetual oversight on CPU utilization, thereby offering developers, performance engineers, and DevOps an always-on and autonomous way to identify application and workload runtime inefficiencies.</p>
                        <p><strong>How it works</strong></p>
                        <p>It combines multiple sampling profilers into a single flame graph, which is a unified visualization of what the CPU is spending time on and, in particular, where high latency or errors are happening in the code.</p>
                        <p><strong>Why you want it</strong></p>
                        <p>Continuous Profiler comes with numerous unique features to help teams find and fix performance errors and smooth deployment, is compatible with Intel Granulate’s continuous optimization services, can be deployed cluster-wide in minutes, and supports a range of programming languages without requiring code changes.</p>
                        <p>Additionally, it’s <a class="b_special_a1" href="">SOC2-certified</a> and held to Intel's high security standards, ensuring reliability and trust in its deployment, and is used by global companies including Snap Inc. (portfolio includes Snapchat and Bitmoji), ironSource (app business platform), and ShareChat (social networking platform).</p>
                        <p style="text-align:center"><a style="padding: 5px 20px;" class="dk_button" href="">Read the press release</a><strong> </strong></p>
                        <p class="mb-0"><strong>Learn more</strong></p>
                        <ul>
                            <li><a class="b_special_a1" href="">Get Continuous Profiler on GitHub</a></li>
                            <li><a class="b_special_a1" href="">Intel Granulate continuous profiling</a></li>
                            <li><a class="b_special_a1" href="">Request a Granulate demo</a></li>
                        </ul>
                        <hr>
                        <p class="mb-0">&nbsp;</p>
                        <p class="mb-0">&nbsp;</p>
                    </section>

                    <section id="dk_february_2024">
                        <h3 style="font-weight: 350; margin: 2.5rem 0 11px;">Intel® Software at KubeCon Europe 2024</h3>
                        <p>February 29, 2024 | <a href="" style="color:blue; text-decoration:underline !important">Intel® Software @ KubeCon Europe 2024</a></p>
                        <p><strong><img alt="" height="113" src="/img/darshit_image/software-kaleidescope-visual-clear.png" style="float:left" width="200">Intel’s Enterprise Software Portfolio enables K8s scalability for enterprise applications</strong></p>
                        <p>Meet <strong>Intel enterprise software experts</strong> at KubeCon Europe 2024 (March 19-22) and discover how you can streamline and scale deployments, reduce Kubernetes costs, and achieve end-to-end security for data.</p>
                        <p>Plus, attend the session <a class="b_special_a1" href="">Above the Clouds with American Airlines</a> to learn how one of the world’s top airlines achieved 23% cost reductions for their largest cloud-based workloads using Intel® Granulate™ software.</p>
                        <p><strong>Why Intel Enterprise Software for K8s?</strong></p>
                        <p>Because its Enterprise Software portfolio is purpose-built to accelerate cloud-native applications and solutions more efficiently, at scale, paving a faster way to AI. Meaning you can run production-level Kubernetes workloads the right way—easier to manage, secure, &nbsp;and efficiently scalable.</p>
                        <p class="mb-0">In a nutshell, you get:</p>
                        <ul>
                            <li>Optimized performance with reduced costs</li>
                            <li>Better models with streamlined workflow</li>
                            <li>Confidential computing that’s safe, secure, and compliant</li>
                        </ul>
                        <p><strong>Stop by Booth #J17</strong> to have a conversation about the depth and breadth of Intel’s enterprise software solutions.</p>
                        <p style="text-align:center"><a style="padding: 5px 20px;" class="dk_button" href="">Explore Intel @ KubeCon EU 2024&nbsp;→</a></p>
                        <p style="text-align:center"><a class="b_special_a1" href="">Book a demo&nbsp;→</a></p>
                        <p><strong>More resources</strong></p>
                        <ul>
                            <li><a class="b_special_a1" href="">Learn more and register</a></li>
                            <li><a class="b_special_a1" href="">Explore the schedule</a></li>
                        </ul>
                        <hr>
                        <p class="mb-0">&nbsp;</p>
                        <p class="mb-0">&nbsp;</p>
                        
                        <h3 style="font-weight: 350; margin: 2.5rem 0 11px;">Prediction Guard Offers Customers LLM Reliability and Security via Intel® Developer Cloud</h3>
                        <p>February 22, 2024 | <a href="" style="color:blue; text-decoration:underline !important">Intel® Developer Cloud</a></p>
                        <p><img alt="" height="113" src="/img/darshit_image/prediction-guard-article-1-figure-2.png" style="float:left" width="200">AI startup <a class="b_special_a1" href="">Prediction Guard</a> is now hosting its LLM API in the secure, private environment of <strong>Intel Developer Cloud</strong>, taking advantage of Intel’s resilient computing resources to deliver peak performance and consistency in cloud operations for its customers’ GenAI applications.</p>
                        <p>Prediction Guard’s AI platform enables enterprises to harness the full potential of large language models while mitigating security and trust issues such as hallucinations, harmful outputs, and prompt injections.</p>
                        <p>By moving to Intel Developer Cloud, the company can offer its customers significant and reliable computing power as well as the latest AI hardware acceleration, libraries, and frameworks: it’s currently leveraging Intel® Gaudi® 2 AI accelerators, the Intel/Hugging Face collaborative Optimum Habana library, and Intel extensions for PyTorch and Transformers.</p>

                        <p class="">&nbsp;</p>
                        <div class="d-flex">
                            <div class="">
                                <i class="fa-solid fa-quote-left" style="font-size: 65px; color: #E5E6E6; padding-right: 15px;"></i>
                            </div>
                            <div class="">
                                <div class="mv_together_content">
                                    <p class="mb-2" style="font-weight: 300;"><i>“For certain models, following our move to Intel Gaudi 2, we have seen our costs decrease while throughput has increased by 2x.”</i></p>
                                    <p>– Daniel Whitenack, founder, Prediction Guard
                                    </p>
                                </div>
                            </div>
                        </div>
                        <p class="mb-0">&nbsp;</p>
                        <p class="mb-0">&nbsp;</p>
                        <p style="text-align:center"><a style="padding: 5px 20px;" class="dk_button" href="">Read the case study&nbsp;→</a></p>
                        <p><strong>Learn more</strong></p>
                        <p>Prediction Guard is part of the <a class="b_special_a1" href="">Intel® Liftoff for Startups</a>, a free program for early-stage AI and machine learning startups that helps them innovate and scale across their entrepreneurial journey.</p>
                        <hr>
                        <p class="mb-0">&nbsp;</p>
                        <p class="mb-0">&nbsp;</p>

                        <h3 style="font-weight: 350; margin: 2.5rem 0 11px;">New Survey Unpacks the State of Cloud Optimization for 2024</h3>
                        <p>February 20, 2024 | <a href="" style="color:blue; text-decoration:underline !important">Intel® Granulate™ software</a></p>
                        <p>A newly released global survey conducted by the Intel® Granulate™ cloud-optimization team assessed key trends and strategies in cloud computing among DevOps, Data Engineering, and IT leaders at 413 organizations spanning multiple industries.</p>
                        <p>Among the findings, the #1 and #2 priorities for the majority of organizations (over 2/3) were cloud cost reduction and application performance improvement. And yet, 54% do not have a team dedicated to cloud-based workload optimization.</p>
                        <p class="mb-0">Get the report today to learn more trends, including:</p>
                        <ul>
                            <li>Cloud optimization priorities and objectives</li>
                            <li>Assessment of current optimization efforts</li>
                            <li>The most costly and difficult-to-optimize cloud-based workloads</li>
                            <li>Optimization tools used in the tech stack</li>
                            <li>Innovations for 2024</li>
                        </ul>
                        <p style="text-align:center">&nbsp;<a style="padding: 5px 20px;" class="dk_button" href="">Download the report&nbsp;→</a><br>
                            <a class="b_special_a1" href="">Request a demo&nbsp;→</a></p>
                            <hr>
                            <p>&nbsp;</p>
                            <p>&nbsp;</p>
                    </section>

                    <section id="dk_january_2024">
                        <h3 style="font-weight: 350; margin: 2.5rem 0 11px;">American Airlines Achieves 23% Cost Reductions for Cloud Workloads using Intel® Granulate™</h3>
                        <p>January 29, 2024 | <a href="" style="color:blue; text-decoration:underline !important">Intel® Granulate™ Cloud Optimization Software</a></p>
                        <p>American Airlines (AA) partnered with Intel Granulate to optimize its most challenging workloads, which were stored in a Databricks data lake, and also mitigate the challenges of an untenable data-management price tag.</p>
                        <p>After deploying the Intel Granulate solution, which delivers autonomous and continuous optimization with no code changes or development efforts required, AA was able to free up engineering teams to process and analyze data at optimal pace and scale, run job clusters with 37% fewer resources, and reduce costs across all clusters by 23%.</p>
                        <p style="text-align:center"><a style="padding: 5px 20px;" class="dk_button" href="">Read the case study&nbsp;→</a><br>
                            <a class="b_special_a1" href="">Request a demo&nbsp;→</a></p>
                            <p style="color: #bbb;"><em>Intel, the Intel logo, and Granulate are trademarks of Intel Corporation or its subsidiaries</em></p>
                            <hr>
                            <p class="mb-0">&nbsp;</p>
                            <p class="mb-0">&nbsp;</p>   
                            
                        <h3 style="font-weight: 350; margin: 2.5rem 0 11px;">Now Available: the First Open Source Release of Intel® SHMEM</h3>
                        <p>January 10, 2024 | <a href="" style="color:blue; text-decoration:underline !important">Intel® SHMEM</a> [GitHub]</p>    
                        <p>V1.0.0 of this open source library extends the <a class="b_special_a1" href="">OpenSHMEM</a> programming model to support Intel® Data Center GPUs using the SYCL cross-platform C++ programming environment.</p>
                        <p>OpenSHMEM (SHared MEMory) is a parallel programming library interface standard that enables Single Program Multiple Data (SPMD) programming of distributed memory systems. This allows users to write a single program that executes many copies of the program across a supercomputer or cluster of computers.</p>
                        <p><strong>Intel® SHMEM</strong> is a C++ library that enables applications to use OpenSHMEM communication APIs with device kernels implemented in SYCL. It implements a Partitioned Global Address Space (PGAS) programming model and includes a subset of host-initiated operations in the current OpenSHMEM standard and new device-initiated operations callable directly from GPU kernels.</p>
                        <p class="mb-0"><strong>Feature Highlights</strong></p>
                        <ul>
                            <li>Supports the Intel® Data Center GPU Max Series</li>
                            <li>Device and host API support for OpenSHMEM 1.5-compliant point-to-point RMA, Atomic Memory Operations, Signaling, Memory Ordering, and Synchronization Operations</li>
                            <li>Device and host API support for OpenSHMEM collective operations</li>
                            <li>Device API support for SYCL work-group and sub-group level extensions of Remote Memory Access, Signaling, Collective, Memory Ordering, and Synchronization Operations</li>
                            <li>Support of C++ template function routines replacing the C11 Generic selection routines from the OpenSHMEM spec</li>
                            <li>GPU RDMA support when configured with Sandia OpenSHMEM with suitable Libfabric providers for high-performance networking services</li>
                            <li>Choice of device memory or USM for the SHMEM Symmetric Heap</li>
                        </ul>
                        <p style="text-align:center"><a class="b_special_a1" href=""><strong>Read the blog for all the details</strong></a><br>
                            (written by 3 Sr. Software Engineers @ Intel)</p>
                            <p class="mb-0"><strong>More resources</strong></p>
                            <ul>
                                <li><a class="b_special_a1" href="">Complete Intel SHMEM spec</a></li>
                                <li><a class="b_special_a1" href="">OpenSHMEM standard</a> [PDF]</li>
                            </ul> 
                            <hr>
                            <p class="mb-0">&nbsp;</p>
                            <p class="mb-0">&nbsp;</p>   
                    </section>

                    <section id="dk_december_2023">
                        <h3 style="font-weight: 350; margin: 2.5rem 0 11px;">Updated: Codeplay oneAPI Plugins for NVIDIA GPUs</h3>
                        <p>December 23, 2023</p>
                        <p>The recent release of <a class="b_special_a1" href="">2024.0.1 Intel® Software Development Tools</a>, comprised of oneAPI and AI tools, include noteworthy additions and improvements to Codeplay’s oneAPI plugins for NVIDIA GPUs.</p>
                        <p>The highlights:</p>
                        <ul>
                            <li><strong>Bindless Images</strong> – a SYCL extension that represents a significant overhaul of the current SYCL 2020 images API.
                        
                            <ul style="list-style-type:circle">
                                <li>Users gain more flexibility over their memory and images.</li>
                                <li>Enables hardware sampling and fetching capabilities for various image types like mipmaps and new&nbsp; ways to copy images like sub-region copies.</li>
                                <li>Offers interoperability features with external graphics APIs like Vulkan and image-manipulation flexibility for integration with Blender.</li>
                            </ul>
                            </li>
                            <li><strong>SYCL Support</strong>
                            <ul style="list-style-type:circle">
                                <li><em>Non-uniform groups</em> – allows developers to perform synchronization operations across some subset of the work items in a workgroup or subgroup.</li>
                                <li><em>Peer-to-peer access</em> – in a multi-GPU system, this may result in lower latency and/or better bandwidth in memory accesses across devices.</li>
                                <li><em>Experimental version of SYCL-Graph</em> – lets developers define ahead of time the operations they want to submit to the GPU, improving performance and saving time.</li>
                            </ul>
                            </li>
                        </ul>
                        <p>Additionally, the <strong>AMD plugin </strong>continues on the path of beta and toward production release in 2024.</p>
                        <p class="mb-0"><strong>Get the plugins</strong></p>
                        <ul>
                            <li><a class="b_special_a1" href="">oneAPI for NVIDIA GPUs</a></li>
                            <li><a class="b_special_a1" href="">oneAPI for AMD GPUs</a></li>
                            <li>Open source from the repos</li>
                        </ul>
                        <p class="mb-0"><strong>More resources</strong></p>
                        <ul>
                            <li><a class="b_special_a1" href="">Read the Codeplay blog</a></li>
                            <li><a class="b_special_a1" href="">Read the blog on the 2024 Intel tools release</a></li>
                        </ul>
                        <hr>
                        <p class="mb-0">&nbsp;</p>
                        <p class="mb-0">&nbsp;</p>

                        <h3 style="font-weight: 350; margin: 2.5rem 0 11px;">Intel’s Newest AI Acceleration CPUs + 2024.0 Software Development Tools = Innovation at Scale</h3>
                        <p>December 14, 2023 | <a href="" style="color:blue; text-decoration:underline !important">AI Everywhere keynote replay</a>, <a href="" style="color:blue; text-decoration:underline !important">Intel® Software Developer Tools 2024.0</a></p>
                        <p><strong>Powering and optimizing AI workloads across data center, cloud, and edge.&nbsp;</strong></p>
                        <p>Today marks the official launch of Intel’s latest AI acceleration platforms: 5th Gen Intel® Xeon® Scalable processors (codenamed Emerald Rapids) and Intel® Core™ Ultra processors (codenamed Meteor Lake). Announced by Pat Gelsinger at the “AI Everywhere” event this morning from Nasdaq in NYC, these systems provide developers and data scientists flexibility and choice for accelerating AI innovation at scale.&nbsp;&nbsp;</p>
                        <p>And the newly released <a class="b_special_a1" href="" rel="noreferrer noopener">Intel® Software Development Tools 2024.0</a> are ready to support applications and solutions targeting these platforms.&nbsp;&nbsp;</p>
                        <p>Here are some of the ways:&nbsp;</p>
                        <p><strong>Targeting 5th Gen Intel® Xeon® Scalable processors&nbsp;</strong></p>
                        <p>The 5th Gen is an evolution of the 4th Gen Intel Xeon platform and delivers impressive performance per watt plus outsized performance and TCO in AI, database, networking, and HPC.&nbsp;</p>
                        <p class="mb-0">Intel’s 2024.0 release of optimized tools, libraries, and AI frameworks powered by oneAPI give developers the keys to maximizing application performance by activating the advanced capabilities of Xeon—both 4th and 5th Gen, as well as Intel® Xeon® CPU Max Series:&nbsp;</p>
                        <ul>
                            <li>Intel® Advanced Matrix Extensions (Intel® AMX) built-in AI accelerator&nbsp;</li>
                            <li>Intel® QuickAssist Technology (Intel® QAT) integrated workload accelerator&nbsp;</li>
                            <li>Intel® Data Streaming Accelerator (Intel® DSA) for high-bandwidth, low-latency data movement&nbsp;</li>
                            <li>Intel® In-Memory Analytics Accelerator (Intel® IAA) for very high throughput compression and decompression + primitive analytic functions&nbsp;</li>
                        </ul>
                        <p style="text-align:center"><a class="b_special_a1" href="" rel="noreferrer noopener">Software Tools for 4th &amp; 5th Gen Intel Xeon &amp; Max Series Processors</a>&nbsp;</p>
                        <p><strong>Targeting Intel Core Ultra processors&nbsp;</strong></p>
                        <p>This combined CPU, GPU, and NPU (neural processing unit) platform is built on the new Intel 4 process and delivers an optimal balance of power efficiency and performance, immersive experiences, and dedicated AI&nbsp; acceleration for gaming, content creation, and productivity on the go.&nbsp;</p>
                        <p class="mb-0">Intel’s 2024.0 release helps ISVs, developers, and professional content creators optimize gaming, content creation, AI, and media applications by putting into action the new platform’s cutting-edge features, including:&nbsp;</p>
                        <ul>
                            <li>Intel® AVX-512&nbsp;</li>
                            <li>Intel® AI Boost and inferencing acceleration&nbsp;</li>
                            <li>AV1 encode/decode&nbsp;</li>
                            <li>Ray-traced hardware acceleration&nbsp;</li>
                        </ul>
                        <p style="text-align:center"><a class="b_special_a1" href="">Software Tools for Intel Core Ultra Processor</a>&nbsp;</p>
                        <p class="mb-0"><strong>Learn more&nbsp;</strong></p>
                        <ul>
                            <li><a class="b_special_a1" href="" rel="noreferrer noopener">Watch the keynote replay</a>&nbsp;</li>
                            <li><a class="b_special_a1" href="" rel="noreferrer noopener">Read the press release</a>&nbsp;</li>
                            <li>Access a new quick start guide:&nbsp; <a class="b_special_a1" href="" rel="noreferrer noopener">Accelerate AI with Intel® AMX</a> using PyTorch and TensorFlow optimizations, and OpenVINO™ toolkit&nbsp;</li>
                        </ul>
                        <div class="">
                            <img class="w-100" src="/img/darshit_image/newsroom-ai-everywhere-event.png" alt="">
                        </div>
                        <hr>
                        <p class="mb-0">&nbsp;</p>
                        <p class="mb-0">&nbsp;</p>
                    </section>

                    <section id="dk_november_2023">
                        <h3 style="font-weight: 350; margin: 2.5rem 0 11px;">Now Available: 2024 Release of Intel Development Tools</h3>
                        <p>November 20, 2023 | <a href="" style=" font-size: 13px; color: blue; text-decoration-line: underline !important;">Intel® Software Development Tools</a></p>
                        <p><strong>Expanding Multiarchitecture Performance, Porting &amp; Productivity for AI &amp; HPC&nbsp;&nbsp;</strong></p>
                        <p>The 2024 Intel® Software Development Tools are available, bringing to developers even more multiarchitecture capabilities to accelerate and optimize AI, HPC, and rendering workloads across Intel CPUs, GPUs, and AI accelerators. Powered by oneAPI (now driven by the Unified Acceleration Foundation), the tools are based on open standards and broad coverage for C++, OpenMP, SYCL, Fortran, MPI and Python.&nbsp;</p>
                        <p style="text-align:center"><a style="padding: 5px 20px;" class="dk_button" href="">Explore &amp; download</a>&nbsp;</p>
                        <p><strong>5 Key Benefits&nbsp;</strong></p>
                        <p>(There are many, many more. See all the deets <a class="b_special_a1" href="">here</a>. Read the blog <a class="b_special_a1" href="">here</a>.)<br>
                            &nbsp;</p>
                            <ol>
                                <li><strong>Future-Ready Programming</strong> – Accelerates performance on the latest Intel GPUs including added support for Python, Modin, XGBoost, and rendering; supports upcoming 5th Gen Intel® Xeon® Scalable and Intel® Core™ Ultra CPUs; and expands AI and HPC capabilities via broadened standards coverage across multiple tools.&nbsp;</li>
                                <li><strong>AI Acceleration</strong> – Speeds up AI and machine learning on Intel CPUs and GPUs with native support through Intel-optimized PyTorch and TensorFlow frameworks and improvements to data-parallel extensions in Python.&nbsp;</li>
                                <li><strong>Vector Math Optimizations</strong> – oneMKL integrates RNG offload on target devices for HPC simulations, statistical sampling, and more on x86 CPUs and Intel GPUs, and supports FP16 datatype on Intel GPUs.&nbsp;</li>
                                <li><strong>Expanded CUDA-to-SYCL Migration</strong> – Intel® DPC++ Compatibility Tool (based on open source SYCLomatic) adds CUDA library APIs and 20 popular applications in AI, deep learning, cryptography, scientific simulation, and imaging.&nbsp;</li>
                                <li><strong>Advanced Preview Features</strong> – These evaluation previews include C++ parallel STL for easy GPU offload, dynamic device selection to optimize compute node resource usage, SYCL graph for reduced GPU offload overhead thread composability to prevent thread oversubscription in OpenMP, and profile offloaded code to NPUs.&nbsp;</li>
                            </ol>
                            <p><strong>Discover the Power of Intel CPUs &amp; GPUs + oneAPI</strong><br>
                                </p>
                                <ul>
                                    <li><a class="b_special_a1" href="">The ATLAS Experiment achieves performance gains</a> by implementing heterogeneous particle reconstruction on Intel GPUs optimized by Intel software tools, including benchmarking of SYCL and CUDA code on Intel and NVIDIA GPUs.&nbsp;</li>
                                    <li><a class="b_special_a1" href="">STAC-A2 Benchmark implementation</a> for oneAPI sets records on Intel GPUs versus NVIDIA.&nbsp;</li>
                                    <li><a class="b_special_a1" href="">VMware and Intel deliver jointly validated AI stack</a> to unlock private AI everywhere for model development and deployment.&nbsp;&nbsp;</li>
                                </ul>   
                                <hr>
                                <p class="mb-0">&nbsp;</p> 
                                <p class="mb-0">&nbsp;</p>

                         <h3 style="font-weight: 350; margin: 2.5rem 0 11px;"></strong>Intel oneAPI Software Tools &amp; Libraries Receive <em>HPCwire Reader’s Choice Award</em></h3>
                         <p>November 13, 2023</p>
                         <p>&nbsp;</p> 
                        <div class="row">
                            <div class="col-md-4 align-content-center">
                                <img class="w-100" src="/img/darshit_image/reinders-readers-award-hpcwire-2023-cropped.png.rendition.intel.web.336.252.png" alt="">
                            </div>
                            <div class="col-md-8">
                                <p>Super News!<i> HPCwire </i>just announced its editors’ and readers choice awards for 2023—and Intel oneAPI software development tools and libraries landed the top readers’ choice for the <a class="b_special_a1" href="">best HPC Programming Tool or Technology</a>. With developers and HPC experts driving new levels of HPC and AI innovation, this honor voted by them validates the importance of open, standards-based multiarchitecture programming. oneAPI has received either the editors’ or readers’ award each year since 2020. Thank you for this highly-esteemed accolade.</p>
                            </div>
                        </div>
                        <p class="mb-0">&nbsp;</p>
                        <p class="mb-0">&nbsp;</p> 
                        <hr>
                        <p class="mb-0">&nbsp;</p>
                        <p class="mb-0">&nbsp;</p> 

                        <h3 style="font-weight: 350; margin: 2.5rem 0 11px;">Accelerate & Scale AI Workloads in Intel® Developer Cloud</h3>
                        <p>September 20, 2023 | <a href="" style="color:blue; text-decoration:underline !important">Intel® Developer Cloud</a></p>
                        <p><strong>Built for developers : access the latest Intel® CPUs, GPUs, and AI accelerators</strong></p>
                        <p>As announced at Intel Innovation 2023, Intel® Developer Cloud is now publicly available. The platform offers developers, data scientists, researchers, and organizations a development environment with direct access to current and, in some cases, pre-release Intel hardware plus software services and tools, all in service to help them build, test, and optimize products and solutions for the newest tech features and bring them to market faster.</p>
                        <p>Both free and paid subscription tiers are available.</p>
                        <p style="text-align:center"><a style="padding: 5px 20px;" class="dk_button" href="">Explore &amp; sign up now&nbsp;→</a></p>
                        <p>The current complement of hardware and software includes:<br></p>
                            <ul>
                                <li>Hardware
                                <ul style="list-style-type:circle">
                                    <li>4<sup>th</sup> Gen Intel® Xeon® Scalable processors (single-node and multiarchitecture platforms and clusters)</li>
                                    <li>Intel® Xeon® CPU Max Series (for high bandwidth memory workloads)</li>
                                    <li>Intel® Data Center GPU Max Series (targeting the most demanding computing workloads)</li>
                                    <li>Habana® Gaudi®2 AI accelerator (for deep learning tasks)<br>
                                    &nbsp;</li>
                                </ul>
                                </li>
                                <li>Software &amp; Services
                                <ul style="list-style-type:circle">
                                    <li>Run small- and large-scale AI training, model optimization, and inference workloads such as Meta AI Llama 2, Databricks Dolly, and more</li>
                                    <li>Utilize small to large VMs, full systems, or clusters</li>
                                    <li>Access software tools including the Intel® oneAPI Base, HPC, and Rendering toolkits; Intel® Quantum SDK; AI tools and optimized frameworks such as Intel® OpenVINO™ toolkit, Intel-optimized TensorFlow and PyTorch, Intel® Neural Compressor, Intel® Distribution for Python, and several more</li>
                                </ul>
                                </li>
                            </ul>    
                            <p>And more will be added all the time.</p>
                            <p><a class="b_special_a1" href="">Sign up today</a>.</p>
                            <p style="    color: #bbb;"><em>Intel, the Intel logo and Gaudi are trademarks of Intel Corporation or its subsidiaries.</em></p>
                            <hr>
                            <p class="mb-0">&nbsp;</p>
                            <p class="mb-0">&nbsp;</p>

                         <h3 style="font-weight: 350; margin: 2.5rem 0 11px;">Intel Innovation 2023 At a Glance</h3>
                         <p>September 20, 2023 | <a href="" style="color:blue; text-decoration:underline !important">Intel® Innovation</a></p>
                         <p>Intel’s premier 2-day developer event was attended by nearly 2,000 attendees who participated in a wealth of sessions—keynotes from CEO Pat Gelsinger, other Intel leaders, and industry luminaries; hands-on labs; tech-insights panels; training sessions; and more—focused on the latest breakthroughs in AI spanning hardware, software, services, and advanced technologies.</p>
                         <p>There were many highlights and announcements. Here are 6 of them:<br></p>
                            <ul>
                                <li><strong>Welcome to the “Siliconomy”. </strong>Pat introduced the term in his opening—a new era of global expansion where computing is foundational to a bigger opportunity and better future for every person on the planet—and its role in a world where AI is delivering a generational shift in computing. <a class="b_special_a1" href="">Read his Siliconomy editorial</a> [PDF]</li>
                                <li><strong>Intel® Developer Cloud general availability.</strong>&nbsp;Developers can accelerate and scale AI in this free and paid development environment with access to the latest Intel hardware and software to build, test, optimize, and deploy AI and HPC applications and workloads. Includes a depth and breadth of hardware and software tools &amp; services such as 4<sup>th</sup> Gen Intel® Xeon® Scalable &amp; Max Series processors, Intel® Data Center GPU Max Series processors, Habana® Gaudi®2 AI accelerators, oneAPI tools and Intel-optimized AI tools and frameworks, and SaaS options such as Hugging Face BLOOM, Meta AI Llama 2, Databricks Dolly, and many more. <a class="b_special_a1" href="">Explore Intel Developer Cloud</a>.</li>
                                <li><strong>Intel joins the Unified Acceleration (UXL) Foundation.</strong> An evolution of the oneAPI open programming model, the Linux Foundation formed the UXL Foundation to establish cross-industry collaboration on an open-standard accelerator programming model that simplifies development of cross-platform applications. Read the blogs from <a class="b_special_a1" href="">Sanjiv Shah</a> (GM Developer Software @ Intel) and <a class="b_special_a1" href="">Rod Burns</a> (VP Ecosystem @ Codeplay)</li>
                                <li><strong>Intel® Certified Developer – MLOps Professional</strong>. This new certification program, taught by MLOps experts, uses self-paced modules, hands-on labs, and practicums to teach you how to incorporate compute awareness into the AI solution design process, maximizing performance across the AI pipeline. <a class="b_special_a1" href="">Explore the program</a>.</li>
                                <li><strong>Intel® Trust Authority</strong>. This suite of trust and security services provides customers with assurance that their apps and data are protected on the platform of their choice, including multiple cloud, edge, and on-premises environments. <a class="b_special_a1" href="">Explore Intel Trust Authority</a> | <a class="b_special_a1" href="">Start a 30-day free trial</a>.</li>
                                <li><strong>New Enterprise Software &amp; Services portfolio</strong>. The new collection is designed to solve some of the biggest enterprise challenges by delivering a scalable, sustainable tech stack with built-in, silicon-based security. Includes products that <a class="b_special_a1" href="">simplify security</a> [Intel Trust Authority], <a class="b_special_a1" href="">deliver enterprise AI with more ROI</a> [Intel Developer Cloud + Cnvrg.io], and <a class="b_special_a1" href="">improve application performance with real-time autonomous workload optimization</a> [Intel® Granulate].</li>
                            </ul>  
                            <div class="mb-3">
                                <img class="w-100" src="/img/darshit_image/newsroom-innovation-2023-gelsinger-rehearsal-2.jpg" alt="">
                            </div>  
                            <p><strong>More to explore:</strong><br></p>
                                <ul>
                                    <li><a class="b_special_a1" href="">Day 1 news release</a></li>
                                    <li><a class="b_special_a1" href="">Day 2 news release</a></li>
                                    <li><a class="b_special_a1" href="">Bringing AI Everywhere – the AI Story</a></li>
                                </ul>
                                <p style="color: #bbb;"><em>Intel, the Intel logo and Gaudi are trademarks of Intel Corporation or its subsidiaries.</em></p>
                                <hr>
                                <p class="mb-0">&nbsp;</p>    
                                <p class="mb-0">&nbsp;</p>

                        <h3 style="font-weight: 350; margin: 2.5rem 0 11px;">Unified Acceleration Foundation Forms to Drive Open, Accelerated Compute & Cross-Platform Performance</h3>
                        <p>September 19, 2023 |&nbsp;<a href="" style="color:blue; text-decoration:underline !important">Unified Acceleration Foundation</a></p>
                        <p>Today, the Linux Foundation announced the formation of the Unified Acceleration (UXL) Foundation, a cross-industry group committed to delivering an open-standard, accelerator programming model that simplifies development of performant, cross-platform applications.</p>
                        <p>An <strong>evolution of the oneAPI initiative</strong>, the UXL Foundation marks the next critical step in driving innovation and implementing the oneAPI specification across the industry. It includes a distinguished list of participating organizations and partners, including Arm, Fujitsu, Google Cloud, Imagination Technologies, Intel and Qualcomm Technologies, Inc., and Samsung. These industry leaders have come together to promote open source collaboration and development of a cross-architecture, unified programming model.</p>
                            
                        
                        <p class="">&nbsp;</p>
                        <div class="d-flex">
                            <div class="">
                                <i class="fa-solid fa-quote-left" style="font-size: 65px; color: #E5E6E6; padding-right: 15px;"></i>
                            </div>
                            <div class="">
                                <div class="mv_together_content">
                                    <p class="mb-2" style="font-weight: 300;"><i>“The Unified Acceleration Foundation exemplifies the power of collaboration and the open-source approach. By uniting leading technology companies and fostering an ecosystem of cross-platform development, we will unlock new possibilities in performance and productivity for data-centric solutions.”</i></p>
                                    <p>— Jim Zemlin, Executive Director, Linux Foundation</p>
                                </div>
                            </div>
                        </div>
                        <p class="mb-0">&nbsp;</p>
                        <p class="mb-0">&nbsp;</p>
                        <p style="text-align:center"><a style="padding: 5px 20px;" class="dk_button" href="">Learn more &amp; get involved →</a></p>
                        <p><strong>More resources</strong></p>
                        <ul>
                            <li><a class="b_special_a1" href="">Our kid’s graduating from college!</a>, <em>Sanjiv Shah, GM of Developer Software Engineering, Intel</em></li>
                            <li><a class="b_special_a1" href="">Announcing the Unified Acceleration (UXL) Foundation</a>, <em>Rod Burns, VP Ecosystem @ Codeplay Software</em></li>
                        </ul>
                        <hr>
                        <p class="mb-0">&nbsp;</p>

                        <h3 style="font-weight: 350; margin: 2.5rem 0 11px;">Pre-set AI Tool Bundles Deliver Enhanced Productivity</h3>
                        <p>August 21, 2023 |&nbsp;<a href="" style="color:blue; text-decoration:underline !important">AI Tools Selector (beta)</a></p>
                        <p><strong>Choose the tools you need with new, flexible AI tool installation service</strong></p>
                        <p>Intel's AI Tools Selector (beta) is now available, delivering streamlined package installation of popular deep learning frameworks, tools, and libraries. Install them individually or in pre-set bundles for data analytics, classic machine learning, deep learning, and inference optimization.</p>
                        <p>The tools:</p>
                        <ul>
                            <li>Deep learning frameworks:
                            <ul style="list-style-type:circle">
                                <li>Intel® Extension for TensorFlow</li>
                                <li>Intel® Extension for PyTorch</li>
                            </ul>
                            </li>
                            <li>Tools &amp; libraries:
                            <ul style="list-style-type:circle">
                                <li>Intel® Optimization for XGBoost</li>
                                <li>Intel® Optimization for Scikit-learn</li>
                                <li>Intel® Distribution of Modin</li>
                                <li>Intel® Neural Compressor</li>
                            </ul>
                            </li>
                            <li>SDKs &amp; Command-line Interfaces (CLIs):
                            <ul style="list-style-type:circle">
                                <li>cnvrg.io SDK <a class="b_special_a1" href="">v2 in Python</a></li>
                            </ul>
                            </li>
                        </ul>
                        <p>All are available via conda, pip, or Docker package managers.</p>
                        <p style="text-align:center"><a style="padding: 5px 20px;" class="dk_button" href="">Bookmark the AI Tools Selector (beta)&nbsp;→</a></p>
                        <hr>
                        <p class="mb-0">&nbsp;</p>
                        <p class="mb-0">&nbsp;</p>

                        <h3 style="font-weight: 350; margin: 2.5rem 0 11px;">Speed Up AI & Gain Productivity with Advances in Intel AI Tools</h3>
                        <p>August 11, 2023 | <a href="" style="color:blue; text-decoration:underline !important">Intel® AI Analytics Toolkit</a>, <a href="" style="color:blue; text-decoration:underline !important">oneDAL</a>, <a href="" style="color:blue; text-decoration:underline !important">oneDNN</a>, <a href="" style="color:blue; text-decoration:underline !important">oneCCL</a></p>
                        <p><strong>Calling all AI practitioners, performance engineers, and framework builders ...</strong></p>
                        <p>Speed up deep learning and machine learning on Intel® CPUs and GPUs with the just-released <strong>2023.2 Intel® AI Analytics Toolkit</strong> and <strong>updated oneAPI libraries</strong>.</p>
                        <p>The latest advances in these tools help improve performance, enhance productivity, and increase cross-platform code portability for end-to-end data science and analytics pipelines.</p>
                        <p><strong>The Highlights</strong></p>
                        <p class="mb-0"><strong>Improved Performance</strong></p>
                        <ul>
                            <li><strong>Faster deep learning</strong> with PyTorch 2.0 compatibility and experimental support for Intel® Arc™ A-Series Graphics cards with <a class="b_special_a1" href="">Intel® Extension for PyTorch</a>. If TF is more your jam, <a class="b_special_a1" href="">Intel® Extension for TensorFlow</a> makes it easier to take full advantage of new CPU optimizations to streamline execution, memory allocation, and task scheduling.</li>
                            <li><strong>Faster, classic machine learning</strong> with <a class="b_special_a1" href="">Intel® Extension for Scikit-learn</a>, now featuring CPU optimizations for extremely random trees and <a class="b_special_a1" href="">Intel® oneAPI Data Analytics Library</a> (oneDAL) distributed algorithms. For GPUs, the <a class="b_special_a1" href="">Intel® Optimization for XGBoost</a> now supports Intel® Data Center GPU Max Series.</li>
                            <li><strong>Accelerated data preprocessing</strong> with pandas 2.0 support in <a class="b_special_a1" href="">Intel® Distribution for Modin</a>, which combines faster memory-efficient operations with the scaling benefits of parallel and distributed computing.</li>
                        </ul>
                        <p class="mb-0"><strong>Enhanced Productivity</strong></p>
                        <ul>
                            <li><strong>New</strong> <strong>model compression automation</strong> in <a class="b_special_a1" href="">Intel® Neural Compressor</a> delivers streamlined quantization, easier accuracy debugging, validation for popular new LLMs, and better framework compatibility with PyTorch, TensorFlow, and ONNX-Runtime.</li>
                            <li><strong>Improved prediction accuracy for training &amp; inference </strong>with new missing values support when using daal4py Model Builders to convert gradient boosting models to use optimized algorithmic building blocks found in <a class="b_special_a1" href="">oneDAL</a>.</li>
                        </ul>
                        <p class="mb-0"><strong>Increased Portability</strong></p>
                        <ul>
                            <li><strong>Expanded hardware choice</strong> including support for ARM, NVIDIA, and AMD platforms as well as new performance optimizations in Intel CPUs and GPUs such as simpler debug and diagnostics and an experimental Graph compiler backend. All available using <a class="b_special_a1" href="">Intel® oneAPI Deep Neural Network Library</a> (oneDNN).</li>
                            <li><strong>Enhanced scaling efficiency</strong> in the cross-platform <a class="b_special_a1" href="">Intel® oneAPI Collective Communication Library</a> (oneCCL) features new support for Intel® Data Streaming Accelerator, found in 4<sup>th</sup> Gen Intel® Xeon® Scalable processors.</li>
                        </ul>

                        <p><strong>Learn More</strong></p>
                        <p style="text-align:center"><a style="padding: 5px 20px;" class="dk_button" href="">Download the Intel AI Analytics Toolkit&nbsp;→</a></p>
                        <p style="text-align:center">Explore the <a class="b_special_a1" href="">release notes</a> for more details</p>

                        <hr>
                        <p class="mb-0">&nbsp;</p>
                        <p class="mb-0">&nbsp;</p>

                        <h3 style="font-weight: 350; margin: 2.5rem 0 11px;">Advancing AI Everywhere: Intel Joins the PyTorch Foundation</h3>
                        <p>August 10, 2023 |&nbsp;<a href="" style="color:blue; text-decoration:underline !important">PyTorch Optimizations from Intel</a></p>
                        <p>Intel has just joined the PyTorch Foundation as a Premier member and will take a seat on its Governing Board to help accelerate the development and democratization of PyTorch.</p>
                        <p>According to its website, the Foundation “is a neutral home for the deep learning community to collaborate on the open source PyTorch framework and ecosystem.” Its mission is “to drive adoption of AI and deep learning tooling by fostering and sustaining an ecosystem of open source, vendor-neutral projects with PyTorch.”</p>
                        <p>It’s a good fit. Intel has been contributing to the framework since 2018, an effort precipitated by the vision of democratizing access to AI through ubiquitous hardware and open software. As an example, the newest Intel PT optimizations and features are regularly released in the <em>Intel® Extension for PyTorch</em> before they’re upstreamed into stock PyTorch. This advanced access to pre-stock-version enhancements helps data scientists and software engineers maintain a competitive edge, developing AI applications that take advantage of the latest hardware technologies.</p>
                        <p style="text-align:center"><a style="padding: 5px 20px;" class="dk_button" href="">Get the full story →</a></p>
                        <p style="text-align:center"><a class="b_special_a1" href="">Download the Intel® Extension for PyTorch</a></p>
                        <hr>
                        <p class="mb-0">&nbsp;</p>
                        <p class="mb-0">&nbsp;</p>

                        <h3 style="font-weight: 350; margin: 2.5rem 0 11px;">Proven Performance Improvements with Intel/Accenture AI Reference Kits</h3>
                        <p>July 24, 2023 |&nbsp;<a href="" style="color:blue; text-decoration:underline !important">AI Reference Kits</a></p>
                        <p><strong>These Pre-Configured Kits Simplify AI Development</strong></p>
                        <p>Likely you’ve seen mention of them here—a total of 34 free, drop-in solutions for AI workloads spanning consumer products, energy and utilities, financial services, health and life sciences, manufacturing, retail, and telecommunications.</p>
                        <p>The new news is that multiple industries are <strong>seeing measurable benefits</strong> from leveraging the code and capabilities inherent in them.</p>
                        <p>Here’s a sampling:</p>
                        <ul>
                            <li>Using the AI reference kit designed to set up interactions with an enterprise conversational AI chatbot was found to inference in batch mode <strong>up to 45% faster</strong> with oneAPI optimizations.<a class="b_special_a1" href=""><sup>1</sup>&nbsp;</a></li>
                            <li>The AI reference kit designed to automate visual quality control inspections for Life Sciences demonstrated <strong>training up to 20% faster and inferencing 55% faster</strong> for visual defect detection with oneAPI optimizations.<a class="b_special_a1" href=""><sup>2</sup></a></li>
                        </ul>
                        <p>To predict utility-asset health and deliver higher service reliability, there is an AI reference kit that provides <strong>up to a 25% increase in prediction accuracy</strong>.<a class="b_special_a1" href=""><sup>3</sup></a></p>

                        <p>&nbsp;</p>

                        <div class="align-content-center">
                            <div class="dk_diagarm_stack">
                                <!-- Convenient Software Model -->
                                <div class="mv_industry_download" data-bs-toggle="modal"
                                    data-bs-target="#mv_convenient_software_model">
                                    <i class="fa-solid fa-up-right-and-down-left-from-center"></i>
                                    <a href=""></a>
                                </div>
                                <img class="w-100"
                                    src="/img/darshit_image/newsroom-reference-kit-infographic.jpg.rendition.intel.web.1920.1080.jpg"
                                    alt="">
                            </div>
                        </div>

                        <!-- Convenient Software Model -->
                        <div class="modal" id="mv_convenient_software_model" tabindex="-1">
                            <div class="modal-dialog mv_industry_dialog">
                                <div class="modal-content mv_industry_model_content">
                                    <div class="modal-body mv_industry_body">
                                        <button type="button" data-bs-dismiss="modal" aria-label="Close">
                                            <span><i class="fa-solid fa-xmark"></i></span>
                                        </button>
                                        <div class="mv_enlarged-image">
                                            <img src="/img/darshit_image/newsroom-reference-kit-infographic.jpg.rendition.intel.web.1920.1080.jpg" alt="">
                                        </div>
                                    </div>
                                </div>
                            </div>
                        </div>

                        <p>&nbsp;</p>

                        <p style="text-align:center"><a style="padding: 5px 20px;" class="dk_button" href="">Get the full story&nbsp;→</a></p>
                        <p style="text-align:center"><a class="b_special_a1" href="">Learn more about the AI ref kits&nbsp;→</a></p>
                        <p style="text-align:center"><a class="b_special_a1" href="">Explore them all and download one or all for FREE&nbsp;→</a></p>
                        <hr>
                        <p class="mb-0">&nbsp;</p>
                        <p class="mb-0">&nbsp;</p>

                        <h3 style="font-weight: 350; margin: 2.5rem 0 11px;">Now Available: 2023.2 Release of Intel® oneAPI Tools</h3>
                        <p>July 20, 2023 | <a href="" style="color:blue; text-decoration:underline !important">Intel® oneAPI Tools</a></p>
                        <p><strong>Extending &amp; strengthening software development for open, multiarchitecture computing. </strong></p>
                        <p>The just-released 2023.2 Intel® oneAPI tools bring the freedom of multiarchitecture software development to Python, simplify migration from CUDA to open SYCL, and ramp performance on the latest GPU and CPU hardware.</p>
                        <p style="text-align:center"><a style="padding: 5px 20px;" class="dk_button" href="">Get the details →</a></p>
                        <p><strong>Benefits of the 2023.2 Release</strong></p>
                        <p class="mb-0">If you haven’t updated your tools to the oneAPI multiarchitecture versions—or if you haven’t tried them at all—here are <strong>5 benefits of doing so</strong> with this release:</p>
                        <ol>
                            <li><strong>Simplified Migration from CUDA to Performant SYCL</strong> – Developers now can experience streamlined CUDA-to-SYCL migration for popular applications such as AI, deep learning, cryptography, scientific simulation, and imaging; plus, the new release supports additional CUDA APIs, the latest version of CUDA, and FP64 for broader migration coverage.</li>
                            <li><strong>Faster &amp; More Accurate AI Inferencing</strong> – The addition of NaN (Not a Number) values support during inference streamlines pre-processing and boosts prediction accuracy for models trained on incomplete data.</li>
                            <li><strong>Accelerated AI-based Image Enhancement on GPUs</strong> – Intel® Open Image Denoise ray-tracing library now supports GPUs from Intel and other vendors, providing hardware choice for fast, high-fidelity, AI-based image enhancements.</li>
                            <li><strong>Faster Python for AI &amp; HPC </strong>– This release introduces the beta version Data Parallel Extensions for Python, extending numerical Python capabilities to GPUs for NumPy and cuPy functions, including Numba compiler support.</li>
                            <li><strong>Streamlined Method to Write Efficient Parallel Code</strong> – Intel® Fortran Compiler extends support for DO CONCURRENT Reductions, a powerful feature that allows the compiler to execute loops in parallel and significantly improve code performance while making it easier to write efficient and correct parallel code.</li>
                        </ol>
                        <p><strong>2023.2 Highlights at the Tool Level</strong></p>
                        <p class="mb-0">Compilers &amp; SYCL Support</p>
                        <ul>
                            <li><a class="b_special_a1" href="">Intel® oneAPI DPC++/C++ Compiler</a> sets the <strong>immediate command lists</strong> feature as its default, benefitting developers looking to offload computation to Intel® Data Center GPU Max Series.</li>
                            <li><a class="b_special_a1" href="">Intel® oneAPI DPC++ Library</a> (oneDPL) improves performance of the C++ STD Library sort and scan algorithms when running on Intel® GPUs; this speeds up these commonly used algorithms in C++ applications.</li>
                            <li><a class="b_special_a1" href="">Intel® DPC++ Compatibility Tool</a> (based on the open source SYCLomatic project) adds support for CUDA 12.1 and more function calls, streamlines migration of CUDA to SYCL across numerous domains (AI, cryptography, scientific simulation, imaging, and more), and adds FP64 awareness to migrated code to ensure portability across Intel GPUs with and without FP64 hardware support.</li>
                            <li><a class="b_special_a1" href="">Intel® Fortran Compiler</a> adds support for DO CONCURRENT Reduction, a powerful feature that can significantly improve the performance of code that performs reductions while making it easier to write efficient parallel code.</li>
                        </ul>

                        <p class="mb-0">AI Frameworks &amp; Libraries</p>
                        <ul>
                            <li><a class="b_special_a1" href="">Intel® Distribution of Python</a> introduces Parallel Extensions for Python (beta) which extends the CPU programming model to GPU and increases performance by enabling CPU and GPU for NumPy and CuPy.</li>
                            <li><a class="b_special_a1" href="">Intel® oneAPI Deep Neural Network Library (oneDNN)</a> enables faster training &amp; inference for AI workloads; simpler debug &amp; diagnostics; support for graph neural network (GNN) processing; and improved performance on a multitude of processors such as 4<sup>th</sup> Gen Intel® Xeon® Scalable processors and GPUs from Intel and other vendors.</li>
                            <li><a class="b_special_a1" href="">Intel® oneAPI Data Analytics Library</a> (oneDAL) Model Builder feature adds missing values for NaN support during inference, streamlining pre-processing and boosting prediction accuracy for models trained on incomplete data.</li>
                        </ul>

                        <p class="mb-0">Performance Libraries</p>
                        <ul>
                            <li><a class="b_special_a1" href="">Intel® oneAPI Math Kernel Library</a> (oneMKL) drastically reduces kernel launch time on Intel Data Center GPU Max and Flex Series processors; introduces LINPACK benchmark for GPU.</li>
                            <li><a class="b_special_a1" href="">Intel® MPI Library</a> boosts message-passing performance for 4<sup>th</sup> Gen Intel Xeon Scalable and Max CPUs, and adds important optimizations for Intel GPUs.</li>
                            <li><a class="b_special_a1" href="">Intel® oneAPI Threading Building Blocks</a> (oneTBB) algorithms and Flow Graph nodes now can accept new types of user-provided callables, resulting in a more powerful and flexible programming environment.</li>
                            <li><a class="b_special_a1" href="">Intel® Cryptography Primitives Library</a> multi-buffer library now supports XTS mode of the SM4 algorithm, benefitting developers by providing efficient and secure ways of encrypting data stored in sectors, such as storage devices.</li>
                        </ul>

                        <p class="mb-0">Analysis &amp; Debug</p>
                        <ul>
                            <li><a href="/content/www/us/en/develop/tools/oneapi/components/vtune-profiler.html">Intel® VTune™ Profiler</a> delivers insights into GPU-offload tasks and execution, improves application profiling support for BLAS level-3 routines on Intel GPUs, and identifies Intel Data Center GPU Max Series devices in the platform diagram.</li>
                            <li><a href="/content/www/us/en/developer/tools/oneapi/distribution-for-gdb.html">Intel® Distribution for GDB</a> rebases to GDB 13, staying current and aligned with the latest enhancements supporting effective application debug and debug for Shared Local Memory (SLM).</li>
                        </ul>

                        <p class="mb-0"><strong>Learn More</strong></p>
                        <ul>
                            <li><a class="b_special_a1" href="">Explore Intel oneAPI &amp; AI tools&nbsp;→</a></li>
                            <li>New to SYCL? <a class="b_special_a1" href="">Get started here&nbsp;→</a></li>
                            <li><a class="b_special_a1" href="">Bookmark the oneAPI Training Portal</a> – Learn the way you want to with learning paths, tools, on-demand training, and opportunities to share and showcase your work.</li>
                        </ul>

                        <p class="utility-text-2"><strong>Notices and Disclaimers</strong></p>
                        <p class="utility-text-2">Codeplay is an Intel company.</p>
                        <p class="utility-text-2">Performance varies by use, configuration and other factors. Learn more at <a href="" style="color:blue; text-decoration:underline !important">www.Intel.com/PerformanceIndex</a>. Results may vary.&nbsp;</p>
                        <p class="utility-text-2">Performance results are based on testing as of dates shown in configurations and may not reflect all publicly available updates. &nbsp;&nbsp;</p>
                        <p class="utility-text-2">No product or component can be absolutely secure. Your costs and results may vary.&nbsp;&nbsp;</p>
                        <p class="utility-text-2">Intel technologies may require enabled hardware, software or service activation.&nbsp;</p>
                        <p class="utility-text-2">Intel does not control or audit third-party data. You should consult other sources to evaluate accuracy</p>
                        <hr>
                        <p class="mb-0">&nbsp;</p> 
                        <p class="mb-0">&nbsp;</p>

                        <h3 style="font-weight: 350; margin: 2.5rem 0 11px;">Blender 3.6 LTS Includes Hardware-Accelerated Ray Tracing through Intel® Embree on Intel® GPUs</h3>
                        <p>June 29, 2023 | <a href="" style="color:blue; text-decoration:underline !important">Intel® Embree</a>, <a href="" style="color:blue; text-decoration:underline !important">Blender 3.6 LTS</a></p>
                        <p>Award-winning Intel® Embree is now part of the Blender 3.6 LTS release. With this addition of Intel’s high-performance ray tracing library, content creators can now take advantage of hardware-accelerated rendering for Cycles on Intel® Arc™ GPUs and Intel® Data Center Flex and Max Series GPUs while significantly decreasing rendering times with no loss in fidelity.</p>

                        <p><br>
                            The 3.6 release also includes premier AI-based denoising through Intel® Open Image Denoise. Both tools are part of the <a class="b_special_a1" href="">Intel® oneAPI Rendering Toolkit</a> (Render Kit), a set of open source rendering and ray tracing libraries for creating high-performance, high-fidelity visual experiences.<br>
                            &nbsp;</p>
                        
                            <ul>
                                <li>Read the blog (includes benchmarks)</li>
                                <li><a class="b_special_a1" href="">Watch the demo</a> [6:20]</li>
                                <li><a class="b_special_a1" href="">Download Blender 3.6 LTS</a></li>
                                <li><a class="b_special_a1" href="">Download the Render Kit</a></li>
                            </ul>
                            <hr>
                            <p class="mb-0">&nbsp;</p>
                            <p class="mb-0">&nbsp;</p>

                         <h3 style="font-weight: 350; margin: 2.5rem 0 11px;">UKAEA Makes Fusion a Reality using Intel® Hardware and oneAPI Software Tools</h3>
                         <p>June 29, 2023 | <a href="" style="color:blue; text-decoration:underline !important">Intel® oneAPI Tools</a></p>
                         <p>Using Intel® hardware, oneAPI tools, and distributed asynchronous object storage (DAOS), the UK Atomic Energy Authority and the Cambridge Open Zettascale Lab are developing the next-generation engineering tools and processes necessary to design, certify, construct, and regulate the world’s first fusion powerplants in the United Kingdom. This aligns with the U.K.’s goals to accelerate the roadmap to commercial fusion power by the early 2040s.</p>
                         <p>The UKAEA team used supercomputing and AI to design the fusion power plant virtually. It will subsequently run a number of HPC workloads on a variety of architectures, including 4th Gen Intel® Xeon® processors as well as multi-vendor GPUs and FPGAs.</p>
                         <p><strong>Why This Matters</strong><br>
                            Being able to program once for multiple hardware is key. By using oneAPI open, standards-based, multiarchitecture programming, the UKAEA team can overcome barriers of code portability and deliver performance and development productivity without vendor lock-in.</p>
                        <p class="mb-0"><strong>Learn more:</strong></p>  
                        <ul>
                            <li><a class="b_special_a1" href="">Read the press release</a></li>
                            <li><a class="b_special_a1" href="">Read the case study</a></li>
                        </ul> 
                        
                        <p class="mb-0"><strong>Resources:</strong></p>
                        <ul>
                            <li><a class="b_special_a1" href="">Learn about oneAPI</a></li>
                            <li><a class="b_special_a1" href="">Learn about UKAEA</a></li>
                            <li><a class="b_special_a1" href="">Intel® oneAPI Tools</a></li>
                            <li><a class="b_special_a1" href="">4<sup>th</sup> Gen Intel Xeon Scalable Processors</a></li>
                        </ul>
                        <hr>
                        <p>&nbsp;</p>
                        <p>&nbsp;</p>

                        <h3 style="font-weight: 350; margin: 2.5rem 0 11px;">Introducing the oneAPI Construction Kit</h3>
                        <p>June 5, 2023</p>
                        <p><strong>Codeplay brings open, standards-based SYCL programming to new, custom, and specialist hardware</strong></p>
                        <p>Today Codeplay announced the latest extension of the oneAPI ecosystem with an open source project that allows code written in SYCL to run on custom architectures for HPC and AI.</p>
                        <p>The <strong>oneAPI Construction Kit</strong> includes a reference implementation for RISC-V vector processors but can be adapted for a range of processors, making it easy to access a wealth of supported SYCL libraries.</p>
                        <p>A benefit for users of custom architectures, rather than having to learn a new custom language, they can instead use SYCL to write high-performance applications efficiently – using a single codebase that works across multiple architectures. This means less time spent on porting efforts and maintaining separate codebases for different architectures, and more time for innovation.</p>
                        <p class="mb-0"><strong>What’s Inside the New Kit:</strong></p>
                        <ul>
                            <li>A framework for bringing oneAPI support to new and innovative hardware – such as specialized AI accelerators</li>
                            <li>Support for x86, ARM, and RISC-V targets</li>
                            <li>Documentation</li>
                            <li>Reference Design</li>
                            <li>Tutorials</li>
                            <li>Modular Software Components</li>
                        </ul>

                        <p class="mb-0"><strong>Learn More &amp; Get It</strong></p>
                        <ul>
                            <li>Get it free at <a class="b_special_a1" href="">developer.codeplay.com</a></li>
                            <li><a class="b_special_a1" href="">Watch the demo</a> [2:32]</li>
                            <li><a class="b_special_a1" href="">Read the blog</a> from Codeplay Principal SW Engineer, Colin Davidson</li>
                            <li>Get the <a class="b_special_a1" href="">documentation</a></li>
                        </ul>
                        <hr>
                        <p class="mb-0">&nbsp;</p>
                        <p class="mb-0">&nbsp;</p>
                         
                        <h3 style="font-weight: 350; margin: 2.5rem 0 11px;">Intel Delivers AI-Accelerated HPC Performance, Uplifted by oneAPI</h3>
                        <p>May 22, 2023 |&nbsp;<a href="" style="color:blue; text-decoration:underline !important">Intel® oneAPI Tools</a></p>
                        <p><strong>ISC’23 takeaway: Broadest, most open HPC+AI portfolio powers performance, generative AI for science</strong></p>
                        <p>Intel’s keynote at <a class="b_special_a1" href="">International Super Computing 2023</a> underscored how the company is making multiarchitecture programming easier for an open ecosystem, as well as driving competitive performance for diverse HPC and AI workloads based on a broad product portfolio of CPUs, GPUs, AI accelerators, and oneAPI software.</p>
                        <p>Here are the highlights.</p>
                        <p class="mb-0">Hardware:</p>
                        <ul>
                            <li>Independent software vendor Ansys showed the Intel® Data Center GPU Max Series outperforms NVIDIA H100 by 50% on AI-accelerated HPC applications, in addition to an average improvement of 30% over H100 on diverse workloads.<sup>*</sup></li>
                            <li>The Habana Gaudi 2 deep learning accelerator delivers up to 2x faster AI performance over NVIDIA A100 for DL training and inference.<sup>*</sup></li>
                            <li>Intel® Xeon CPUs (including the Max Series and 4<sup>th</sup> Gen) deliver, respectively, 65% speedup over AMD Genoa for bandwidth-limited problems and 50% average speed-up over AMD Milan.<sup>*</sup></li>
                        </ul>
                        <p class="mb-0">Software:</p>
                        <ul>
                            <li>Worldwide, about 90% of all developers benefit from or use software developed for or optimized by Intel.<sup>*</sup></li>
                            <li>oneAPI has been demonstrated on diverse CPU, GPU, FPGA and AI silicon from multiple hardware providers, addressing the challenges of single-vendor accelerated programming models.</li>
                            <li>New features in the latest oneAPI tools—such as OpenMP GPU offload, extended support for OpenMP and Fortran, and optimized TensorFlow and PyTorch frameworks and AI tools—unleash the capabilities of Intel’s most advanced HPC and AI CPUs and GPUs.</li>
                            <li>Real-time, ray-traced scientific visualization with hardware acceleration is now available on Intel GPUs, and AI-based denoising completes in milliseconds.</li>
                        </ul>
                        <p>The oneAPI SYCL standard implementation has been shown to outperform NVIDIA native system languages; case in point: DPEcho SYCL code run on Max Series GPU outperformed by 48% the same CUDA code run on NVIDIA H100.</p>

                        <p class="">&nbsp;</p>
                        <div class="d-flex">
                            <div class="">
                                <i class="fa-solid fa-quote-left" style="font-size: 65px; color: #E5E6E6; padding-right: 15px;"></i>
                            </div>
                            <div class="">
                                <div class="mv_together_content">
                                    <p class="mb-2" style="font-weight: 300;"><i>ntel is committed to serving the HPC and AI community with products that help customers and end-users make breakthrough discoveries faster. Our product portfolio spanning Xeon Max Series CPUs, Max Series GPUs, 4th Gen Xeon and Gaudi 2 are outperforming the competition on a variety of workloads, offering energy and total cost of ownership advantages, democratizing AI and providing choice, openness and flexibility.</i></p>
                                    <p>Jeff McVeigh, Intel corporate VP and GM of the Super Compute Group</p>
                                </div>
                            </div>
                        </div>
                        <p class="mb-0">&nbsp;</p>
                        <p class="mb-0">&nbsp;</p>
                        <p style="text-align:center"><a style="padding: 5px 20px;" class="dk_button" href="">Read the full story</a></p>
                        <p style="text-align:center">&nbsp;</p>
                        <p class="utility-text-2"><a class="b_special_a1" href="">*See press release and disclaimers and configurations for details.&nbsp;</a></p>
                        <hr>
                        <p class="mb-0">&nbsp;</p>
                        <p class="mb-0">&nbsp;</p>

                        <h3 style="font-weight: 350; margin: 2.5rem 0 11px;">Intel Flex Series GPUs Expanded with Open Software Stack</h3>
                        <p>May 18, 2023 |&nbsp;<a href="" style="color:blue; text-decoration:underline !important">Software for Intel® Data Center GPU Flex Series</a></p>
                        <p><strong>New software updates optimize workloads for cloud gaming, AI inference, media acceleration &amp; digital content creation</strong></p>
                        <p>Introduced as a flexible, general-purpose GPU for the data center and the intelligent visual cloud, the&nbsp;<a class="b_special_a1" href="">Intel® Data Center GPU Flex Series</a>&nbsp;was expanded with new production-level software to optimize workloads for cloud gaming, AI inference, media acceleration, digital content creation, and more. This GPU platform has an open and full software stack, no licensing fees, and a unified programming model for CPUs and GPUs for performance and productivity via&nbsp;<a class="b_special_a1" href="">oneAPI</a>.</p>
                        <p class="mb-0"><strong>New Software Capability Highlights:</strong></p>
                        <ul>
                            <li><strong>Windows Cloud Gaming</strong> – Tap into the GPU’s power for remote gaming with a new reference stack.</li>
                            <li><strong>AI Inference</strong> – Boost deep learning and visual inference in applications used for&nbsp;smart city, library indexing and compliance, AI-guided video enhancement, intelligent traffic management, smart buildings and factories, and retail.</li>
                            <li><strong>Digital Content Creation</strong> –&nbsp;Deliver real-time rendering tapping into dedicated hardware acceleration, complete&nbsp;AI-based denoising in milliseconds.</li>
                            <li><strong>Autonomous Driving </strong>– Utilize Unreal Engine 4 to advance training and validation of AD systems.</li>
                        </ul>
                        <p>Learn what comprises the open software stack, available tools, and how to <a class="b_special_a1" href="">get started with pre-configured containers</a>.</p>
                        <p style="text-align:center"><a style="padding: 5px 20px;" class="dk_button" href="">Get the details</a></p>
                        <hr>
                        <p class="mb-0">&nbsp;</p>
                        <p class="mb-0">&nbsp;</p>


                        <h3 style="font-weight: 350; margin: 2.5rem 0 11px;">2023.1.1 Release of Intel AI Analytics Toolkit Includes New Features & Fixes</h3>
                        <p>May 3, 2023 |&nbsp;<a href="" style="color:blue; text-decoration:underline !important">Intel® AI Analytics Toolkit</a></p>
                        <p>The latest release of the AI Kit continues to help AI developers, data scientists, and researchers accelerate end-to-end data science and analytics pipelines on Intel® architecture.</p>
                        <p style="text-align:center"><a style="padding: 5px 20px;" class="dk_button" href="">Get it now</a></p>

                        <p><strong>Highlights</strong></p>
                        <ul>
                            <li><a class="b_special_a1" href=""><strong>Intel® Neural Compressor</strong></a> optimizes auto- and multi-node tuning strategy and large language model (LLM) memory.</li>
                            <li><a class="b_special_a1" href=""><strong>Intel® Distribution of Modin</strong></a> introduces a new, experimental NumPy API that provides basic support for distributed numerical calculations.</li>
                            <li><a class="b_special_a1" href=""><strong>Model Zoo for Intel® Architecture</strong></a> now supports Intel® Data Center GPU Max Series and extends support for dataset downloader and data connectors.</li>
                            <li><a class="b_special_a1" href=""><strong>Intel® Extension for TensorFlow</strong></a> now supports TensorFlow 2.12 and adds Ubuntu 22.04 and Red Hat Enterprise Linux 8.6 to the list of supported platforms.</li>
                            <li><a class="b_special_a1" href=""><strong>Intel® Extension for PyTorch</strong></a> is now compatible with Intel® oneAPI Deep Neural Network Library (oneDNN) 3.1, which improves on PyTorch 1.13 operator coverage.&nbsp;</li>
                        </ul>
                        <p>See the <a class="b_special_a1" href="">AI KIt release notes</a> for full details.</p>

                        <p class="mb-0"><strong>More References</strong></p>
                        <ul>
                            <li><a class="b_special_a1" href="">Ready to use AI &amp; Analytics code samples</a></li>
                            <li><a class="b_special_a1" href="">Essential Tools for Jumpstarting AI Development Projects</a></li>
                            <li><a class="b_special_a1" href="">Intel Neural Compressor Tuning Strategies</a> [GitHub]</li>
                            <li><a class="b_special_a1" href="">10-Minute Quick-start Guide for Modin</a></li>
                        </ul>
                        <p class="mb-0">&nbsp;</p>
                        <hr>
                        <p class="mb-0">&nbsp;</p>


                        <h3 style="font-weight: 350; margin: 2.5rem 0 11px;">Explore Ready-to-Use Code Samples for CPUs, GPUs, and FPGAs</h3>
                        <p>April 20, 2023 | <a href="" style="color:blue; text-decoration:underline !important">oneAPI &amp; AI Code Samples</a></p>
                        <p>Intel’s newly launched <strong>Code Samples portal</strong> provides direct access to a sizable (and always growing) collection of open source, high-quality, ready-to-use code that can be used to develop, offload, and optimize multiarchitecture applications.</p>
                        <p>Each sample is purpose-built to help any developer at any level understand concepts and techniques for adapting parallel programming methods to heterogeneous compute; they span high-performance computing, code and performance optimization, AI and machine learning, and scientific or general graphics rendering.</p>
                        <p>No matter their experience level, developers can find a variety of useful samples—all resident in the GitHub repository—with helpful instructions and commented code.</p>
                        <p style="text-align:center"><a class="b_special_a1" href="">Bookmark the Code Samples page→</a></p>
                        <p style="text-align:center"><a style="padding: 5px 20px;" class="dk_button" href="">Get the details</a></p>
                        <p class="mb-0">&nbsp;</p>
                        <hr>
                        <p class="mb-0">&nbsp;</p>
                        
                        <h3 style="font-weight: 350; margin: 2.5rem 0 11px;">VMWare-Intel Collaboration Delivers Video and Graphics Acceleration via AV1 Encode/Decode on Intel® GPUs</h3>
                        <p>April 11, 2023 | <a href="" style="color:blue; text-decoration:underline !important">Intel® Arc™ Graphics</a>, <a href="" style="color:blue; text-decoration:underline !important">Intel® Data Center GPU Flex Series</a></p>
                        <p><strong>Next-gen, multimedia codec offers more compression efficiency and performance</strong></p>
                        <p>The latest release of <a class="b_special_a1" href="">VMware Horizon</a> supports <a class="b_special_a1" href="">Intel®&nbsp;GPUs</a> and provides media acceleration enabled by <a class="b_special_a1" href="">Intel®&nbsp;oneAPI Video Library</a> (oneVPL). With Intel GPU support, VMware customers have greater choice, flexibility, and cost options on a wider range of hardware systems for deployment without being locked to a single GPU vendor. Running VMware Horizon on systems with Intel GPUs does not require license server setup, licensing costs, or ongoing support costs.&nbsp;</p>

                        <p>This Horizon release for desktops and servers utilizes AV1 encoding, optimized by oneVPL, on both <strong>Intel®&nbsp;Arc™ graphics</strong> and <strong>Intel®&nbsp;Data Center GPU Flex Series</strong>. The solution also delivers fast hardware encoding on supported Intel®&nbsp;X<sup>e</sup> architecture-based and newer GPUs (integrated and discrete). With a GPU-backed virtual machine (VM), users can have a better media experience with improved performance, reduced latency, more consistent frames per second, and lower CPU utilization.&nbsp;&nbsp;&nbsp;</p>

                        <p style="text-align:center"><a style="padding: 5px 20px;" class="dk_button" href="">Get the details</a></p>
                        <p class="mb-0">&nbsp;</p>
                        <hr>

                        <h3 style="font-weight: 350; margin: 2.5rem 0 11px;">Now Available: Intel® oneAPI 2023.1 Tools</h3>
                        <p>April 4, 2023 | <a href="" style="color:blue; text-decoration:underline !important">Intel® oneAPI and AI Tools</a></p>
                        <h3 style="font-weight: 500;">Delivering new performance and code-migration capabilities</h3>
                        <p>The just-released Intel® oneAPI 2023.1 tools augment the latest Intel® architecture features with high-bandwidth memory analysis, photorealistic ray tracing and path guiding, and extended CUDA-to-SYCL code migration support. Additionally, they continue to support the latest update of Codeplay’s oneAPI plugins for <a class="b_special_a1" href="">NVIDIA</a> and <a class="b_special_a1" href="">AMD</a> that make it easier to write multiarchitecture SYCL code. (These free-to-download plugins deliver quality improvements, support Joint_matrix extension and CUDA 11.8/testing 12, and enable gfx1032 for AMD. The AMD plugin backend now works with ROCm 5.x driver.)</p>
                        <p style="text-align:center"><a style="padding: 5px 20px;" class="dk_button" href="">Get the details</a></p>
                        <p><strong>2023.1 Highlights:</strong></p>
                        <p class="mb-0">Compilers &amp; SYCL Support</p>
                        <ul>
                            <li><a class="b_special_a1" href="">Intel® oneAPI DPC++/C++ Compiler</a> delivers AI acceleration with BF16 full support, auto-CPU dispatch, and SYCL kernel properties, and adds more SYCL 2020 and OpenMP 5.0 and 5.1 features to improve productivity and boost CPU and GPU performance.</li>
                            <li><a class="b_special_a1" href="">Intel® oneAPI DPC++ Library</a> (oneDPL) improves performance of the sort, scan, and reduce algorithms.</li>
                            <li><a class="b_special_a1" href="">Intel® DPC++ Compatibility Tool</a> (based on the open source SYCLomatic project) delivers easier CUDA-to-SYCL code migration with support for the latest release of CUDA’s headers, and adds more equivalent SYCL language and oneAPI library mapping functions such as runtime, math, and neural network domains.</li>
                        </ul>

                        <p class="mb-0">Performance Libraries</p>
                        <ul>
                            <li><a class="b_special_a1" href="">Intel® oneAPI Math Kernel Library</a> (oneMKL) improves data center GPU performance via new real FFTs, plus 1D and 2D optimizations, random number generators, and Sparse BLAS and LAPACK inverse optimizations.</li>
                            <li><a class="b_special_a1" href="">Intel® MPI Library</a> enhances performance for collectives using GPU buffers and default process pinning on CPUs with E-cores and P-cores.</li>
                            <li><a class="b_special_a1" href="">Intel® oneAPI Threading Building Blocks</a> (oneTBB) improves robustness of thread-creation algorithms on Linux and provides full support of Thread Sanitizer on macOS and full-hybrid Intel® CPU support. &nbsp;</li>
                            <li><a class="b_special_a1" href="">Intel® oneAPI Data Analytics Library</a> (oneDAL) is reduced in size by 30%.</li>
                            <li><a class="b_special_a1" href="">Intel® oneAPI Collective Communications Library (oneCCL)</a> improves scaling efficiency of the Scaleup algorithms for <em>Alltoall</em> and<em> Allgather</em> and adds collective selection for scaleout algorithm for device (GPU) buffers.</li>
                            <li><a class="b_special_a1" href="">Intel® Integrated Performance Primitives (Intel® IPP)</a> expands cryptography offerings with CCM/GCM modes, which enables Crypto Multi-Buffer for greater performance compared to scalar implementations, and adds support for asymmetric cryptographic algorithm SM2 for key exchange protocol and encryption/decryption APIs.</li>
                        </ul>

                        <p class="mb-0">Analysis &amp; Debug</p>
                        <ul>
                            <li><a class="b_special_a1" href="">Intel® VTune™ Profiler</a> identifies the best profile to gain performance utilizing high-bandwidth memory (HBM) on Intel® Xeon® Processor Max Series.&nbsp;It displays X<sup>e</sup> Link cross-card traffic issues such as CPU/GPU imbalances, stack-to-stack traffic, and throughput and bandwidth bottlenecks on Intel® Data Center GPU Max Series.</li>
                            <li><a class="b_special_a1" href="">Intel® Distribution for GDB</a> adds debug support for Intel® Arc™ GPUs on Windows and improves the debug performance on Linux for Intel discrete GPUs.</li>
                        </ul>

                        <p class="mb-0">Rendering &amp; Visual Computing</p>
                        <ul>
                            <li><a class="b_special_a1" href="">Intel® Open Path Guiding Library (Intel® Open PGL)</a> is integrated in Blender and Chaos V-Ray and provides state-of-the-art path-guiding methods for rendering.</li>
                            <li><a class="b_special_a1" href="">Intel® Embree</a> supports Intel Arc GPUs&nbsp;and Intel® Data Center GPU Flex Series, and delivers performance increases on 4<sup>th</sup> Gen Intel® Xeon® processors per <a class="b_special_a1" href="">Phoronix benchmarks</a>.</li>
                            <li><a class="b_special_a1" href="">Intel® OSPRay Studio</a> add functionality from open Tiny EXR, Tiny DNG (for .tiff files), and Open Image IO.</li>
                        </ul>

                        <h4 style="font-weight: 500;">oneAPI tools drive ecosystem innovation</h4>
                        <p class="mb-0">oneAPI tools adoption is ramping multiarchitecture programming on new accelerators, and the ecosystem is rapidly pioneering unique solutions using the open, standards-based, unified programming model. Here are the most recent:</p>
                        <ul>
                            <li><strong>Cross-platform: </strong><a class="b_special_a1" href="">Purdue University</a> launched a oneAPI Center of Excellence to advance AI and HPC teaching in the United States.</li>
                            <li><strong>Cloud: </strong><a class="b_special_a1" href="">University of Tennessee</a> launched oneAPI Center-of-Excellence Research which enabled a cloud-based Rendering as a Service (RaaS) learning environment for students.</li>
                            <li><strong>AI:</strong> Hugging Face accelerated PyTorch Transformers on 4<sup>th</sup> Gen Intel Xeon processors (explore <a class="b_special_a1" href="">part 1</a> and <a class="b_special_a1" href="">part 2</a>), and HippoScreen <a class="b_special_a1" href="">increased AI performance by 2.4x</a> to improve efficiency and build deep learning models.</li>
                            <li><strong>Graphics &amp; Ray Tracing: </strong>Thousands of artists, content creators, and 3D experts can easily access advanced ray tracing, denoising, and path guiding capabilities through Intel rendering libraries integrated in popular renderers including Blender, Chaos V-Ray, and <a class="b_special_a1" href="">DreamWorks open source MoonRay</a>.</li>
                        </ul>

                        <p class="mb-0"><strong>Learn More</strong></p>
                        <ul>
                            <li><a class="b_special_a1" href="">Explore Intel oneAPI &amp; AI tools &gt;</a></li>
                            <li>New to SYCL? <a class="b_special_a1" href="">Get started here &gt;</a></li>
                            <li><a class="b_special_a1" href="">Bookmark the oneAPI Training Portal</a> – Learn the way you want to with learning paths, tools, on-demand training, and opportunities to share and showcase your work.</li>
                        </ul>
                        <p class="utility-text-2"><strong>Notices and Disclaimers</strong><br>
                            Codeplay is an Intel company.<br>
                            Performance varies by use, configuration and other factors. Learn more at <a href="" style="color:blue; text-decoration:underline !important">www.Intel.com/PerformanceIndex</a>. Results may vary.&nbsp;<br>
                            Performance results are based on testing as of dates shown in configurations and may not reflect all publicly available updates. &nbsp;&nbsp;<br>
                            No product or component can be absolutely secure. Your costs and results may vary.&nbsp;&nbsp;<br>
                            Intel technologies may require enabled hardware, software or service activation.&nbsp;<br>
                            Intel does not control or audit third-party data. You should consult other sources to evaluate accuracy</p>
                        
                        <p>&nbsp;</p>

                        <h3><a id="purdue-coe" name="purdue-coe"></a><strong>Purdue Launches oneAPI Center of Excellence to Advance AI &amp; HPC Teaching in the U.S.</strong></h3>
                        <p>March 27, 2023 | <a href="http://www.oneapi.io" style="color:blue; text-decoration:underline !important">oneAPI</a>, <a href="" style="color:blue; text-decoration:underline !important">Intel® oneAPI Toolkits</a></p>
                        <h4><strong>Building oneAPI multiarchitecture programming concepts into the ECE curriculum</strong></h4>
                        <p>Purdue University will establish a oneAPI Center of Excellence on its West Lafayette campus. Facilitated through Purdue University’s <a class="b_special_a1" href="">Elmore Family School of Electrical and Computer Engineering</a> (ECE), the center will take students’ original AI and HPC research projects to the next level through teaching oneAPI in the classroom.</p>
                        <p>The facility will use curated content from Intel including teaching kits and certified instructor courses, and students will have access to the latest Intel® hardware and software via the <a class="b_special_a1" href="">Intel® Developer Cloud</a>.</p>

                        <p class="">&nbsp;</p>
                        <div class="d-flex">
                            <div class="">
                                <i class="fa-solid fa-quote-left" style="font-size: 65px; color: #E5E6E6; padding-right: 15px;"></i>
                            </div>
                            <div class="">
                                <div class="mv_together_content">
                                    <p class="mb-2" style="font-weight: 300;"><i>“Purdue’s track record as one of the most innovative universities in America with its world-changing research, programs and culture of inclusion is a perfect fit for the oneAPI Center of Excellence. By giving Purdue students access to the latest AI software and hardware, we’ll see the next generation of developers, scientists and engineers delivering innovations that will change the world. We’re excited to assist Purdue in embracing the next giant leap in accelerated computing.”</i></p>
                                    <p>– Scott Apeland, Director of Intel Developer Relations</p>
                                </div>
                            </div>
                        </div>
                        <p class="mb-0">&nbsp;</p>
                        <p class="mb-0">&nbsp;</p>
                        <p class="intro-paragraph"><a style="padding: 5px 20px;" class="dk_button" href="">Learn more</a></p>
                        <p class="mb-0">&nbsp;</p>
                        <hr>
                        <p class="mb-0">&nbsp;</p>


                        <h3><strong>Just Released the 6 Final AI Reference Kits</strong></h3>
                        <p>March 24, 2023 |&nbsp;<a href="" style="color:blue; text-decoration:underline !important">AI Reference Kits</a></p>
                        <h4><strong>A Total of 34 Kits to Streamline AI Solutions</strong></h4>
                        <p class="mb-0">The final six AI reference kits, powered by oneAPI, are now available to help data scientists and developers more easily and quickly develop and deploy innovative business solutions with maximum performance on Intel® hardware:</p>
                        <ol>
                            <li><a class="b_special_a1" href=""><strong>Visual Process Discovery</strong></a> for detecting UI elements in real time from inputted website screenshots (e.g., buttons, links, texts, images, headings, fields, labels, iframes) that users interacted with.</li>
                            <li><a class="b_special_a1" href=""><strong>Text Data Generation</strong></a> for generating synthetic text, such as the provided source dataset, using a large language model (LLM).</li>
                            <li><a class="b_special_a1" href=""><strong>Image Data Generation</strong></a><strong> </strong>for generating synthetic images using generative adversarial networks (GANs).</li>
                            <li><a class="b_special_a1" href=""><strong>Voice Data Generation</strong></a> for translating input text data to generate speech using transfer learning with VOCODER models.</li>
                            <li><a class="b_special_a1" href=""><strong>AI Data Protection</strong></a> for minimizing challenges with PII (personally identifiable information) in the design and development stages such as data masking, data de-identification, and anonymization.&nbsp;</li>
                            <li><a class="b_special_a1" href=""><strong>Engineering Design Optimization</strong></a><strong> </strong>for helping manufacturing engineers generate realistic designs whilst reducing manufacturing costs and accelerating product development processes.<br>
                            &nbsp;</li>
                        </ol>
                        <p><a style="padding: 5px 20px;" class="dk_button" href="">Learn more about the AI ref kits</a></p>
                        <hr>
                        <p class="mb-0">&nbsp;</p>
                        <p class="mb-0">&nbsp;</p> 


                        <h3><a id="MoonRay" name="MoonRay"></a><strong>DreamWorks Animation’s Open Source MoonRay Software Optimized via Intel® Embree</strong></h3>
                        <p>March 15, 2023 | <a href="" style="color:blue; text-decoration:underline !important">Intel® oneAPI Rendering Toolkit</a></p>
                        <h4>Advancing Open Rendering Innovation</h4>
                        <p>DreamWorks Animation’s production renderer is now open source, <strong>with photo-realistic ray-tracing acceleration provided by <a class="b_special_a1" href="">Intel® Embree</a></strong>, a high-performance ray-tracing library that’s part of the oneAPI Rendering Toolkit.</p>
                        <p>Formerly an in-house Monte Carlo ray tracer, Dreamworks’ MoonRay team worked with beta testers to adapt the code base—including enhancements and features—so it could be built and run outside of the company’s pipeline environment.</p>

                        <p class="">&nbsp;</p>
                        <div class="d-flex">
                            <div class="">
                                <i class="fa-solid fa-quote-left" style="font-size: 65px; color: #E5E6E6; padding-right: 15px;"></i>
                            </div>
                            <div class="">
                                <div class="mv_together_content">
                                    <p class="mb-2" style="font-weight: 300;"><i>“As part of this release and in collaboration with DreamWorks, MoonRay users have access to Intel® technologies, Intel Embree, and oneAPI tools, as building blocks for an open and performant rendering ecosystem.”</i></p>
                                    <p>– Anton Kaplanyan, VP graphics research, Intel</p>
                                </div>
                            </div>
                        </div>
                        <p class="mb-0">&nbsp;</p>
                        <p class="mb-0">&nbsp;</p>
                        <p class="intro-paragraph"><a style="padding: 5px 20px;" class="dk_button" href="">Learn more</a></p>
                        <p>Download <a class="b_special_a1" href="">Intel Embree code samples</a> [GitHub]</p>
                        <hr>
                        <p class="mb-0">&nbsp;</p>
                        <p class="mb-0">&nbsp;</p>

                    </section>

                </div>
            </div>
            <p class="mb-0">&nbsp;</p>
        </div>
    </section>

    <section style="border-top: 1px solid #d7d7d7;" class="container py-5">
        <h4 class="h6">Product and Performance Information</h4>
        <div class="disclaimer" style="font-size: 12px;"><sup>1</sup> Performance varies by use, configuration and other
            factors. Learn more at <a class="b_special_a1" href="">www.Intel.com/PerformanceIndex</a>.</div>
        </div>
    </section>

    <!-- footer -->
    <div id="footer"></div>

    <!-- script header and footer -->
    <script>
        // navbar include  
        fetch('/y_index/y_navbar.html')
            .then(response => response.text())
            .then(data => {
                document.getElementById('navbar').innerHTML = data;
            });
        // footer include 
        fetch('/y_index/y_footer.html')
            .then(response => response.text())
            .then(data => {
                document.getElementById('footer').innerHTML = data;
            });
    </script>

    <!-- nav script -->
    <script>
        document.addEventListener('DOMContentLoaded', function () {
            const nav = document.querySelector('.VK_client_app_navigation');
            const navLinks = document.querySelectorAll('.VK_ai_nav_bar a');
            const sections = document.querySelectorAll('section[id]');
            let navOffset = nav.offsetTop;

            // Add smooth scrolling to all links
            navLinks.forEach(link => {
                link.addEventListener('click', function (e) {
                    e.preventDefault();
                    document.querySelector(this.getAttribute('href')).scrollIntoView({
                        behavior: 'smooth'
                    });
                });
            });

            // Sticky Navigation
            window.addEventListener('scroll', () => {
                if (window.pageYOffset >= navOffset) {
                    nav.classList.add('VK_sticky_nav_bar');
                } else {
                    nav.classList.remove('VK_sticky_nav_bar');
                }
                // Section highlighting
                sections.forEach(section => {
                    const sectionTop = section.offsetTop - nav.clientHeight;
                    const sectionHeight = section.clientHeight;
                    console.log(sectionTop);
                    console.log(sectionHeight);
                    if (window.pageYOffset >= sectionTop && window.pageYOffset <= sectionTop + sectionHeight) {
                        navLinks.forEach(link => {
                            link.classList.remove('active');
                            if (link.getAttribute('href') === `#${section.id}`) {
                                link.classList.add('active');

                                // Ensure the active link is visible in the nav bar
                                const navBar = document.querySelector('.VK_ai_nav_bar');
                                const activeLink = document.querySelector('.VK_ai_nav_bar a.active');
                                const linkRect = activeLink.getBoundingClientRect();
                                const navBarRect = navBar.getBoundingClientRect();

                                if (linkRect.left < navBarRect.left || linkRect.right > navBarRect.right) {
                                    activeLink.scrollIntoView({ inline: 'center', behavior: 'smooth' });
                                }
                            }
                        });
                    }
                });
            });
        });
    </script>

    <!-- copy script -->
    <script>
        document.addEventListener("DOMContentLoaded", () => {
            document.querySelectorAll(".mv_copy_icon").forEach((icon) => {
                icon.addEventListener("click", async function () {
                    try {
                        const toolbar = this.closest(".mv_code_toolbar");
                        const codeBlock = toolbar.querySelector(".code-content");
                        const codeContent = codeBlock.innerText;

                        // Use Clipboard API to copy text
                        await navigator.clipboard.writeText(codeContent);

                        // Hide the copy icon and show the "Copied!" message
                        const message = toolbar.querySelector(".mv_copy_message");
                        this.classList.add("hidden"); // Hide the copy icon
                        message.classList.remove("hidden"); // Show the "Copied!" message

                        // Hide the message and show the icon again after 2 seconds
                        setTimeout(() => {
                            message.classList.add("hidden");
                            this.classList.remove("hidden");
                        }, 2000); // Adjust delay as needed

                    } catch (err) {
                        console.error('Failed to copy text: ', err);
                    }
                });
            });
        });
    </script>

</body>

</html>

