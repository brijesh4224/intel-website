<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Download Product Brief</title>

    <!-- slider -->
    <link rel="stylesheet" href="/css/all.min.css">
    <link rel="stylesheet" href="/css/animate.min.css">
    <link rel="stylesheet" href="/css/owl.carousel.min.css">
    <link rel="stylesheet" href="/css/owl.theme.default.min.css">

    <!-- Style Css -->
    <!-- <link rel="stylesheet" href="/css/mv_intel_data_center_GPU_max_series_overview.css"> -->
    <link rel="stylesheet" href="/css/Elevate_VDI_with_Intel_Data_Center_GPUs.css">

    <!-- header footer -->
    <link rel="stylesheet" href='/css/yatri.css'>

    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/css/bootstrap.min.css" rel="stylesheet"
        integrity="sha384-EVSTQN3/azprG1Anm3QDgpJLIm9Nao0Yz1ztcQTwFspd3yD65VohhpuuCOmLASjC" crossorigin="anonymous">
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/js/bootstrap.bundle.min.js"
        integrity="sha384-MrcW6ZMFYlzcLA8Nl+NtUVF0sA7MsXsP1UyJoMp4YLEuNSfAP+JcXn/tWtIaxVXM"
        crossorigin="anonymous"></script>
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.6.0/css/all.min.css" rel="stylesheet">

</head>

<body>
    <div id="navbar"></div>
    <section>
        <section style="padding-top: 70px;">
            <div class="mv_intel_data_bg_color text-center">
                <div class="">
                    <div class="row">
                        <div class="mv_intel_data_content col-lg-12 col-md-12">
                            <h1>Artificial Intelligence (AI)</h1>
                        </div>
                    </div>
                </div>
            </div>
        </section>

        <div class="mv_main_elevate11 mx-auto">
            <nav aria-label="breadcrumb" class="mv_border_nav11">
                <ol class="breadcrumb mv_breadvrumb1">
                    <li class="breadcrumb-item"><a href="#">Intel Community</a></li>
                    <li class="breadcrumb-item"><a href="#">Blogs</a></li>
                    <li class="breadcrumb-item"><a href="#">Tech Innovation</a></li>
                    <li class="breadcrumb-item active" aria-current="page">Artificial Intelligence (AI)</li>
                </ol>
            </nav>


            <div class="main-content mv_maincontent123">
                <div class="d-flex justify-content-between align-items-center mb-4 mt-3">
                    <h3>Architecting for Accelerators – Intel® AMX and Intel® XMX</h3>
                    <div>
                        <button class="btn subscribe-btn me-2 mv_subscribe1 mv_subscribe_btn123">Subscribe</button>
                        <button class="btn options-btn dropdown-toggle mv_article_btn123" type="button"
                            id="dropdownMenuButton" data-bs-toggle="dropdown" aria-expanded="false">
                            Article Options
                        </button>
                        <ul class="dropdown-menu" aria-labelledby="dropdownMenuButton">
                            <li><a class="dropdown-item" href="#">Subscribe to RSS Feed</a></li>
                            <hr>
                            <li><a class="dropdown-item text-muted" href="#">Mark as New</a></li>
                            <li><a class="dropdown-item text-muted" href="#">Mark as Read</a></li>
                            <hr>
                            <li><a class="dropdown-item text-muted" href="#">Book Mark</a></li>
                            <li><a class="dropdown-item text-muted" href="#">Subcribe</a></li>
                            <hr>
                            <li><a class="dropdown-item" href="#">Prienter Friendly Page</a></li>
                            <li><a class="dropdown-item" href="#">Report inappropriate Content</a></li>
                        </ul>
                    </div>
                </div>
                <!-- Add your article content here -->
            </div>

            <div>
                <div class="mv_elevating_image1">
                    <img src="../img/mv_image/intelxmx.png" />
                </div>
            </div>


            <p class="mv_font_elevate">
                In order to understand the advantages of the new built-in AI acceleration engines on Intel hardware, it
                is important to first understand the following two datatypes used in AI/ML workloads: the short
                precision datatypes INT8 and BF16. These two datatypes are better for inferencing than the default
                datatype FP32 more commonly associated with AI/ML inferencing. FP32 has a higher memory footprint and
                higher latency, whereas low-precision models with INT8 or BF16 result in faster computation speeds, so
                why is it still the default over INT8 or BF16? Well, to optimize and support these two efficient
                datatypes, HW needs special features/instructions. Intel provides them in the form of <a>Intel® Advanced
                    Matrix Extensions (Intel® AMX)</a> on Intel® Xeon® 4th Gen Scalable processors and Intel® Xᵉ Matrix
                Extensions (Intel® XMX) on <a>Intel® Data Center GPU Max Series or Intel® Data Center GPU Flex
                    Series</a>.
            </p>

            <h3>Anything Goes: Datatypes Free-for-all</h3>
            <p class="mv_font_elevate">
                One way to enable the support of these datatypes is the SYCL based coding abstraction <a href="">SYCL
                    Joint
                    Matrix
                    Extension</a>, which is invoked on Intel® AMX and Intel® XMX to ensure portability and performance
                improvements to the relevant code. Intel® AMX consists of extensions to the x86 instruction set
                architecture (ISA) for microprocessors using two-dimensional registers called tiles upon which
                accelerators can perform operations. On the GPU side, Intel® Xᵉ Matrix Extensions, also known as DPAS,
                specializes in executing dot product and accumulate operations on 2D systolic arrays. It supports a wide
                range of datatypes including U8, S8, U4, S4, U2, S2, INT8, FP16, BF16, and TF32. Both instruction sets
                require having <a href="">Intel® oneAPI Base Toolkit</a> version 2023.1.0 or later installed.
            </p>

            <p class="mv_font_elevate">
                Some of the most commonly used 16-bit formats and 8-bit formats are 16-bit IEEE floating point (fp16),
                bfloat16 (bf16), 16-bit integer (int16), 8-bit integer (int8), and 8-bit Microsoft* floating point
                (ms-fp8). Figure 1 below visualizes the differences between some of these formats.
            </p>

            <div>
                <div class="mv_elevating_image1">
                    <img src="../img/mv_image/tablefp.png" />
                </div>
                <p>Figure 1. Various numerical datatype representations. s represents the sign bit. Also, note that FP32
                    and BF16 provide the same dynamic range but FP32 provides higher precision due to the larger
                    mantissa.</p>
            </div>
            <h3>Invoking Intel® AMX and Intel® XMX</h3>

            <p class="mv_font_elevate">
                Intel® AMX and Intel® XMX are invoked at various levels of abstractions to ultimately create the
                seamless optimizations seen by the end user. Closer to the hardware level is the CPU and GPU assembly
                layer providing the machine instructions, followed by the CPU and GPU intrinsics levels that encompass
                the variables and operations. At a more abstract coding level, above the intrinsics but below the
                libraries, there is the Joint Matrix API, which is a SYCL extension implemented to invoke Intel® AMX and
                Intel® XMX through SYCL code. This way of coding enables users to scale their matrix programming across
                several vendors invoking the systolic implementation of the respective platforms. A level up from here
                is where libraries such as the <a>Intel® oneAPI Deep Neural Network Library (oneDNN)</a> and <a>Intel®
                    oneAPI Math
                    Kernel Library (oneMKL)</a> are situated. At this moment this is enabled only for Intel Hardware,
                but
                scalable across supported CPUs and GPUs. It is thus from the Intel® AMX and Intel® XMX hardware level
                all the way up through the libraries level that Intel is making the acceleration magic possible for the
                end users. Application programmers will be more focused on algorithmic improvements at the highest
                abstractions, such as the framework and AI applications levels.
            </p>

            <h3>Implementing Joint Matrix for Tensor Hardware Programming</h3>
            <p class="mv_font_elevate">
                Diving into the main innovation, the new SYCL Joint Matrix Extension for tensor hardware programming
                unifies targets like Intel® Advanced Matrix Extensions (Intel® AMX) for CPUs, Intel® Xe Matrix
                Extensions (Intel® XMX) for GPUs, and NVIDIA* Tensor Cores. The joint matrix extension consists of the
                following functions: joint_matrix_load and joint_matrix_store for explicit memory operations,
                joint_matrix_fill for matrix initialization, joint_matrix_mad for the multiplication and addition
                operations; and lastly get_wi_data for element wise operations, which splits the matrix into its work
                items and performs operations. Furthermore, to reach optimal performance levels on Intel® XMX, the GEMM
                kernel must be written in a way that continuously feeds Intel® XMX with the data needed to perform the
                most multiplication and additions operations as possible per cycle. (See <b><a href="">the optimization
                        guide
                        here</a></b>.)
            </p>

            <p class="mv_font_elevate">
                While frameworks like <b><a href="">TensorFlow</a></b> and libraries like oneDNN are often enough to
                handle most user
                needs,
                there are users who want to build their own neural networks applications, but these libraries and
                frameworks may appear too high-level and too heavy (very large libraries) and do not provide a clear
                pathway for developers to create custom optimizations. Furthermore, these large frameworks are often too
                slow-paced in implementing support for the frequent additions and changes to available operations and
                tools in the evolving machine learning space. As such, users may consider using joint matrix with its
                lower level of abstraction than the frameworks, providing performance, productivity, and fusion
                capabilities while still retaining portability across different systems by using only one code to target
                different tensor hardware. For developers looking for more control over their neural networking projects
                and would like to try it out, you can find the SYCL Joint Matrix Extension specification <b><a
                        href="">here</a></b>. We
                also
                encourage you to check out Intel’s other <b><a href="">AI Tools</a></b> and Framework optimizations and
                learn about the
                unified, open, standards-based <b><a href="">oneAPI</a></b> programming model that forms the foundation
                of
                Intel’s AI Software
                Portfolio.
            </p>

            <p class="mv_font_elevate">See the video: <b><a href="">Architecting for Accelerators</a></b></p>

            <div>
                <div class="mv_elevating_image1">
                    <img src="../img/mv_image/intelxe.png" />
                </div>
            </div>
            <h3>About our experts</h3>
            <b>Subarnarekha Ghosal</b>
            <p>Technical Consulting Engineer<br> intel</p>

            <p class="mv_font_elevate">
                Subarnarekha is a Software Technology Consulting Engineer responsible for helping internal and external
                customers succeed on Intel platforms using Intel software development tools specifically Intel
                compilers. She uses code modernization techniques for optimal code performance on the Intel Central
                Processing Unit (CPU) and Intel Graphics Processing Unit (GPU). Subarna has a master’s degree in
                Computer Science and is working with C++ technology and Computer Architecture since last 7+ years.
            </p>

            <b>Mayur Pandey</b>
            <p>Technical Consulting Engineer<br> intel</p>

            <p class="mv_font_elevate">
                Mayur is a Software Technical Consulting Engineer at Intel with 12+ years of experience on different
                proprietary and open-source compilers and compiler tools with the main focus being the Compiler Frontend
                and Optimizer. He also has experience in Application Benchmarking, Performance Evaluation, and Tuning of
                applications on HPC clusters and Enablement of HPC Applications on different architectures.
            </p>

            <a href="">Translate</a>
            <div class="d-flex ">
                <p>Tags:</p>
                <p class="ms-4"><a href="">AI</a></p>
                <p class="ms-4"><a href="">AIDev
                    </a></p>
                <p class="ms-4"><a href="">AMX
                    </a></p>
                <p class="ms-4"><a href="">CPU</a></p>
                <p class="ms-4"><a href="">GPU</a></p>
                <p class="ms-4"><a href="">oneapi</a></p>
                <p class="ms-4"><a href="">oneDNN
                    </a></p>
                <p class="ms-4"><a href="">OneMKL
                    </a></p>
                <p class="ms-4"><a href="">XMX
                    </a></p>
            </div>

            <div>
                <button class="mv_tokudos"><i class="fa-regular fa-thumbs-up"></i>1 Kudos</button>
            </div>

            <div class="container mt-5">
                <h5 class="mb-4">About the Author</h5>
                <hr>
                <div class="row">
                    <div class="col-md-3 mb-3">
                        <img src="../img/mv_image/wolf.png" alt="Glenn Le Vernois" class=" img-fluid mv_glenn_img">
                    </div>
                    <div class="col-md-9">
                        <p>AI Software Marketing Engineer creating insightful content surrounding the cutting edge AI
                            and ML technologies and software tools coming out of Intel</p>
                    </div>
                </div>
            </div>

            <div class="mt-5">
                <div class="comment-banner">
                    <p class="mb-0">You must be a registered user to add a comment. If you've already registered, sign
                        in. Otherwise, register and sign in.</p>
                    <a href="#" class="comment-link">Comment</a>
                </div>
            </div>

            <p><b>Community support is provided Monday to Friday. Other contact methods are available <a
                        href="">here</a>.</b>
            </p>

            <p class="mv_font_elevate">
                Intel does not verify all solutions, including but not limited to any file transfers that may appear in
                this community. Accordingly, Intel disclaims all express and implied warranties, including without
                limitation, the implied warranties of merchantability, fitness for a particular purpose, and
                non-infringement, as well as any warranty arising from course of performance, course of dealing, or
                usage in trade.
            </p>

            <p>
                For more complete information about compiler optimizations, see our <a href="">Optimization Notice</a>.
            </p>
        </div>
    </section>

    <div id="footer"></div>

    <script>
        // navbar include
        fetch('/y_index/y_navbar.html')
            .then(response => response.text())
            .then(data => {
                document.getElementById('navbar').innerHTML = data;
            });
        // footer include 
        fetch('/y_index/y_footer.html')
            .then(response => response.text())
            .then(data => {
                document.getElementById('footer').innerHTML = data;
            });
    </script>

    <script>
        document.querySelectorAll('.faq-question').forEach(question => {
            question.addEventListener('click', () => {
                const answer = question.nextElementSibling;
                const icon = question.querySelector('.toggle-icon');
                answer.style.display = answer.style.display === 'block' ? 'none' : 'block';
                icon.textContent = answer.style.display === 'block' ? '▲' : '▼';
            });
        });
    </script>
</body>

</html>