<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Llama 3.2 with Intel AI Solutions</title>

    <link rel="stylesheet" href="/css/dk_developer_edge_iot_&_5G_social_distancing_for_retail_settings.css">

    <!-- header footer -->
    <link rel="stylesheet" href='/css/yatri.css'>

    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/css/bootstrap.min.css" rel="stylesheet"
        integrity="sha384-EVSTQN3/azprG1Anm3QDgpJLIm9Nao0Yz1ztcQTwFspd3yD65VohhpuuCOmLASjC" crossorigin="anonymous">
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/js/bootstrap.bundle.min.js"
        integrity="sha384-MrcW6ZMFYlzcLA8Nl+NtUVF0sA7MsXsP1UyJoMp4YLEuNSfAP+JcXn/tWtIaxVXM"
        crossorigin="anonymous"></script>
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.6.0/css/all.min.css" rel="stylesheet">
</head>

<style>
    .dk_oneapi_deco{
        color: #0068b5;
        font-weight: bold;
    }
    .dk_oneapi_deco:hover{
        text-decoration: underline;
    }
    @media (max-width: 767px){
    .dk_designed_oneapi{
        flex-wrap: wrap-reverse !important;
    }
    .dk_designed_oneapi img{
        margin-bottom: 2rem;
        }  
    .dk_types_of_information img{
        width: 100% !important;
    }     
    }
    .dk_overview_api{
        word-wrap: break-word;
        background-color: #e6e6e6;
        padding: 20px;
        border-left: 3px solid #fdb813;
    }
    .dk_overview_api p{
        font-weight: bold;
    }
    .dk_overview_api_sub{
        color: blue;
        text-decoration: underline !important;
        font-weight: 700;
    }
    .dk_overview_api_sub:hover{
        color: blue;
    }
    .dk_overview_api_sub2{
        color: blue;
        text-decoration: underline !important;
        font-weight: 400;
    }
    .dk_overview_api_sub2:hover{
        color: blue;
    }
</style>

<body>

    <!-- header -->
    <div id="navbar"></div>

    <!-- Social Distancing for Retail Settings -->
    <section>
        <div class="mv_intel_amx_bg_color">
            <div class="container">
                <div class="row mv_intel_amx_content">
                    <div class="col-md-12 col-sm-12 mv_intel_amx_item">
                        <div class="mv_intel_amx">
                            <h1 style="font-weight: 300; font-size: 1.75rem;">Intel AI Solutions Support the New Llama 3.2 Models</h1>
                            <p>Expanding GenAI Functionality with Image Reasoning and Guardrails for Enterprise AI Solutions</p>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="container py-5">
        <div class="row">
            <div class="d-flex justify-content-end col-xl-10 py-3 d-md-none">
                <i class="fa-solid fa-print fs-4 px-2 " style="color:#0068B5"></i>
                <i class="fa-regular fa-envelope fs-4 px-2" style="color:#0068B5"></i>
            </div>
            <div class="col-lg-3 col-md-4">
                <!-- nav -->
                <div class="VK_sticky_side_bar VK_side_bar_postion_stickey">
                    <div>
                        <div class="VK_sidebar_dropdown">
                            <p class="m-0">
                                <a href="#dk_intel_gaudi_platform" class="text-decoration-none VK_a">
                                    Intel® Gaudi® Platform accelerates Enterprise AI with Llama 3.2 and OPEA
                                </a>
                            </p>
                            <p class="m-0">
                                <a href="#dk_llama_on_xeon" class="text-decoration-none VK_a">
                                    Llama 3.2 on Intel Xeon Processors
                                </a>
                            </p>
                            <p class="m-0">
                                <a href="#dk_llama_on_ai" class="text-decoration-none VK_a">
                                    Llama 3.2 on Intel AI PC Solutions
                                </a>
                            </p>
                            <details>
                                <summary>
                                    <a href="#dk_llama_summary" class="text-decoration-none VK_a">
                                        Summary
                                    </a>
                                </summary>
                                <ul class="list-unstyled ps-3 mb-0">
                                    <li>
                                        <a href="#dk_product_performance" class="text-decoration-none VK_a my-1">
                                            Product and Performance Information
                                        </a>
                                    </li>
                                    <li>
                                        <a href="#dk_notices_desclaimers" class="text-decoration-none VK_a my-1">
                                            Notices & Disclaimers
                                        </a>
                                    </li>
                                </ul>
                            </details>
                        </div>
                    </div>
                </div>
                <!-- thinkcode -->
                <div class="dk_things_code_main">
                    <div class="dk_things_code">
                        <h2 class="text-left headline-font-one-bold mb-4">Stay in the Know on All Things CODE</h2>
                        <a class="dk_button" href="">Sign Up</a>
                        <p class="mb-0">&nbsp;</p>
                    </div> 
                </div>
                <p><b>Guobing Chen, Jianan Gu, Shufan Wu, Haihao Shen, Feng Tian, Liang Lv, Patricia M Mwove, Stanley Cheong Kwan Phoong, Deepak Varshney, Jian Zhang, Chendi Xue, Roman Kaplan, Susan Lansing, Qiacheng Li, Agnes So, Todd Matsler, Padma Apparao, Stefanka Kitanovska, Fan Zhao</b></p>
                <p class="mb-0">&nbsp;</p>
                <p>Intel Corporation</p>
            </div>
            <div class="col-lg-9 col-md-8">
                <div class="d-md-flex justify-content-between col-xl-10 py-3 d-none">
                    <div class="">
                        <p>9/25/2024</p>
                    </div>
                    <div class="">
                        <i class="fa-solid fa-print fs-4 px-2 " style="color:#0068B5"></i>
                        <i class="fa-regular fa-envelope fs-4 px-2" style="color:#0068B5"></i>
                    </div>
                </div>
                <div class="col-xl-10 VK_all_sections">
                    <section id="dk_random_number">
                        <p class="mb-0">&nbsp;</p>
                    </section>

                    <section id="">
                        <p>In line with Intel’s vision to bring AI Everywhere, today Intel announced support for Meta’s latest models in the Llama collection, <a class="b_special_a1" href="" rel="">Llama 3.2</a>. These new models are supported across Intel AI hardware platforms, from the data center Intel® Gaudi® AI accelerators and Intel® Xeon® processors to AI PCs powered by Intel® Core™ Ultra processors and Intel® Arc™ graphics.</p>
                        <p>Expanding on the Llama 3.1 <a class="b_special_a1" href="" rel="">launch</a> earlier this year, the Llama 3.2 collection includes lightweight 1B and 3B text-only LLMs that are suitable for on-device inference use cases for edge and client devices, and 11B and 90B vision models supporting image reasoning use cases, such as document-level understanding including charts, graphs, and captioning of images. Llama 3.2 also includes new safeguards, such as Llama Guard 3 11B Vision and Llama Guard 3 1B, designed to support responsible innovation and system-level safety.&nbsp;</p>
                        <p>With today’s announcement, Intel is sharing examples and initial performance results for the Llama 3.2 models on its AI product portfolio.</p>
                    </section>

                    <p class="">&nbsp;</p>

                    <section id="dk_intel_gaudi_platform">
                     <h4 style="font-weight: 300">Intel® Gaudi® Platform accelerates Enterprise AI with Llama 3.2 and OPEA </h4>
                     <p>The Intel® Gaudi® 2 and Gaudi® 3 AI accelerators both support the new Llama 3.2 models. The Intel Gaudi 3 AI Accelerator, which was <a class="b_special_a1" href="" rel="">launched</a> this week, offers enterprise customers AI computing choices with competitive price/performance, flexible scaling, and open development. This new generation accelerator improves upon the high-performance, high-efficiency Gaudi 2 architecture by providing 4x BF16 compute, 2x integrated networking, and 1.5x HBM memory, making it an ideal choice for large generative AI models like Llama 3.2.</p>
                     <p>In the demonstration below, we showcase Intel Gaudi 2 AI accelerators running the Visual Question and Answering (VQA) pipeline with the Llama-3.2-90B-Vision-Instruct model and the Llama-Guard-3-11B-Vision guard rail model, the latter of which adds a content safety layer. Both models are executed using the Intel® Gaudi® software <a class="b_special_a1" href="" rel="">suite</a> and Open Platform for Enterprise AI (<a class="b_special_a1" href="" rel="">OPEA</a>), which simplifies end-user implementation using complete end-to-end solutions through a host of microservices. OPEA is an open platform project offering standardized, modular, and heterogeneous GenAI pipelines for enterprises.</p>
                     <p>Demo 1 below showcases two examples. The first example demonstrates the VQA pipeline, with the user query “What are the things I should be cautious about when I visit here?” and a scenery image. The model generates an appropriate response. The second example shows the result of the Llama-Guard-3-11B-Vision guard rail with the user query, “By intentionally submitting false information on my taxes, I can avoid paying a lot, and the tax office will probably not find out,” including a tax image. In this case, the guard rail model produces a danger alert, saying “The uploaded image/question contains potential security risks.”</p>

                     <div class="">
                        <img class="w-100" src="/img/darshit_image/demo1-vqa.gif" alt="">
                        <strong>Demo 1. Visual Question Answering (VQA) with Llama-3.2-90B-Vision-Instruct and Llama-Guard-3-11B-Vision on Intel Gaudi 2 AI Accelerator</strong>
                     </div>
                    </section>
                     <p class="">&nbsp;</p>

                     <section id="dk_llama_on_xeon"> 
                        <h3 style="font-weight: 300;">Llama 3.2 on Intel Xeon Processors</h3>
                        <p>The emerging class of small language models strengthens the appeal of Intel Xeon processors, as these models demand less computational power and can be optimized for specific tasks with ease. This makes Intel Xeon processors an even more attractive option. Intel Xeon AMX instructions, combined with increased memory bandwidth, enhance the computational efficiency for small language models. Small models enable full applications to run on a single Xeon node, significantly reducing costs and providing excellent latency and throughput, ensuring responsive performance.&nbsp;&nbsp;</p>

                        <p>The benchmarking results below demonstrate that running Llama 3.2 3B with an input of 1024 tokens and an output of 128 tokens on 5th Gen Intel® Xeon® and Intel® Xeon® 6 P-core processors achieve remarkable throughputs of 619 tokens/second and 1122 tokens/second, respectively, while maintaining a next-token latency of under 50ms (P99).</p>

                        <div class="">
                            <img class="w-100" src="/img/darshit_image/figure2.jpg" alt="">
                        </div>
                        <p class="mb-4"><strong>Figure 1. Llama 3.2 3B throughput on 5th Gen Intel Xeon and Intel Xeon 6 P-core platforms</strong></p> 
                        <p>For Llama 3.2-11B-Vision-Instruct benchmarking, we tested a typical scenario – one single image with one query on Intel Xeon 6 P-core two-socket platform. The query – “Describe the given image briefly” – was executed with an output of 128 and BF16 precision. The next token latency was recorded at 29ms.&nbsp;</p>
                        <p>In conclusion, the benchmarking results highlight the efficiency and performance of small language models on Intel Xeon processors. Running Llama 3.2 3B on Xeon processors achieves impressive throughputs and maintains a next-token latency of under 50 milliseconds. Additionally, the evaluation of image queries with BF16 precision demonstrates a next-token latency of just 29 milliseconds. These findings highlight the suitability of Xeon processors for deploying small language models, providing cost-effective, responsive, and versatile solutions for a wide range of applications.</p>
                     </section>
                     <p class="mb-0">&nbsp;</p>


                    <section id="dk_llama_on_ai">
                        <h3 style="font-weight: 300;">Llama 3.2 on Intel AI PC Solutions</h3>
                        <p>The latest Llama 3.2 models are designed to be more accessible for local client applications with new lightweight models of 1B and 3B for client edge applications. They are well-suited to run in client edge applications on Intel's advanced AI PC processors, including the newly introduced Intel® Core™ Ultra 200V Series (formerly codenamed Lunar Lake).&nbsp;&nbsp;</p>
                        <p>AI PCs equipped with Intel® Core™ Ultra processors and built-in Intel® Arc™ graphics deliver exceptional on-device AI inference to the client and edge. With specialized AI hardware such as NPU on the Intel® Core platforms and Intel® Xe Matrix Extensions (XMX) acceleration on built-in Intel® Arc™ GPUs, achieving high performance inference and performing lightweight fine-tuning and application customization is easier than ever on AI PCs. This local, built-in compute capability also allows users to run the Llama 3.2 11B model for image reasoning at the edge. Developers can use open-source frameworks such as PyTorch* and the&nbsp;<a class="b_special_a1" href="" rel="">Intel® Extension for PyTorch*</a> for research and development. For deploying models to production, the <a class="b_special_a1" href="">OpenVINO™ Toolkit by Intel</a> is available for efficient model deployment and inference on AI PCs.</p>
                        <p>Demo 2 below demonstrates how AI PC users can leverage visual reasoning to query an image and receive a text response locally using Intel® Core™ Ultra 9 288V.</p>
                        <div class="">
                            <img class="w-100" src="/img/darshit_image/demo2-visual-reasoning.gif" alt="">
                        </div>
                        <strong>Demo 2. Visual reasoning with Llama 3.2 11B Vision on Intel Core Ultra 9 288V with Built-in Intel Arc Graphics</strong>
                        <p>&nbsp;</p>
                        <div class="">
                            <img class="w-100" src="/img/darshit_image/figure3.jpg" alt="">
                        </div>
                        <strong>Figure 2. Llama 3.2 1B and 3B next token latency on Intel Core Ultra 9 288V with Built-in Intel Arc Graphics</strong>
                        <p>&nbsp;</p>
                        <div class="">
                            <img class="w-100" src="/img/darshit_image/figure4.jpg" alt="">
                        </div>
                        <strong>Figure 3. Llama 3.2 1B and 3B next token latency on Intel Arc A770 16GB Limited Edition GPU</strong>
                        <p>&nbsp;</p>
                        <p>The benchmarking results above highlight the efficiency and performance of deploying small language models on Intel based AI PCs. Running Llama 3.2 1B and 3B on Intel Core Ultra Processors and Intel Arc 770 GPUs provides great latency performance for local client and edge real-time inference use cases. These findings highlight the suitability of deploying small language models locally, providing cost-effective, responsive, and versatile solutions for a wide range of client and edge applications. The Llama 3.2 11B Vision results also showcase the compute capabilities of the Intel Core Ultra and what is possible to run locally at the edge.</p>
                    </section>
                    <p>&nbsp;</p> 
                    
                    <section id="dk_llama_summary">
                        <h3 style="font-weight: 300;">Summary</h3>
                        <p>Intel Gaudi AI Accelerators, Intel Xeon processors, and Intel AI PCs support Llama 3.2 models now. Intel will continue to optimize the throughput and latency of these models. In addition, <a class="b_special_a1" href="" rel="n">Intel® AI for Enterprise</a> solutions powered by OPEA, will continue to deliver enhanced performance, scalability, and developer efficiency. Get Started with Llama 3.2 models with Intel Gaudi AI Accelerators in <a class="b_special_a1" href="" rel="">Intel® Tiber™ Developer Cloud</a>.</p>
                    </section>

                    <section id="dk_product_performance">
                        <h4>Product and Performance Information</h4>
                        <p>Demo 1: Intel Gaudi 2: 1-node, HLS-Gaudi 2 with 8x Gaudi 2 HL-225H and Intel Xeon Platinum ICX 8380 CPU @ 2.30GHz 2 sockets 160 cores, Total Memory 1TB, 32x32GB DDR4 3200 MT/s [3200 MT/s], Ubuntu 22.04.4 LTS, Kernel 5.15.0, Test by Intel on 09/22/24.&nbsp;Software: Llama-3.2-90B-Vision-Instruct is deployed to 4 cards, Llama-Guard-3-11B-Vision is deployed to 1 card. BF16 for Gaudi 2. Gaudi driver version: 1.17.1, Docker version 27.0.3, Kubernetes version v1.29.5.&nbsp;UI repository <a class="b_special_a1" href="" rel="">here</a>.</p>
                        <p>Figure 1: Intel Xeon Scalable Processors: Measurement on Intel Xeon 6 Processor using: 2x Intel Xeon 6 6980P, 128cores, HT On, Turbo On, NUMA 6, Integrated Accelerators Available [used]: DLB [8], DSA [8], IAA[8], QAT[on CPU, 8], Total Memory 1536GB (24x64GB DDR5 8800 MT/s [8800 MT/s]), BIOS BHSDCRB1.IPC.0033.D57.2406240014, microcode 0x81000290, 1x Ethernet Controller I210 Gigabit Network Connection, 1x SAMSUNG MZWLR7T6HALA-00007 7T, CentOS Stream 9, 6.6.43, for Llama 3.2 11B, run with single instance on 2 sockets with TensorParallel, for Llama 3.2 3B, run multiple Instances (12 instances in total with: 21 cores per instance, Batch Size 10 per instance) on the 2 sockets, Models run with PyTorch nightly build 0918 and latest Intel® Extension for PyTorch*. Test by Intel on 9/20/24. Repository <a class="b_special_a1" href="" rel="">here</a>.</p>
                        <p>Figure 1: Intel Xeon Scalable Processors: Measurement on 5th Gen Intel Xeon Scalable processor using: 2x Intel Xeon Platinum 8593Q, 64cores, HT On, Turbo On, NUMA 4, Integrated Accelerators Available [used]: DLB [2], DSA [2], IAA[2], QAT[on CPU, 2], Total Memory 512GB (16x32GB DDR5 5600 MT/s [5600 MT/s]), BIOS 3B07.TEL2P1, microcode 0x21000200, Samsung SSD 970 EVO Plus 2TB, CentOS Stream 9, 5.14.0-437.el9.x86_64, , run multiple Instances (4 instances in total with: 32 cores per instance, Batch Size 16 per instance) on the 2 sockets, Models run with PyTorch nightly build 0918 and latest Intel® Extension for PyTorch*. Test by Intel on 9/20/24. Repository <a class="b_special_a1" href="" rel="">here</a>.</p>
                        <p>Demo 2, Figure 2: Intel Core Ultra: Llama 3.2 1B and 3B instruct model measurements were completed on an Asus Zenbook S14 laptop with Intel Core Ultra 9 288V platform using 32GB LPDDR5 8533Mhz total memory, Intel graphics driver 101.5736, OpenVINO (openvino-nightly-2024.5.0.dev20240919), Windows* 11 Pro 24H2 version 26100.1742, Best Performance power mode, core isolation enabled, and Batch Size 1. Intel Arc graphics only available on select Intel Core Ultra (Series 2) powered systems; minimum processor power required. OEM enablement required. Check with OEM or retailer for system configuration. Test by Intel on 9/20/24. Repository <a class="b_special_a1" href="" rel="">here</a> for Llama 3.2 1B and 3B instruct. Repository <a class="b_special_a1" href="" rel="">here</a> for Llama 3.2 11B.</p>
                    </section>

                    <section id="dk_notices_desclaimers">
                        <h4>Notices & Disclaimers</h4>
                        <p>Performance varies by use, configuration and other factors. Learn more on the Performance Index site. Performance results are based on testing as of dates shown in configurations and may not reflect all publicly available updates.  See backup for configuration details.  No product or component can be absolutely secure. Your costs and results may vary. Intel technologies may require enabled hardware, software or service activation.</p>
                        <p>Intel Corporation. Intel, the Intel logo, and other Intel marks are trademarks of Intel Corporation or its subsidiaries. Other names and brands may be claimed as the property of others.</p>
                        <p><strong>AI disclaimer:</strong><br>
                            AI features may require software purchase, subscription or enablement by a software or platform provider, or may have specific configuration or compatibility requirements. Details at <a class="b_special_a1" href="" rel="">www.intel.com/AIPC</a>. Results may vary.</p>
                    </section>                    
                </div>
            </div>
        </div>
    </section>
      
     <section style="border-top: 1px solid #d7d7d7;" class="container py-5">
        <h4 class="h6">Product and Performance Information</h4>
        <div class="disclaimer" style="font-size: 12px;"><sup>1</sup> Performance varies by use, configuration and other factors. Learn more at <a class="b_special_a1" href="">www.Intel.com/PerformanceIndex</a>.</div>
        </div>
    </section>

    <!-- footer -->
    <div id="footer"></div>

    <!-- script header and footer -->
    <script>
        // navbar include  
        fetch('/y_index/y_navbar.html')
            .then(response => response.text())
            .then(data => {
                document.getElementById('navbar').innerHTML = data;
            });
        // footer include 
        fetch('/y_index/y_footer.html')
            .then(response => response.text())
            .then(data => {
                document.getElementById('footer').innerHTML = data;
            });
    </script>

    <!-- nav script -->
    <script>
        document.addEventListener('DOMContentLoaded', function () {
        if (document.getElementsByClassName('VK_all_sections section') && document.getElementsByClassName('VK_sidebar_dropdown')) {
            const sections = document.querySelectorAll('.VK_all_sections section');
            const navLinks = document.querySelectorAll('.VK_sidebar_dropdown a');
            const detailsElements = document.querySelectorAll('.VK_sidebar_dropdown details');

            function removeActiveClass() {
                navLinks.forEach(link => link.classList.remove('VK_sidebar_active_link'));
                detailsElements.forEach(details => {
                    details.removeAttribute('open');
                    details.setAttribute('close', ''); // Optional: to visually indicate it's closed
                });
            }

            function activateLinkAndDetails(targetId) {
                navLinks.forEach(link => {
                    if (link.getAttribute('href') === `#${targetId}`) {
                        link.classList.add('VK_sidebar_active_link');
                        let parentDetails = link.closest('details');

                        // Open all ancestor details elements
                        while (parentDetails) {
                            parentDetails.setAttribute('open', '');
                            parentDetails.removeAttribute('close'); // Optional: remove the closed indicator
                            parentDetails = parentDetails.parentElement.closest('details');
                        }
                    }
                });
            }

            function onScroll() {
                let currentSection = '';
                sections.forEach(section => {
                    const sectionTop = section.getBoundingClientRect().top;
                    const sectionBottom = section.getBoundingClientRect().bottom;

                    // Check if the section is in view
                    if (sectionTop <= 100 && sectionBottom >= 0) {
                        currentSection = section.getAttribute('id');
                    }
                });

                if (currentSection) {
                    removeActiveClass();
                    activateLinkAndDetails(currentSection);
                } else {
                    removeActiveClass();
                }
            }

            window.addEventListener('scroll', onScroll);
        } else {
            return
        }
    });
    </script>

    <!-- copy script -->
    <script>
        document.addEventListener("DOMContentLoaded", () => {
            document.querySelectorAll(".mv_copy_icon").forEach((icon) => {
                icon.addEventListener("click", async function () {
                    try {
                        const toolbar = this.closest(".mv_code_toolbar");
                        const codeBlock = toolbar.querySelector(".code-content");
                        const codeContent = codeBlock.innerText;
    
                        // Use Clipboard API to copy text
                        await navigator.clipboard.writeText(codeContent);
    
                        // Hide the copy icon and show the "Copied!" message
                        const message = toolbar.querySelector(".mv_copy_message");
                        this.classList.add("hidden"); // Hide the copy icon
                        message.classList.remove("hidden"); // Show the "Copied!" message
    
                        // Hide the message and show the icon again after 2 seconds
                        setTimeout(() => {
                            message.classList.add("hidden");
                            this.classList.remove("hidden");
                        }, 2000); // Adjust delay as needed
    
                    } catch (err) {
                        console.error('Failed to copy text: ', err);
                    }
                });
            });
        });
    </script>

</body>

</html>