<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Get Started with Generative AI Using Intel® AI</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/css/bootstrap.min.css" rel="stylesheet"
        integrity="sha384-EVSTQN3/azprG1Anm3QDgpJLIm9Nao0Yz1ztcQTwFspd3yD65VohhpuuCOmLASjC" crossorigin="anonymous">

    <link href=" https://fonts.cdnfonts.com/css/intel-clear " rel="stylesheet">
    <link rel="stylesheet" href="../css/owl.carousel.min.css">
    <link rel="stylesheet" href="../css/yatri.css">
    <link rel="stylesheet" href="../css/rushita.css">
    <link rel="stylesheet" href="../css/rushita_automotive.css">

    <link rel="stylesheet" href="../css/owl.theme.default.min.css">
    <style>
        * {
            font-family: 'Intel Clear', sans-serif;
        }

        .dk_new_options_table {
            border: none !important;
            justify-content: center !important;
            text-align: center !important;
        }

        .dk_new_options_table thead tr td {
            margin-left: 0;
            max-width: fit-content;
            white-space: normal;
            overflow: auto;
            background: #fff;
            padding: 1rem;
            text-align: left;
            border-left: .125rem solid #e2e2e2;
            font-weight: 700 !important;
        }

        .dk_new_options_table thead tr:nth-child(even) td {
            background-color: #f7f7f7;
        }

        .mv_intel_amx h2 {
            font-weight: 300;
        }

        .mv_intel_amx {
            font-size: 1.25rem;
        }

        .mv_intel_amx_bg_color {
            background-color: #0068B5;
        }

        .mv_intel_amx_content {
            padding-right: 200px;
        }

        .mv_intel_amx_item {
            align-content: center;
        }

        .mv_intel_amx {
            color: #fff;
            padding: 1rem;
        }

        .mv_intel_amx h3 {
            font-weight: 300;
        }

        .mv_intel_amx p {
            margin-top: 1.5rem;
            margin-bottom: 0rem;
        }

        .mv_intel_amx_image {
            padding: 1rem;
        }

        .mv_intel_amx_image img {
            width: 100%;
            /* height: 199px; */
        }

        .mv_intel_amx_heading_text {
            font-size: 1.2rem;
            line-height: 1.15;
        }

        .VK_sidebar_active_link {
            color: #262626;
            font-weight: 700;
        }

        .VK_sidebar_active_link:hover {
            color: #262626;
        }

        .VK_print_email_font span {
            font-size: 1.375rem;
        }

        .VK_side_bar_postion_stickey {
            position: sticky;
            top: 0px;
            z-index: 3;
            background-color: #fff;
        }

        .VK_side_heading {
            font-size: 26px;
            font-weight: 400;
        }

        .VK_sidebar_dropdown P {
            margin-left: .313rem !important;
            margin-bottom: .313rem !important;
        }

        .VK_sidebar_dropdown details {
            margin-left: .313rem !important;
            margin-bottom: .313rem !important;
        }
    </style>
</head>

<body>
    <div id="navbar"></div>

    <section class="m_ai_tdrop">
        <div>
            <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false">
                Developers
            </button>
            <ul class="dropdown-menu">
                <li><a class="dropdown-item m_dropActive"
                        href="../dhruvin_developer-tools/ds_Developer-Zone.html">Overview</a></li>
                <li><a class="dropdown-item m_dropActive"
                        href="../rushita_Solutions/k_solutions_in_Viewall_SDF_Industrial-Automation_Control-flow-Enforcement_Technology.html">Topics
                        & Technologies</a></li>
                <li><a class="dropdown-item m_dropActive"
                        href="../dhruvin_developer-tools/ds_Development-Tools.html">Tools</a></li>
                <li><a class="dropdown-item m_dropActive"
                        href="../rushita_Solutions/k_solutions_in_Viewall_SDF_Industrial-Automation_Hardware-Platforms.html">Hardware
                        Platforms</a></li>
                <li><a class="dropdown-item m_dropActive"
                        href="../dhruvin_developer-tools/ds_Resource-Documentation-Center.html">Resources &
                        Documentation</a></li>
                <li><a class="dropdown-item m_dropActive" href="../dhruvin_developer-tools/ds_Learn.html">Learn</a>
                </li>
                <li><a class="dropdown-item m_dropActive"
                        href="../dhruvin_developer-tools/ds_Communities-and-Events.html">Community & Events</a></li>
                <li><a class="dropdown-item m_dropActive"
                        href="../dhruvin_developer-tools/ds_Developer-Programs.html">Developer Programs</a></li>
                <li><a class="dropdown-item m_dropActive" href="../dhruvin_developer-tools/ds_Get-Help.html">Get
                        Help</a></li>
            </ul>
        </div>


        <div class="m_ai_shlash">/</div>
        <div>
            <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false">
                Tools
            </button>
            <ul class="dropdown-menu">
                <li><a class="dropdown-item m_dropActive" href="../Product/B20_developer_catelog.html">Software
                        Catalog</a></li>
                <li><a class="dropdown-item m_dropActive" href="../Product/B17_oneapi.html">oneAPI</a></li>
                <li><a class="dropdown-item m_dropActive"
                        href="../rushita_Solutions/k_solutions_Intel-Tiber™Edge-Platform.html">Intel® Tiber™ Edge
                        Platform</a></li>
                <li><a class="dropdown-item m_dropActive" href="../Product/B10_intel_Quarts.html">FPGA</a></li>
                <li><a class="dropdown-item m_dropActive" href="../VK_developers/VK_technology_sdk.html">Intel®
                        Active Management Technology SDK</a></li>
                <li><a class="dropdown-item m_dropActive" href="../VK_developers/VK_intel_adviser.html">Intel®
                        Advisor</a></li>
                <li><a class="dropdown-item m_dropActive" href="#">Intel® AI Reference Models</a></li>
                <li><a class="dropdown-item m_dropActive"
                        href="../rushita_Solutions/k_solutions_Intel-Collaboration-Suite-for-WebRTC.html">Intel®
                        Collaboration Suite for WebRTC SDK</a>
                </li>
                <li><a class="dropdown-item m_dropActive" href="../Product/B1_19_Intel_AICloud.html">Intel® Tiber™
                        AI Cloud</a></li>
                <li><a class="dropdown-item m_dropActive"
                        href="../rushita_Solutions/k_solutions_Intel-Distribution-for-Python.html">Intel® Distribution
                        for Python*</a></li>
                <li><a class="dropdown-item m_dropActive"
                        href="../rushita_Solutions/k_solutions_Intel-Distribution-for-GDB.html">Intel® Distribution for
                        GDB*</a></li>
                <li><a class="dropdown-item m_dropActive" href="#">Intel® DPC++ Compatibility Tool</a></li>
                <li><a class="dropdown-item m_dropActive" href="#">Intel® Dynamic Application Loader</a></li>
                <li><a class="dropdown-item m_dropActive"
                        href="../rushita_Solutions/k_solutions_Topics_viewall_Cloud-Insider-Program_AI-Frameworks-and-Tools.html">Frameworks</a>
                </li>
                <li><a class="dropdown-item m_dropActive" href="#">Intel® Embree</a></li>
                <li><a class="dropdown-item m_dropActive" href="../VK_developers/VK_ai_scikit.html">Intel® Extension
                        for Scikit-learn*</a></li>
                <li><a class="dropdown-item m_dropActive"
                        href="../rushita_Solutions/k_solutions_Intel-Fortran-Compiler.html">Intel® Fortran Compiler</a>
                </li>
                <li><a class="dropdown-item m_dropActive" href="#">Intel® Graphics Performance Analyzers</a></li>
                <li><a class="dropdown-item m_dropActive"
                        href="../rushita_Solutions/k_solutions_Intel-HE-Toolkit.html">Intel® Homomorphic Encryption
                        Toolkit</a></li>
                <li><a class="dropdown-item m_dropActive"
                        href="../rushita_Solutions/k_solutions_Intel-In-Band-Manageability.html">Intel® In-Band
                        Manageability</a></li>
                <li><a class="dropdown-item m_dropActive"
                        href="../rushita_Solutions/k_solutions_Intel-Cryptography-Primitives-Library.html">Intel®
                        Integrated Performance Primitives</a></li>
                <li><a class="dropdown-item m_dropActive" href="../rushita_Solutions/">Intel® Integrated Simulation
                        Infrastructure with
                        Modeling</a></li>
                <li><a class="dropdown-item m_dropActive"
                        href="../rushita_Solutions/k_solutions_Instruction-Set-Architecture(ISA)Extensions.html">Instruction
                        Set Architecture (ISA) Extensions</a>
                </li>
                <li><a class="dropdown-item m_dropActive"
                        href="../rushita_Solutions/k_solutions_Intel-Intelligent-Storage-Acceleration-Library.html">Intel®
                        Intelligent Storage Acceleration Library
                        (Intel® ISA-L)</a></li>
                <li><a class="dropdown-item m_dropActive" href="#">Modin*</a></li>
                <li><a class="dropdown-item m_dropActive" href="#">Intel® MPI Library</a></li>
                <li><a class="dropdown-item m_dropActive" href="#">Intel® Neural Compressor</a></li>
                <li><a class="dropdown-item m_dropActive" href="#">OpenCL™ Runtime</a></li>
                <li><a class="dropdown-item m_dropActive" href="#">Intel® Open Image Denoise</a></li>
                <li><a class="dropdown-item m_dropActive" href="#">Intel® Open Path Guiding Library (Intel® Open
                        PGL)</a></li>
                <li><a class="dropdown-item m_dropActive" href="#">Pin</a></li>
                <li><a class="dropdown-item m_dropActive" href="#">Intel® Platform Analysis Library</a></li>
                <li><a class="dropdown-item m_dropActive" href="#">Intel Tools for OpenCL™ Software</a></li>
                <li><a class="dropdown-item m_dropActive" href="#">Intel Tools for OpenCL™ Applications</a></li>
                <li><a class="dropdown-item m_dropActive" href="#">Intel® OpenSWR</a></li>
                <li><a class="dropdown-item m_dropActive" href="#">OpenVINO™ Toolkit</a></li>
                <li><a class="dropdown-item m_dropActive" href="#">Intel® Open Volume Kernel Library</a></li>
                <li><a class="dropdown-item m_dropActive" href="#">Intel® Optimization for XGBoost*</a></li>
                <li><a class="dropdown-item m_dropActive" href="#">Intel® OSPRay</a></li>
                <li><a class="dropdown-item m_dropActive" href="#">Intel® OSPRay for Hydra*</a></li>
                <li><a class="dropdown-item m_dropActive" href="#">Intel® OSPRay Studio</a></li>
                <li><a class="dropdown-item m_dropActive" href="#">PyTorch* Optimizations from Intel</a></li>
                <li><a class="dropdown-item m_dropActive" href="#">TensorFlow* Optimizations from Intel</a></li>
                <li><a class="dropdown-item m_dropActive" href="#">Intel® Rendering Toolkit</a></li>
                <li><a class="dropdown-item m_dropActive" href="#">Intel® Smart Edge Open</a></li>
                <li><a class="dropdown-item m_dropActive" href="#">Intel® Software Guard Extensions</a></li>
                <li><a class="dropdown-item m_dropActive" href="#">Intel® Secure Device Onboard</a></li>
                <li><a class="dropdown-item m_dropActive" href="#">Intel® Software Development Emulator</a></li>
                <li><a class="dropdown-item m_dropActive" href="#">Intel® Trust Domain Extensions (Intel® TDX)</a>
                </li>
                <li><a class="dropdown-item m_dropActive" href="#">Intel® Quantum SDK</a></li>
                <li><a class="dropdown-item m_dropActive" href="#">Intel® Query Processing Library (Intel® QPL)</a>
                </li>
                <li><a class="dropdown-item m_dropActive" href="#">Intel® Video Processing Library</a></li>
                <li><a class="dropdown-item m_dropActive" href="#">Intel® VTune™ Profiler</a></li>
                <li><a class="dropdown-item m_dropActive" href="#">Resellers</a></li>
            </ul>
        </div>
        <div class="m_ai_shlash">/</div>
        <div>
            <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false">
                oneAPI
            </button>
            <ul class="dropdown-menu">
                <li><a class="dropdown-item m_dropActive" href="#">Overview</a></li>
                <li><a class="dropdown-item m_dropActive" href="#">Data Parallel C++/SYCL*</a></li>
                <li><a class="dropdown-item m_dropActive" href="#">Toolkits</a></li>
                <li><a class="dropdown-item m_dropActive" href="#">Tech Articles & How-Tos</a></li>
                <li><a class="dropdown-item m_dropActive" href="#">Components</a></li>
                <li><a class="dropdown-item m_dropActive" href="#">Code Samples</a></li>
                <li><a class="dropdown-item m_dropActive" href="#">Training</a></li>
                <li><a class="dropdown-item m_dropActive" href="#">Documentation</a></li>
                <li><a class="dropdown-item m_dropActive" href="#">Support</a></li>
            </ul>
        </div>
        <div class="m_ai_shlash">/</div>
        <div>
            <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false">
                Tech Articles & How-Tos
            </button>
            <ul class="dropdown-menu">
                <li><a class="dropdown-item m_dropActive" href="#">Library</a></li>
                <li><a class="dropdown-item m_dropActive" href="#">News Updates</a></li>
                <li><a class="dropdown-item m_dropActive" href="#">Webinars</a></li>
            </ul>
        </div>
        <div class="m_ai_shlash">/</div>
        <div class="m_ai_httl">Get Started with Generative AI Using Intel® AI
        </div>
    </section>
    <section class="k_bgblue">
        <div class="k_container11">
            <div class="row text-white py-4">
                <h3 class="fw-light">A Developer's Guide to Getting Started with Generative AI: A Use-Case-Specific
                    Approach</h3>
            </div>
        </div>
    </section>
    <section class="k_space k_spacenone">
        <div class="k_container11">
            <div class="row  g-3">
                <div class="float-end k_bignone" style="padding-left:70%;">
                    <div>
                        <a href="#" class="fs-4"><i class="fa-solid fa-print mx-3 k_icon0"></i></a>
                        <a href="#" class="fs-4"> <i class="fa-regular fa-envelope k_icon0"></i></a>
                    </div>
                </div>
                <div class="col-lg-3 col-md-4 col-sm-12">
                    <div class="VK_sticky_side_bar VK_side_bar_postion_stickey">
                        <div>
                            <div class="VK_sidebar_dropdown">
                                <ul class="k_listnone k_menu ">
                                    <li class="k_text px-2"><a href="#section1">More Effective NLP Training</a></li>
                                    <li class="k_text px-2"><a href="#section2">Take the Use Case Approach for AI
                                            Application Development</a></li>
                                    <li class="k_text px-2"><a href="#section2">How Generative AI Technologies from
                                            Intel Can Help</a></li>
                                    <li class="k_text px-2"><a href="#section2">Generative AI for Customer Service:
                                            Chatbot Use Case</a></li>
                                    <li class="k_text px-2"><a href="#section2">Generative AI for Retail: Virtual Try-on
                                            Use Case</a></li>
                                    <li class="k_text px-2"><a href="#section2">Generative AI for Healthcare: Patient
                                            Monitoring Use Case</a></li>
                                    <li class="k_text px-2"><a href="#section2">How to Get Started</a></li>
                                    <li class="k_text px-2"><a href="#section2">Resources</a></li>

                                </ul>
                            </div>
                        </div>
                    </div>
                    <div>
                        <p><b>Get the Latest on All Things CODE</b></p>
                        <p class="text-center k_btnget py-2 k_under k_btn100 my-2 kcokbtn1024" style="width: 80px;">
                            <a href="#" class="text-white">Sign Up</a>
                        </p>
                        <div class=" mt-4 ">
                            <div>
                                <p>Jack Erickson, principal product marketing manager, AI software, Intel</p>
                                <p>Chandan Damannagari, director, AI Software, Intel</p>
                            </div>
                        </div>
                    </div>
                </div>
                <div class="col-lg-8 col-md-7 col-sm-12  k_16smpx k_width75">
                    <div class="float-end mx-3 k_smnone">
                        <div>
                            <a href="#" class="fs-4"><i class="fa-solid fa-print mx-3 k_icon0"></i></a>
                            <a href="#" class="fs-4"> <i class="fa-regular fa-envelope k_icon0"></i></a>
                        </div>
                    </div>
                    <div class=" k_smpaddnone k_smpaddnone1 pt-5">
                        <p class="k_plink1 k_intext1blue1">This article was originally published on <a
                                href="#">VentureBeat</a>.</p>
                       <div class="my-5">
                            <img src="../img/rushita_img/llam7.jpeg" alt="" width="100%">
                       </div>
                       <p class="k_plink1 k_intext1blue1">Generative AI promises to greatly enhance human productivity but only a handful of enterprises possess the skills and <a href="#">resources</a> to develop and train from scratch the foundation models essential for its deployment. The challenges are twofold. First, collecting the data to train the models was already challenging and has become even more so as content owners assert their intellectual property rights. Next, the resources needed for training can be prohibitively expensive. However, the societal value of <a href="#">unlocking the availability of and access to generative AI technologies</a> remains high.</p>
                       <p>So, how can small enterprises or individual developers incorporate generative AI into their applications? By creating and deploying custom versions of the larger foundation models.</p>
                       <p>The large investment and effort to develop new generative AI models means that they will need to be general enough to address a wide range of uses—consider all the ways in which GPT*-based models have been used already. However, a general-purpose model often can't address the domain-specific needs of individual and enterprise use cases. Using a large general-purpose model for a narrow application also consumes excess computing resources, time, and energy.</p>
                       <p class="k_plink1 k_intext1blue1">Therefore, most enterprises and developers can find the best fit for their requirements and budget by starting with a large generative AI model as a foundation to adapt to their own needs at a fraction of the development effort. This also provides infrastructure flexibility by using existing CPUs or AI accelerators instead of being limited by <a href="#">shortages</a> of specific GPUs. The key is to focus on the specific use case and narrow the scope while maximizing project flexibility by using <a href="#">open, standards-based software</a> and <a href="#">ubiquitous hardware</a>.</p>

                      
                        <h2 class="fw-light mt-5" id="section1">Take the Use Case Approach for AI Application Development</h2>
                        <p>In software development, a use case defines the characteristics of the target user, the problem to be solved, and how the application will be used to solve it. This defines product requirements, dictates the software architecture, and provides a roadmap for the product lifecycle. Most crucially, this scopes the project and defines what does not need to be included.</p>
                        <p>Similarly, in the case of a generative AI project, defining a use case can reduce the size, compute requirements, and energy consumption of the AI model. At the same time, it can improve model accuracy by focusing on a specific dataset. Along with these benefits come reduced development effort and costs.</p>
                        <p>The factors that define a use case for generative AI will vary by project, but some common helpful questions can guide the process:</p>
                        <ul>
                            <li>Data requirements: What, and how much, training data is necessary and available? Is the data structured (data warehouse) or unstructured (data lake)? What are the regulations or restrictions with it? How will the application process the data—through batch or streaming? How often do you need to maintain or update the model? Training large language models (LLMs) from scratch takes so much time that they lack awareness of recent knowledge—so if being up-to-date is important to your application, then you would need a different approach. Or, if you are developing a healthcare application, privacy, and security restrictions on patient data typically dictate unique approaches to training and inference.</li>
                            <li class="k_plink1 k_intext1blue1">Model requirements: Model size, model performance, openness, and explainability of results are all important considerations when choosing the right model for you. Performant LLM models range in size from billions to trillions of parameters—<a href="#">Llama 2</a> from Meta* offers versions ranging from 7 billion to 70 billion parameters, while <a href="#">GPT-4*</a> from OpenAI reportedly has 1.76 trillion parameters. While larger model sizes are typically associated with higher performance, smaller models may better fit your overall requirements. Open models offer more choices for customization, whereas closed models work well off the shelf but are limited to API access. Control over customization allows you to ground the model in your data with traceable results, which would be important in an application such as generating summaries of financial statements for investors. On the other hand, allowing an off-the-shelf model to extrapolate beyond its trained parameters ("hallucinate") may be perfectly fine for generating ideas for advertising copy.</li>
                            <li>Application requirements: What are the accuracy, latency, privacy, and safety standards that must be met? How many simultaneous users does it need to handle? How will users interact with it? For example, your implementation decisions will depend on whether your model should run on a low-latency edge device owned by the end-user or in a high-capacity cloud environment where each inference call costs you money.</li>
                            <li class="k_plink1 k_intext1blue1">Compute requirements: Once the previous requirements are understood, what compute resources are required to meet them? Do you need to parallelize your pandas data processing <a href="#">using Modin*</a>? Do your fine-tuning and inference requirements differ enough to require a hybrid cloud-edge compute environment? While you may have the talent and data to train a generative AI model from scratch, consider whether you have the budget to overhaul your compute infrastructure.</li>
                        </ul>
                        <p>The previous factors will help drive conversations to define and scope the project requirements. Economics also factor in—the budget for data engineering, up-front development costs, and the ultimate business model that will provide a requirement for the inference costs dictate the data, training, and deployment strategies.</p>

                        <h2 class="fw-light mt-5" id="section1">How Generative AI Technologies from Intel Can Help</h2>
                        <p class="k_plink1 k_intext1blue1">Intel provides <a href="#">heterogeneous AI hardware</a> options for a wide variety of compute requirements. To get the most out of your hardware, Intel provides optimized versions of the data analysis and <a href="#">end-to-end AI tools</a> most teams use today. More recently, Intel has begun providing optimized models, including the number-one ranked 7B parameter model on the Hugging Face* <a href="#">open LLM leaderboard</a> (as of November 2023). These tools and models, together with those provided by its <a href="#">AI developer ecosystem</a>, can satisfy your application's accuracy, latency, and security considerations. First, you can start with the hundreds of pretrained models on <a href="#">Hugging Face</a> or <a href="#">GitHub*</a> that are optimized for Intel® hardware. Next, you can preprocess your data using Intel-optimized tools such as <a href="#">Modin</a>, fine-tune foundation models using application-specific optimization tools such as <a href="#">Intel® Extension for Transformers*</a> or <a href="#">Hugging Face* Optimum</a>, and automate model tuning with <a href="#">SigOpt®</a>. All of this builds on the optimizations that Intel has already contributed to open source <a href="#">AI frameworks</a>, including TensorFlow*, PyTorch*, and DeepSpeed*.</p>
                        <p>Let's illustrate with some generative AI use case examples for customer service, retail, and healthcare applications.</p>

                        <h2 class="fw-light mt-5" id="section1">Generative AI for Customer Service: Chatbot Use Case</h2>
                        <p class="k_plink1 k_intext1blue1"><a href="#">Chatbots</a> based on LLMs can improve customer service efficiency by providing instant answers to common questions, freeing customer service representatives to focus on more complex cases.</p>
                        <p>Foundation models are already trained to converse in multiple languages on a broad range of topics but lack depth on the offerings of a given business. A general-purpose LLM may also hallucinate, confidently generating output even in the absence of trained knowledge.</p>
                        <p class="k_plink1 k_intext1blue1">Fine-tuning and retrieval are two of the more popular methods to customize a foundation model. <a href="#">Fine-tuning</a> incrementally updates a foundation model with custom information. Retrieval-based methods, such as <a href="#">retrieval-augmented generation (RAG)</a>, fetch information from a database external to the model. This database is built using the offering-specific data and documents, vectorized for use by the AI model. Both methods deliver offering-specific results and can be updated using only CPUs (such as <a href="#">Intel® Xeon® Scalable processors</a>), which are ubiquitous and more readily available than specific accelerators.</p>
                        <p>The use case helps determine which method best fits the application’s requirements. Fine-tuning offers latency advantages since the knowledge is built into the generative AI model. Retrieval offers traceability from its answers directly to actual sources in the knowledge base, and updating this knowledge base does not require incremental training.</p>
                        <p class="k_plink1 k_intext1blue1">It’s also important to consider the compute requirements and costs for ongoing inference operations. The transformer architecture that powers most chatbots is usually limited more by <a href="#">memory bandwidth</a> than raw compute power. Model optimization techniques such as <a href="#">quantization</a> can reduce the memory bandwidth requirements, which reduces latency and inference compute costs.</p>
                        <p>There are plenty of foundation models to choose from. Many come in different parameter sizes. Starting with a clearly defined use case helps choose the right starting point and dictates how to customize it from there.</p>
                        <div class="my-4">
                            <img src="../img/rushita_img/llam8.jpg" alt="" width="100%">
                        </div>
                        <p>Customize a chatbot foundation model with RAG.</p>

                        <h2 class="fw-light mt-5" id="section1">Generative AI for Retail: Virtual Try-on Use Case</h2>
                        <p>Retailers can use generative AI to offer their customers a better, more immersive online experience. An example is the ability to try on clothes virtually so they see how they look and fit before buying. This improves customer satisfaction and retail supply chain efficiency by reducing returns and better forecasting customers' wants.</p>
                        <p class="k_plink1 k_intext1blue1">This use case is based on image generation, but the foundation model must be focused on generating images using the retailer’s clothing line. <a href="#">Fine-tuning</a> image-based foundation models such as Stable Diffusion* may only require a small number of images running on CPU platforms. Techniques such as <a href="#">Low-Rank Adaptation (LoRA)</a> can more surgically insert the retailer’s offerings into the Stable Diffusion model.</p>
                        <p>The other key input to this use case is the imagery or scan of the customer’s body. The use case implications start with how to preserve the customer's privacy. The images must stay on the local edge device, perhaps the customer's phone or a locally installed image capture device.</p>
                        <p class="k_plink1 k_intext1blue1">Does this mean the entire generative AI pipeline must run on the edge, or can this application be architected in a way that encodes the necessary information from the images to upload to the rest of the model running in a data center or cloud? This type of architecture decision is the domain of <a href="#">MLOps professionals</a>, who are vital to the successful development of generative AI applications.</p>
                        <p class="k_plink1 k_intext1blue1">Now, given that some amount of AI inference needs to run efficiently on a variety of edge devices, it becomes vital to choose a <a href="#">framework</a> that can optimize for deployment without rewriting code for each type of device.</p>
                        <div class="my-4">
                            <img src="../img/rushita_img/llam9.jpg" alt="" width="100%">
                        </div>
                        <p>See a generative AI virtual try-on application in action.</p>

                        <h2 class="fw-light mt-5" id="section1">Generative AI for Healthcare: Patient Monitoring Use Case</h2>
                        <p>Pairing generative AI with real-time patient monitoring data can generate personalized reports, action plans, or interventions. Synthesizing data, imagery, and case notes into a summary or a recommendation can improve healthcare provider productivity while reducing the need for patients to travel to or stay in healthcare facilities.</p>
                        <p class="k_plink1 k_intext1blue1">This use case requires multimodal AI, which combines different types of models to process the heterogeneous input data, likely combined with an LLM to generate reports. Because this is a more complex use case, starting with a <a href="#">multimodal reference implementation</a> for a similar use case may accelerate a project.</p>
                        <p class="k_plink1 k_intext1blue1">Training healthcare models typically raises patient data privacy questions. Often, patient data must remain with the provider, so collecting data from multiple providers to train or fine-tune a model becomes impossible. <a href="#">Federated learning</a> addresses this by sending the model to the data locations for training locally and then combining the results from the various locally trained models.</p>
                        <p class="k_plink1 k_intext1blue1">Inference also needs to maintain patient privacy. The most straightforward approach would be to run inference locally to the patient. Given the size and complexity of a multimodal generative AI system, running entirely on edge devices may be challenging. It may be possible to architect the system to combine edge and data center processing, but <a href="#">model optimization</a> techniques will likely still be required for the models running on edge devices.</p>
                        <p class="k_plink1 k_intext1blue1">Developing a hybrid MLOps architecture like this is much more efficient if the AI <a href="#">tools</a> and frameworks run optimally on a variety of devices without having to rewrite low-level code to optimize for each type of device.</p>
                        <p>Learn about the architecture behind a patient monitoring system.</p>

                        <h2 class="fw-light mt-5" id="section1">How to Get Started</h2>
                        <p>Start by doing your best to define your use case, using the previous questions as guidance to determine the data, compute, model, and application requirements for the problem you are trying to solve with generative AI.</p>
                        <p class="k_plink1 k_intext1blue1">Then, determine what relevant foundation models, <a href="#">reference implementations</a>, and resources are available in the <a href="#">AI ecosystem</a>. From there, identify and implement the fine-tuning and model optimization techniques most relevant to your use case.</p>
                        <p class="k_plink1 k_intext1blue1">Compute needs will likely not be apparent at the beginning of the project and typically evolve throughout the project. <a href="#">Intel® Tiber™ AI Cloud</a> offers access to a variety of CPUs, GPUs and AI accelerators to try out or to get started developing with.</p>
                        <p class="k_plink1 k_intext1blue1">Finally, to efficiently adapt to different compute platforms during development and then to deployment, use <a href="#">AI tools and frameworks</a> that are open, standards-based, and run optimally on any of the above devices without having to rewrite low-level code for each type of device.</p>

                        <h2 class="fw-light mt-5" id="section1">Resources</h2>
                        <p class="k_plink1 k_intext1blue1"><a href="#">Intel® AI Software</a></p>
                        <p class="k_plink1 k_intext1blue1"><a href="#">Intel Tiber AI Cloud</a></p>
                        <p class="k_plink1 k_intext1blue1"><a href="#">oneAPI for Unified Programming</a></p>

                        
                    </div>
                </div>
            </div>
    </section>
    <section class="my-4" style="font-size: 13px;">
        <div class="k_container11">
            <div class="row">
                <div>
                    <hr class="mb-5">
                    <p>Product and Performance Information </p>

                    <div class="k_plink1 k_intext1blue1"><sup>1</sup>Performance varies by use, configuration and other
                        factors. Learn more at &nbsp;<a href="#">www.Intel.com/PerformanceIndex.</a>.
                    </div>
                </div>
            </div>
        </div>
    </section>
    <div id="footer"></div>
    <script>
        // navbar include  
        fetch('../y_index/y_navbar.html')
            .then(response => response.text())
            .then(data => {
                document.getElementById('navbar').innerHTML = data;
            });
        // footer include 
        fetch('../y_index/y_footer.html')
            .then(response => response.text())
            .then(data => {
                document.getElementById('footer').innerHTML = data;
            });
    </script>
    <script src="https://cdn.jsdelivr.net/npm/@splidejs/splide@4.1.4/dist/js/splide.min.js"></script>
    <script src="../js/jquery-3.7.1.js"></script>

    <script src="../js/owl.carousel.min.js"></script>
    <script src="../js/rushita.js"></script>
</body>

</html>