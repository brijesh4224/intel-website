<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Document</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/css/bootstrap.min.css" rel="stylesheet"
        integrity="sha384-EVSTQN3/azprG1Anm3QDgpJLIm9Nao0Yz1ztcQTwFspd3yD65VohhpuuCOmLASjC" crossorigin="anonymous">

    <link href=" https://fonts.cdnfonts.com/css/intel-clear " rel="stylesheet">
    <link rel="stylesheet" href="../css/owl.carousel.min.css">

    <link rel="stylesheet" href="../css/rushita.css">
    <link rel="stylesheet" href="../css/rushita_automotive.css">
    <link rel="stylesheet" href="../css/yatri.css">
    <link rel="stylesheet" href="../css/owl.theme.default.min.css">
    <style>
        * {
            font-family: 'Intel Clear', sans-serif;
        }
    </style>
</head>

<body>
    <div id="navbar"></div>

    <section class="m_ai_tdrop" style="padding-top: 75px;">
        <div class="m_ai_httl">How to Implement RAG</div>
    </section>
    <section class="k_bgblue">
        <div class="k_container11">
            <div class="row k_bgpadd pt-5 py-3">
                <div
                    class="col-lg-8 col-md-12 col-sm-12   mx-0 text-white align-content-center  k_tstartauto k_text2  px-0 mx-0 k_mdorder2 ">
                    <h3 class="fs-3 mb-2 fw-lighter k_marketsmall">Implement Retrieval-Augmented Generation (RAG) to Accelerate LLM Application Development</h3>
                    <p class=" fs-5">Learn how you can customize large language models (LLMs) more cost-effectively.</p>
                </div>
                <div class="col-lg-3 col-md-12 col-sm-12 k_mdimg  k_mdorder1">
                    <img src="../img/rushita_img/rag.avif" alt="" width="100%" >
                </div>
            </div>
        </div>
    </section>
    <section class="my-3 k_spacenone">
        <div class="k_container11">
            <div class="row g-3">
                <div class="float-end k_bignone" style="padding-left:70%;">
                    <div>
                        <a href="#" class="fs-4"><i class="fa-solid fa-print mx-3 k_icon0"></i></a>
                        <a href="#" class="fs-4"> <i class="fa-regular fa-envelope k_icon0"></i></a>
                    </div>
                </div>
                <div class="col-lg-3 col-md-4 col-sm-12">
                    <!-- <div id="k_sticky-section"> -->
                    <div>
                        <div>
                            <div class="tab-container">
                                <div class="k_btn k_incorbtn1 k_text active" data-tab="key1">
                                    <a href="#key1">Understand the Benefits of RAG</a>
                                </div>
                                <div class="k_btn k_incorbtn1 k_text" data-tab="key2">
                                    <a href="#key2">Select Hardware</a>
                                </div>
                                <div class="k_btn k_incorbtn1 k_text" data-tab="key3">
                                    <a href="#key3">Select a Framework</a>
                                </div>
                                <div class="k_btn k_incorbtn1 k_text" data-tab="key3">
                                    <a href="#key4">Build Your Knowledge Base</a>
                                </div>
                                <div class="k_btn k_incorbtn1 k_text" data-tab="key3">
                                    <a href="#key4">Optimize</a>
                                </div>
                                <div class="k_btn k_incorbtn1 k_text" data-tab="key3">
                                    <a href="#key4">Accelerate RAG with Intel</a>
                                </div>
                            </div>
                        </div>
                    </div>
                    <div class="k_bggrey p-3">
                        <p><b>Key Takeaways</b></p>
                        <ul>
                            <li><p data-ninja-font="intel-clear_regular_normal_sw50z" class="">RAG is an ideal approach for your LLM application if you don’t have the time or budget for fine-tuning.</p>
                            </li>
                            <li><p data-ninja-font="intel-clear_regular_normal_sw50z" class="">Choose a compute platform that can power the full pipeline, including demanding LLM inferencing workloads.</p>
                            </li>
                            <li><p data-ninja-font="intel-clear_regular_normal_sw50z" class="">Implement an integrated RAG framework such as LangChain or Intel Lab’s fastRAG to help streamline development. </p>
                            </li>
                            <li><p data-ninja-font="intel-clear_regular_normal_sw50z" class="">Take advantage of purpose-built processors and key optimizations to maximize RAG pipeline performance.</p>
                            </li>
                            <li><p data-ninja-font="intel-clear_regular_normal_sw50z" class="">Test RAG application performance on the Intel® AI portfolio and cloud providers with the Intel® Tiber™ Developer Cloud</p>
                            </li>
                </ul>
                    </div>
                </div>
                <div class="col-lg-8 col-md-7 col-sm-12 k_20px k_16smpx k_width75">
                    <div class="float-end mx-3 k_smnone">
                        <div>
                            <a href="#" class="fs-4 k_iconauto"><i class="fa-solid fa-print mx-3 k_icon0"></i></a>
                            <a href="#" class="fs-4"> <i class="fa-regular fa-envelope k_icon0"></i></a>
                        </div>
                    </div>
                    <div>
                        <div class="my-5 k_smpaddnone" style="padding-top: 40px !important;">
                            <h4 class="fw-lighter">
                                Retrieval-augmented generation (RAG) enables you to customize LLMs without fine-tuning, helping you save money and accelerate time to deployment. Read on to find out how to optimize RAG implementation for your organization.</h4>
                        </div>
                        <h2 class="k_26px mt-5 pt-5"><strong>Accelerate Your RAG and Generative AI Success</strong></h2>
                        <p>Large language model (LLM) applications, such as chatbots, are unlocking powerful benefits across industries. Organizations use LLMs to reduce operational costs, boost employee productivity, and deliver more-personalized customer experiences.</p>
                        <p>As organizations like yours race to turn this revolutionary technology into a competitive edge, a significant portion will first need to customize off-the-shelf LLMs to their organization’s data so models can deliver business-specific AI results. However, the cost and time investments required by fine-tuning models can create sizable roadblocks that hold many would-be innovators back.</p>
                        <p>To overcome these barriers, retrieval-augmented generation (RAG) offers a more cost-effective approach to LLM customization. By enabling you to ground models on your proprietary data without fine-tuning, RAG can help you quickly launch LLM applications tailored to your business or customers. Instead of requiring retraining or fine-tuning, the RAG approach allows you to connect the off-the-shelf LLM to a curated external knowledge base built on your organization’s unique, proprietary data. This knowledge base informs the model’s output with organization-specific context and information.</p>
                        <p>In this article, you’ll learn how to set up key components of your RAG implementation, from choosing your hardware and software foundations to building your knowledge base and optimizing your application in production. We’ll also share tools and resources that can help you get the most power and efficiency out of each phase of the pipeline.</p>

                        <h2 class="k_26px mt-5 pt-3"><strong>When Is RAG the Right Approach?</strong></h2>
                        <p>Before you start evaluating pipeline building blocks, it’s important to consider whether RAG or fine-tuning is the best fit for your LLM application.</p>
                        <p>Both approaches start with a foundational LLM, offering a shorter pathway to customized LLMs than training a model from scratch. Foundational models have been pretrained and don’t require access to massive datasets, a team of data experts, or extra computing power for training.</p>
                        <p>However, once you choose a foundational model, you’ll still need to customize it to your business, so your model can deliver results that address your challenges and needs. RAG can be a great fit for your LLM application if you don’t have the time or money to invest in fine-tuning. RAG also reduces the risk of hallucinations, can provide sources for its outputs to improve explainability, and offers security benefits since sensitive information can be kept safely in private databases.</p>
                        <p><a href="#" style="color:#0563c1; text-decoration:underline">Learn more about the benefits RAG can bring to your generative AI initiative</a></p>


                        <h2 class="k_26px mt-5 pt-3"><strong>Choose Hardware that Prioritizes Performance and Security</strong></h2>
                        <p>The RAG pipeline includes many computationally intensive components, and end users expect low-latency responses. This makes choosing your compute platform one of the most important decisions you’ll make as you seek to support the pipeline from end to end.</p>
                        <p><a href="#" style="color:#0563c1; text-decoration:underline">Intel® Xeon® processors</a>&nbsp;enable you to power and manage the full RAG pipeline on a single platform, streamlining development, deployment, and maintenance. Intel® Xeon® processors include integrated AI engines to accelerate key operations across the pipeline—including data ingesting, retrieval, and AI inference—on the CPU without the need for additional hardware.</p>
                        <p>For RAG applications that require the highest throughput or lowest latency, you can integrate <a href="#" style="color:#0563c1; text-decoration:underline">Intel® Gaudi® AI accelerators</a>&nbsp;to meet advanced performance demands cost-effectively. Intel® Gaudi® accelerators are purpose-built to accelerate inferencing and can even replace CPUs and other accelerators for RAG inference.</p>
                        <p>Because organizations often use RAG when working with confidential data, securing your pipeline during development and in production is paramount. Intel® Xeon® processors use built-in security technologies—<a href="#" style="color:#0563c1; text-decoration:underline">Intel® Software Guard Extensions (Intel® SGX)</a>&nbsp;and <a href="#" style="color:#0563c1; text-decoration:underline">Intel® Trust Domain Extensions (Intel® TDX)&nbsp;</a>—to enable secure AI processing across the pipeline via confidential computing and data encryption.</p>
                        <p>Once deployed, your application may experience increased latency due to an uptick in end user demand. Intel® hardware is highly scalable, so you can quickly add infrastructure resources to meet growing use. You can also integrate optimizations to support key operations across the pipeline, such as data vectorization, vector search, and LLM inference.</p>
                        <p>You can test RAG performance on Intel® Xeon® and Intel® Gaudi® AI processors via the <a href="#" style="color:#0563c1; text-decoration:underline">Intel® Tiber™ Developer Cloud</a></p>

                        <h2 class="k_26px mt-5 pt-3"><strong>Use a RAG Framework to Easily Integrate AI Toolchains</strong></h2>
                        <p>To connect many components, RAG pipelines combine several AI toolchains for data ingestion, vector databases, LLMs, and more.</p>
                        <p>As you begin developing your RAG application, integrated RAG frameworks such as <a href="#" style="color:#0563c1; text-decoration:underline">LangChain</a>, <a href="#" style="color:#0563c1; text-decoration:underline">Intel Lab’s fastRAG</a>, and <a href="#" style="color:#0563c1; text-decoration:underline">LlamaIndex</a>&nbsp;can streamline development. RAG frameworks often provide APIs to integrate AI toolchains across the pipeline seamlessly and offer template-based solutions for real-world use cases.</p>
                        <p>Intel offers optimizations to help maximize overall pipeline performance on Intel® hardware. For example, fastRAG integrates <a href="#" style="color:#0563c1; text-decoration:underline">Intel® Extension for PyTorch</a>&nbsp;and <a href="#" style="color:#0563c1; text-decoration:underline">Optimum Habana</a>&nbsp;to optimize RAG applications on Intel® Xeon® processors and Intel® Gaudi® AI accelerators.</p>
                        <p>Intel has also contributed optimizations to LangChain to enhance performance on Intel® hardware.&nbsp;<a href="#" style="color:#0563c1; text-decoration:underline">Find out how you can easily set up this workflow using LangChain and Intel® Gaudi® 2 AI accelerators</a></p>


                        <h2 class="k_26px mt-5 pt-3"><strong>Build Your Knowledge Base</strong></h2>
                        <p>RAG allows organizations to feed LLMs important proprietary information about their business and customers. This data is stored in a vector database you can build yourself.</p>
                        <h4 class="my-3"><b>Identify Information Sources</b></h4>
                        <p>Imagine using RAG to deploy an AI personal assistant that can help answer employee questions about your organization. You could feed an LLM key data such as product information, company policies, customer data, and department-specific protocol. Employees could ask the RAG-powered chatbot questions and get organization-specific answers, helping employees complete tasks more quickly and empowering them to focus on strategic thinking.</p>
                        <p>Of course, knowledge bases will vary across different industries and applications. A pharmaceutical company may want to use an archive of test results and patient history. A manufacturer could feed equipment specs and historical performance data to a RAG-based robotic arm so it can detect potential equipment issues early. A financial institution may want to connect an LLM to proprietary financial strategies and real-time market trends to enable a chatbot to provide personalized financial advice.</p>
                        <p>Ultimately, to build your knowledge base, you need to collect the important data you want your LLM to access. This data can come from a variety of text-based sources, including PDFs, video transcripts, emails, presentation slides, and <a href="#" style="color:#0563c1; text-decoration:underline">even tabular data from sources such as Wikipedia pages and spreadsheets</a>. RAG also supports multimodal AI solutions, which combine multiple AI models to process data of any modality, including sound, images, and video.</p>
                        <p>For instance, a retailer could use a multimodal RAG solution to search surveillance footage for key events quickly. To do this, the retailer would create a database of video footage and use text prompts—such as “man putting something in his pocket”—to identify relevant clips without having to search through hundreds of hours of video manually.</p>
                        <h4 class="my-3 pt-3"><strong>Prepare Your Data</strong></h4>
                        <p>To prepare your data for efficient processing, you will first need to clean up the data, such as by removing duplicate information and noise, and break it into manageable chunks. <a href="#" style="color:#0563c1; text-decoration:underline">You can read more tips for cleaning up your data here</a></p>
                        <p>Next, you’ll need to use an AI framework called an embedding model to convert your data into vectors, or mathematical representations of the text that help the model understand greater context. Embedding models can be downloaded from a third party—such as those featured on <a href="#" style="color:#0563c1; text-decoration:underline">Hugging Face’s open source embedding model leaderboard</a>—and can often be seamlessly integrated into your RAG framework via Hugging Face APIs. After vectorization, you can store your data in a vector database so it’s ready for efficient retrieval by the model.</p>
                        <p>Depending on the volume and complexity of your data, processing data and creating embeddings can be as computationally intensive as LLM inference. Intel® Xeon® processors can efficiently handle all your data ingesting, embedding, and vectoring on a CPU-based node without the need for any additional hardware.</p>
                        <p>Additionally, Intel® Xeon® processors can pair with quantized embedding models to optimize the vectorization process, improving encoding throughput by up to 4x compared to nonquantized models<sup>1</sup>.</p>


                        <h2 class="k_26px mt-5 pt-4"><strong>Optimize Query and Context Retrieval</strong></h2>
                        <p>When a user submits a query to a RAG-based model, a retriever mechanism searches your knowledge base for relevant external data to enrich the LLM’s final output. This process relies on vector search operations to find and rank the most-relevant information.</p>
                        <p>Vector search operations are highly optimized on Intel® Xeon® processors. <a href="#" style="color:#0563c1; text-decoration:underline">Intel® Advanced Vector Extensions 512 (Intel® AVX-512)</a>&nbsp;built into Intel® Xeon® processors enhance key operations in vector search and reduce the number of instructions, delivering significant improvements in throughput and performance.</p>
                        <p>You can also take advantage of Intel Lab’s <a href="#" style="color:#0563c1; text-decoration:underline">Scalable Vector Search (SVS)</a>&nbsp;solution to enhance vector database performance. SVS optimizes vector search capabilities on Intel® Xeon® CPUs to improve retrieval times and overall pipeline performance.</p>


                        <h2 class="k_26px mt-5 pt-3"><strong>Optimize LLM Response Generation</strong></h2>
                        <p>Once equipped with additional data from your vector store, the LLM can generate a contextually accurate response. This involves LLM inferencing, which is typically the most computationally demanding phase of the RAG pipeline.</p>
                        <p>Intel® Xeon® processors use <a href="#" style="color:#0563c1; text-decoration:underline">Intel® Advanced Matrix Extensions (Intel® AMX)</a>, a built-in AI accelerator, to enable more-efficient matrix operations and improved memory management, helping to maximize inference performance. For midsized and large LLMs, use Intel® Gaudi® AI accelerators to accelerate inference with purpose-built AI performance and efficiency.</p>
                        <p>Intel also offers several optimization libraries to help you maximize LLM inference on your hardware resources. Our <a href="#" style="color:#0563c1; text-decoration:underline">Intel® oneAPI libraries</a>&nbsp;provide low-level optimizations for popular AI frameworks like PyTorch and TensorFlow, enabling you to use familiar open source tools that are optimized on Intel® hardware. You can also add extensions such as the <a href="#" style="color:#0563c1; text-decoration:underline">Intel® Extension for PyTorch</a>&nbsp;to enable advanced quantized inference techniques to boost overall performance.</p>
                        <p>Once your application is in production, you may want to upgrade to the latest LLM to keep pace with end user demand. Because RAG does not involve fine-tuning and your knowledge base exists outside the model, RAG allows you to quickly replace your LLM with a new model to support faster inference.</p>
                        <h2 class="my-3 pt-5"><strong>Accelerate Your RAG Journey with Intel</strong></h2>
                        <p>RAG can help you deploy customized LLM applications quickly and cost-effectively without requiring fine-tuning. With the right building blocks, you can set up an optimized RAG pipeline in just a few steps.</p>
                        <p>As you pursue your AI initiative, be sure to take advantage of the Intel® AI portfolio to enhance each phase of your RAG pipeline. Our hardware and software solutions are built to accelerate your success.</p>


                        <div class="row py-5">
                            <div class="col-lg-4 col-md-4 col-sm-12">
                                <h4 class="fw-lighter k_plink1 k_bluehover1"><a href="#">Intel Tiber™ Developer Cloud</a></h4>
                                <p>Explore and get hands-on experience with key Intel® technologies for RAG.</p>
                                <p class="k_plink1 k_intext1blue1"><a href="#">View now</a></p>
                            </div>
                            <div class="col-lg-4 col-md-4 col-sm-12">
                                <h4 class="fw-lighter k_plink1 k_bluehover1"><a href="#">Building Blocks of RAG with Intel</a></h4>
                                <p>Learn more about Intel optimizations across the RAG pipeline.</p>
                                <p class="k_plink1 k_intext1blue1"><a href="#">Read now</a></p>
                            </div>
                            <div class="col-lg-4 col-md-4 col-sm-12">
                                <h4 class="fw-lighter k_plink1 k_bluehover1"><a href="#">Developer Tutorial: RAG on Intel® Gaudi® 2</a></h4>
                                <p>Get a step-by-step guide with code examples for deploying RAG applications on an Intel® Gaudi® 2 AI processor.</p>
                                <p class="k_plink1 k_intext1blue1"><a href="#">Read now</a></p>
                            </div>
                        </div>
                        
                    </div>
                </div>
            </div>
        </div>
    </section>
    <section>
        <div class="k_container11">
            <hr class="mt-5">
        </div>
    </section>
    <section class="k_space">
        <div class="k_container11">
            <div class="row ">
                <p>Product and Performance Information</p>
                <div class="k_plink1 k_intext1blue1"><sup>1</sup>“CPU Optimized Embeddings with Optimum Intel and fastRAG,” HuggingFace, March 15, 2024,<a href="#">https://huggingface.co/blog/intel-fast-embedding.</a></div>
            </div>
        </div>
    </section>
    <div id="footer"></div>
    <script>
        // navbar include  
        fetch('../y_index/y_navbar.html')
            .then(response => response.text())
            .then(data => {
                document.getElementById('navbar').innerHTML = data;
            });
        // footer include 
        fetch('../y_index/y_footer.html')
            .then(response => response.text())
            .then(data => {
                document.getElementById('footer').innerHTML = data;
            });
    </script>
    <script src="https://cdn.jsdelivr.net/npm/@splidejs/splide@4.1.4/dist/js/splide.min.js"></script>
    <script src="../js/jquery-3.7.1.js"></script>
    <!-- <script src="js/jquery-3.6.4.min.js"></script> -->
    <script src="../js/owl.carousel.min.js"></script>
    <script src="../js/rushita.js"></script>
    <script src="../js/monika.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/js/bootstrap.bundle.min.js"
        integrity="sha384-MrcW6ZMFYlzcLA8Nl+NtUVF0sA7MsXsP1UyJoMp4YLEuNSfAP+JcXn/tWtIaxVXM"
        crossorigin="anonymous"></script>



</body>

</html>