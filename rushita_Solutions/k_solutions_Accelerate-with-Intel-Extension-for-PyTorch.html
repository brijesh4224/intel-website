<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Accelerate with Intel® Extension for PyTorch</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/css/bootstrap.min.css" rel="stylesheet"
        integrity="sha384-EVSTQN3/azprG1Anm3QDgpJLIm9Nao0Yz1ztcQTwFspd3yD65VohhpuuCOmLASjC" crossorigin="anonymous">

    <link href=" https://fonts.cdnfonts.com/css/intel-clear " rel="stylesheet">
    <link rel="stylesheet" href="../css/owl.carousel.min.css">
    <link rel="stylesheet" href="../css/yatri.css">
    <link rel="stylesheet" href="../css/rushita.css">
    <link rel="stylesheet" href="../css/rushita_automotive.css">

    <link rel="stylesheet" href="../css/owl.theme.default.min.css">
    <style>
        * {
            font-family: 'Intel Clear', sans-serif;
        }

        .dk_new_options_table {
            border: none !important;
            justify-content: center !important;
            text-align: center !important;
        }

        .dk_new_options_table thead tr td {
            margin-left: 0;
            max-width: fit-content;
            white-space: normal;
            overflow: auto;
            background: #fff;
            padding: 1rem;
            text-align: left;
            border-left: .125rem solid #e2e2e2;
            font-weight: 700 !important;
        }

        .dk_new_options_table thead tr:nth-child(even) td {
            background-color: #f7f7f7;
        }

        .mv_intel_amx h2 {
            font-weight: 300;
        }

        .mv_intel_amx {
            font-size: 1.25rem;
        }

        .mv_intel_amx_bg_color {
            background-color: #0068B5;
        }

        .mv_intel_amx_content {
            padding-right: 200px;
        }

        .mv_intel_amx_item {
            align-content: center;
        }

        .mv_intel_amx {
            color: #fff;
            padding: 1rem;
        }

        .mv_intel_amx h3 {
            font-weight: 300;
        }

        .mv_intel_amx p {
            margin-top: 1.5rem;
            margin-bottom: 0rem;
        }

        .mv_intel_amx_image {
            padding: 1rem;
        }

        .mv_intel_amx_image img {
            width: 100%;
            /* height: 199px; */
        }

        .mv_intel_amx_heading_text {
            font-size: 1.2rem;
            line-height: 1.15;
        }

        .VK_sidebar_active_link {
            color: #262626;
            font-weight: 700;
        }

        .VK_sidebar_active_link:hover {
            color: #262626;
        }

        .VK_print_email_font span {
            font-size: 1.375rem;
        }

        .VK_side_bar_postion_stickey {
            position: sticky;
            top: 0px;
            z-index: 3;
            background-color: #fff;
        }

        .VK_side_heading {
            font-size: 26px;
            font-weight: 400;
        }

        .VK_sidebar_dropdown P {
            margin-left: .313rem !important;
            margin-bottom: .313rem !important;
        }

        .VK_sidebar_dropdown details {
            margin-left: .313rem !important;
            margin-bottom: .313rem !important;
        }
        
    </style>
</head>

<body>
    <div id="navbar"></div>

    <section class="m_ai_tdrop">
        <div>
            <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false">
                Developers
            </button>
            <ul class="dropdown-menu">
                <li><a class="dropdown-item m_dropActive"
                        href="../dhruvin_developer-tools/ds_Developer-Zone.html">Overview</a></li>
                <li><a class="dropdown-item m_dropActive"
                        href="../rushita_Solutions/k_solutions_in_Viewall_SDF_Industrial-Automation_Control-flow-Enforcement_Technology.html">Topics
                        & Technologies</a></li>
                <li><a class="dropdown-item m_dropActive"
                        href="../dhruvin_developer-tools/ds_Development-Tools.html">Tools</a></li>
                <li><a class="dropdown-item m_dropActive"
                        href="../rushita_Solutions/k_solutions_in_Viewall_SDF_Industrial-Automation_Hardware-Platforms.html">Hardware
                        Platforms</a></li>
                <li><a class="dropdown-item m_dropActive"
                        href="../dhruvin_developer-tools/ds_Resource-Documentation-Center.html">Resources &
                        Documentation</a></li>
                <li><a class="dropdown-item m_dropActive" href="../dhruvin_developer-tools/ds_Learn.html">Learn</a>
                </li>
                <li><a class="dropdown-item m_dropActive"
                        href="../dhruvin_developer-tools/ds_Communities-and-Events.html">Community & Events</a></li>
                <li><a class="dropdown-item m_dropActive"
                        href="../dhruvin_developer-tools/ds_Developer-Programs.html">Developer Programs</a></li>
                <li><a class="dropdown-item m_dropActive" href="../dhruvin_developer-tools/ds_Get-Help.html">Get
                        Help</a></li>
            </ul>
        </div>


        <div class="m_ai_shlash">/</div>
        <div>
            <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false">
                Tools
            </button>
            <ul class="dropdown-menu">
                <li><a class="dropdown-item m_dropActive" href="../Product/B20_developer_catelog.html">Software
                        Catalog</a></li>
                <li><a class="dropdown-item m_dropActive" href="../Product/B17_oneapi.html">oneAPI</a></li>
                <li><a class="dropdown-item m_dropActive"
                        href="../rushita_Solutions/k_solutions_Intel-Tiber™Edge-Platform.html">Intel® Tiber™ Edge
                        Platform</a></li>
                <li><a class="dropdown-item m_dropActive" href="../Product/B10_intel_Quarts.html">FPGA</a></li>
                <li><a class="dropdown-item m_dropActive" href="../VK_developers/VK_technology_sdk.html">Intel®
                        Active Management Technology SDK</a></li>
                <li><a class="dropdown-item m_dropActive" href="../VK_developers/VK_intel_adviser.html">Intel®
                        Advisor</a></li>
                <li><a class="dropdown-item m_dropActive" href="#">Intel® AI Reference Models</a></li>
                <li><a class="dropdown-item m_dropActive"
                        href="../rushita_Solutions/k_solutions_Intel-Collaboration-Suite-for-WebRTC.html">Intel®
                        Collaboration Suite for WebRTC SDK</a>
                </li>
                <li><a class="dropdown-item m_dropActive" href="../Product/B1_19_Intel_AICloud.html">Intel® Tiber™
                        AI Cloud</a></li>
                <li><a class="dropdown-item m_dropActive"
                        href="../rushita_Solutions/k_solutions_Intel-Distribution-for-Python.html">Intel® Distribution
                        for Python*</a></li>
                <li><a class="dropdown-item m_dropActive"
                        href="../rushita_Solutions/k_solutions_Intel-Distribution-for-GDB.html">Intel® Distribution for
                        GDB*</a></li>
                <li><a class="dropdown-item m_dropActive" href="#">Intel® DPC++ Compatibility Tool</a></li>
                <li><a class="dropdown-item m_dropActive" href="#">Intel® Dynamic Application Loader</a></li>
                <li><a class="dropdown-item m_dropActive"
                        href="../rushita_Solutions/k_solutions_Topics_viewall_Cloud-Insider-Program_AI-Frameworks-and-Tools.html">Frameworks</a>
                </li>
                <li><a class="dropdown-item m_dropActive" href="#">Intel® Embree</a></li>
                <li><a class="dropdown-item m_dropActive" href="../VK_developers/VK_ai_scikit.html">Intel® Extension
                        for Scikit-learn*</a></li>
                <li><a class="dropdown-item m_dropActive"
                        href="../rushita_Solutions/k_solutions_Intel-Fortran-Compiler.html">Intel® Fortran Compiler</a>
                </li>
                <li><a class="dropdown-item m_dropActive" href="#">Intel® Graphics Performance Analyzers</a></li>
                <li><a class="dropdown-item m_dropActive"
                        href="../rushita_Solutions/k_solutions_Intel-HE-Toolkit.html">Intel® Homomorphic Encryption
                        Toolkit</a></li>
                <li><a class="dropdown-item m_dropActive"
                        href="../rushita_Solutions/k_solutions_Intel-In-Band-Manageability.html">Intel® In-Band
                        Manageability</a></li>
                <li><a class="dropdown-item m_dropActive"
                        href="../rushita_Solutions/k_solutions_Intel-Cryptography-Primitives-Library.html">Intel®
                        Integrated Performance Primitives</a></li>
                <li><a class="dropdown-item m_dropActive" href="../rushita_Solutions/">Intel® Integrated Simulation
                        Infrastructure with
                        Modeling</a></li>
                <li><a class="dropdown-item m_dropActive"
                        href="../rushita_Solutions/k_solutions_Instruction-Set-Architecture(ISA)Extensions.html">Instruction
                        Set Architecture (ISA) Extensions</a>
                </li>
                <li><a class="dropdown-item m_dropActive"
                        href="../rushita_Solutions/k_solutions_Intel-Intelligent-Storage-Acceleration-Library.html">Intel®
                        Intelligent Storage Acceleration Library
                        (Intel® ISA-L)</a></li>
                <li><a class="dropdown-item m_dropActive" href="#">Modin*</a></li>
                <li><a class="dropdown-item m_dropActive" href="#">Intel® MPI Library</a></li>
                <li><a class="dropdown-item m_dropActive" href="#">Intel® Neural Compressor</a></li>
                <li><a class="dropdown-item m_dropActive" href="#">OpenCL™ Runtime</a></li>
                <li><a class="dropdown-item m_dropActive" href="#">Intel® Open Image Denoise</a></li>
                <li><a class="dropdown-item m_dropActive" href="#">Intel® Open Path Guiding Library (Intel® Open
                        PGL)</a></li>
                <li><a class="dropdown-item m_dropActive" href="#">Pin</a></li>
                <li><a class="dropdown-item m_dropActive" href="#">Intel® Platform Analysis Library</a></li>
                <li><a class="dropdown-item m_dropActive" href="#">Intel Tools for OpenCL™ Software</a></li>
                <li><a class="dropdown-item m_dropActive" href="#">Intel Tools for OpenCL™ Applications</a></li>
                <li><a class="dropdown-item m_dropActive" href="#">Intel® OpenSWR</a></li>
                <li><a class="dropdown-item m_dropActive" href="#">OpenVINO™ Toolkit</a></li>
                <li><a class="dropdown-item m_dropActive" href="#">Intel® Open Volume Kernel Library</a></li>
                <li><a class="dropdown-item m_dropActive" href="#">Intel® Optimization for XGBoost*</a></li>
                <li><a class="dropdown-item m_dropActive" href="#">Intel® OSPRay</a></li>
                <li><a class="dropdown-item m_dropActive" href="#">Intel® OSPRay for Hydra*</a></li>
                <li><a class="dropdown-item m_dropActive" href="#">Intel® OSPRay Studio</a></li>
                <li><a class="dropdown-item m_dropActive" href="#">PyTorch* Optimizations from Intel</a></li>
                <li><a class="dropdown-item m_dropActive" href="#">TensorFlow* Optimizations from Intel</a></li>
                <li><a class="dropdown-item m_dropActive" href="#">Intel® Rendering Toolkit</a></li>
                <li><a class="dropdown-item m_dropActive" href="#">Intel® Smart Edge Open</a></li>
                <li><a class="dropdown-item m_dropActive" href="#">Intel® Software Guard Extensions</a></li>
                <li><a class="dropdown-item m_dropActive" href="#">Intel® Secure Device Onboard</a></li>
                <li><a class="dropdown-item m_dropActive" href="#">Intel® Software Development Emulator</a></li>
                <li><a class="dropdown-item m_dropActive" href="#">Intel® Trust Domain Extensions (Intel® TDX)</a>
                </li>
                <li><a class="dropdown-item m_dropActive" href="#">Intel® Quantum SDK</a></li>
                <li><a class="dropdown-item m_dropActive" href="#">Intel® Query Processing Library (Intel® QPL)</a>
                </li>
                <li><a class="dropdown-item m_dropActive" href="#">Intel® Video Processing Library</a></li>
                <li><a class="dropdown-item m_dropActive" href="#">Intel® VTune™ Profiler</a></li>
                <li><a class="dropdown-item m_dropActive" href="#">Resellers</a></li>
            </ul>
        </div>
        <div class="m_ai_shlash">/</div>
        <div>
            <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false">
                oneAPI
            </button>
            <ul class="dropdown-menu">
                <li><a class="dropdown-item m_dropActive" href="#">Overview</a></li>
                <li><a class="dropdown-item m_dropActive" href="#">Data Parallel C++/SYCL*</a></li>
                <li><a class="dropdown-item m_dropActive" href="#">Toolkits</a></li>
                <li><a class="dropdown-item m_dropActive" href="#">Tech Articles & How-Tos</a></li>
                <li><a class="dropdown-item m_dropActive" href="#">Components</a></li>
                <li><a class="dropdown-item m_dropActive" href="#">Code Samples</a></li>
                <li><a class="dropdown-item m_dropActive" href="#">Training</a></li>
                <li><a class="dropdown-item m_dropActive" href="#">Documentation</a></li>
                <li><a class="dropdown-item m_dropActive" href="#">Support</a></li>
            </ul>
        </div>
        <div class="m_ai_shlash">/</div>
        <div>
            <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false">
                Tech Articles & How-Tos
            </button>
            <ul class="dropdown-menu">
                <li><a class="dropdown-item m_dropActive" href="#">Library</a></li>
                <li><a class="dropdown-item m_dropActive" href="#">News Updates</a></li>
                <li><a class="dropdown-item m_dropActive" href="#">Webinars</a></li>
            </ul>
        </div>
        <div class="m_ai_shlash">/</div>
        <div class="m_ai_httl">Accelerate AI Workloads on Amazon Web Services
        </div>
    </section>
    <section class="k_bgblue">
        <div class="k_container11">
            <div class="row text-white py-4">
                <h2 class="fw-light">Accelerating PyTorch* with Intel® Extension for PyTorch</h2>
                <p>An Open-Source Extension to Boost PyTorch Performance</p>
            </div>
        </div>
    </section>
    <section class="k_space k_spacenone">
        <div class="k_container11">
            <div class="row  g-3">
                <div class="float-end k_bignone" style="padding-left:70%;">
                    <div>
                        <a href="#" class="fs-4"><i class="fa-solid fa-print mx-3 k_icon0"></i></a>
                        <a href="#" class="fs-4"> <i class="fa-regular fa-envelope k_icon0"></i></a>
                    </div>
                </div>
                <div class="col-lg-3 col-md-4 col-sm-12">
                    <div class="VK_sticky_side_bar VK_side_bar_postion_stickey">
                        <div>
                            <div class="VK_sidebar_dropdown">
                                <ul class="k_listnone k_menu ">
                                    <li class="k_text px-2"><a href="#section1">A Peek at the Optimizations</a></li>
                                    <li class="k_text"><a href="#" class="k_submenu-toggle">Examples</a>
                                        <ul class="k_listnone k_submenu">
                                            <li class="k_text"><a href="#"></a>BF16 Training</a></li>
                                            <li class="k_text"><a href="#"></a>BF16 Inference</a></li>
                                            <li class="k_text"><a href="#"></a>INT8 Inference – Calibration</a></li>
                                            <li class="k_text"><a href="#"></a>INT8 Inference – Deployment</a></li>
                                        </ul>
                                    </li>
                                    <li class="k_text px-2"><a href="#section1">Performance</a></li>
                                    <li class="k_text px-2"><a href="#section1">Future Work</a></li>
                                  
                                    <li class="k_text"><a href="#" class="k_submenu-toggle">See Related Content</a>
                                        <ul class="k_listnone k_submenu">
                                            <li class="k_text"><a href="#"></a>Technical Articles</a></li>
                                            <li class="k_text"><a href="#"></a>On-Demand Webinars & Workshops</a></li>
                                        </ul>
                                    </li>
                                    <li class="k_text px-2"><a href="#section1">Get the Software</a></li>

                                </ul>
                            </div>
                        </div>
                    </div>
                    <div>
                        <p><b>Get the Latest on All Things CODE</b></p>
                        <p class="text-center k_btnget py-2 k_under k_btn100 my-2 kcokbtn1024" style="width: 80px;">
                            <a href="#" class="text-white">Sign Up</a>
                        </p>
                        <div class=" mt-4 ">
                            <div>
                                <p class="k_plink1"><b>Stefana Raileanu</b>, <i>AI software solutions engineer</i><br>
                                    <a href="#">LinkedIn</a></p>
                                    <p class="k_plink1"><b>Ramya Ravi</b>, <i>AI software marketing engineer</i><br>
                                        <a href="#">LinkedIn</a></p>
                                        <p>Intel Corporation</p>
                            </div>
                        </div>
                    </div>
                </div>
                <div class="col-lg-8 col-md-7 col-sm-12  k_16smpx k_width75">
                    <div class="float-end mx-3 k_smnone">
                        <div>
                            <a href="#" class="fs-4"><i class="fa-solid fa-print mx-3 k_icon0"></i></a>
                            <a href="#" class="fs-4"> <i class="fa-regular fa-envelope k_icon0"></i></a>
                        </div>
                    </div>
                    <div class=" k_smpaddnone k_smpaddnone1 pt-5">
                        <p>Intel engineers work with the PyTorch* open-source community to improve deep learning (DL) training and inference performance. Intel® Extension for PyTorch is an open-source extension that optimizes DL performance on Intel® processors. Many of the optimizations will eventually be included in future PyTorch mainline releases, but the extension allows PyTorch users to get up-to-date features and optimizations more quickly. In addition to CPUs, Intel Extension for PyTorch will also include support for Intel® GPUs in the near future.</p>
                        <p>Intel Extension for PyTorch optimizes both imperative mode and graph mode (<strong>Figure 1</strong>). The optimizations cover PyTorch operators, graph, and runtime. Optimized operators and kernels are registered through the PyTorch dispatching mechanism. During execution, Intel Extension for PyTorch overrides a subset of ATen operators with their optimized counterparts and offers an extra set of custom&nbsp;operators and optimizers for popular use-cases. In graph mode, additional graph optimization passes are applied to maximize the performance. Runtime optimizations are encapsulated in the runtime extension module, which provides a couple of PyTorch frontend APIs for users to get finer-grained control of the thread runtime.</p>
                        <div class="my-4">
                            <img src="../img/rushita_img/extention.png" alt="" width="100%">
                        </div>
                        <p class="utility-text-1" style="text-align:center"><strong>Figure 1. Intel® Extension for PyTorch*.</strong></p>
                        <h2 class="fw-light">A Peek at the Optimizations</h2>
                        <p>Memory layout is a fundamental optimization for vision-related operators. Using the right memory format for input tensors can significantly improve the performance of PyTorch models. “Channels last memory format” is generally beneficial for multiple hardware backends:</p>
                        <ul>
                            <li><a href="#" style="color:blue; text-decoration:underline">(Beta) Channels Last Memory Format in PyTorch</a></li>
                            <li><a href="#" style="color:blue; text-decoration:underline">Efficient PyTorch: Tensor Memory Format Matters</a></li>
                            <li><a href="#" style="color:blue; text-decoration:underline">Understanding Memory Formats</a></li>
                        </ul>
                        <p>This holds true for Intel processors. With Intel Extension for PyTorch, we recommend using the “channels last” memory format, i.e.:</p>
                        <p style="background-color: #E6E6E6;"><span class="code-simple"><em>model = model.to(memory_format=torch.channels_last)</em></span></p>
                        <p style="background-color: #E6E6E6;"><span class="code-simple"><em>input = input.to(memory_format=torch.channels_last)</em></span></p>
                        <p>The oneAPI Deep Neural Network Library (<a href="#">oneDNN</a>) introduces blocked memory layout for weights to achieve better vectorization and cache reuse. To avoid runtime conversion, we convert weights to predefined optimal block format prior to the execution of oneDNN operators. This technique is called weight prepacking, and it’s enabled for both inference and training when users call the ipex.optimize frontend API provided by the extension.</p>
                        <p>Intel Extension for PyTorch provides several customized operators to accelerate popular topologies, including fused interaction and merged embedding bag, which are used for recommendation models like DLRM, ROIAlign and FrozenBatchNorm for object detection workloads.</p>
                        <p>Optimizers play an important role in training performance, so we provide highly tuned fused and split optimizers in Intel Extension for PyTorch. We provide the fused kernels for Lamb, Adagrad, and SGD through the ipex.optimize frontend so users won’t need to change their model code. The kernels fuse the chain of memory-bound operators on model parameters and their gradients in the weight update step so that the data can reside in cache without being loaded from memory again. We are working to provide more fused optimizers in the upcoming extension releases.</p>
                        <p>BF16 mixed precision training offers a significant performance boost through accelerated computation, reduced memory bandwidth pressure, and reduced memory consumption. However, weight updates would become too small for accumulation in late stages of training. A common practice is to keep a master copy of weights in FP32, which doubles the memory requirement. The added memory usage burdens workloads that require many weights like recommendation models, so we apply a “split” optimization for BF16 training. We split FP32 parameters into top and bottom halves. The top half is the first 16 bits, which can be viewed exactly as a BF16 number. The bottom half is the last 16 bits, which are kept preserve accuracy. When performing forward and backward propagations, the top half benefits from native BF16 support on Intel CPUs. While performing parameter updates, we concatenate the top and bottom halves to recover the parameters back to FP32, thus avoiding accuracy loss.</p>
                        <p>BF16 mixed precision training offers a significant performance boost through accelerated computation, reduced memory bandwidth pressure, and reduced memory consumption. However, weight updates would become too small for accumulation in late stages of training. A common practice is to keep a master copy of weights in FP32, which doubles the memory requirement. The added memory usage burdens workloads that require many weights like recommendation models, so we apply a “split” optimization for BF16 training. We split FP32 parameters into top and bottom halves. The top half is the first 16 bits, which can be viewed exactly as a BF16 number. The bottom half is the last 16 bits, which are kept preserve accuracy. When performing forward and backward propagations, the top half benefits from native BF16 support on Intel CPUs. While performing parameter updates, we concatenate the top and bottom halves to recover the parameters back to FP32, thus avoiding accuracy loss.</p>
                        <p>BF16 mixed precision training offers a significant performance boost through accelerated computation, reduced memory bandwidth pressure, and reduced memory consumption. However, weight updates would become too small for accumulation in late stages of training. A common practice is to keep a master copy of weights in FP32, which doubles the memory requirement. The added memory usage burdens workloads that require many weights like recommendation models, so we apply a “split” optimization for BF16 training. We split FP32 parameters into top and bottom halves. The top half is the first 16 bits, which can be viewed exactly as a BF16 number. The bottom half is the last 16 bits, which are kept preserve accuracy. When performing forward and backward propagations, the top half benefits from native BF16 support on Intel CPUs. While performing parameter updates, we concatenate the top and bottom halves to recover the parameters back to FP32, thus avoiding accuracy loss.</p>
                        <p>BF16 mixed precision training offers a significant performance boost through accelerated computation, reduced memory bandwidth pressure, and reduced memory consumption. However, weight updates would become too small for accumulation in late stages of training. A common practice is to keep a master copy of weights in FP32, which doubles the memory requirement. The added memory usage burdens workloads that require many weights like recommendation models, so we apply a “split” optimization for BF16 training. We split FP32 parameters into top and bottom halves. The top half is the first 16 bits, which can be viewed exactly as a BF16 number. The bottom half is the last 16 bits, which are kept preserve accuracy. When performing forward and backward propagations, the top half benefits from native BF16 support on Intel CPUs. While performing parameter updates, we concatenate the top and bottom halves to recover the parameters back to FP32, thus avoiding accuracy loss.</p>
                        <p>BF16 mixed precision training offers a significant performance boost through accelerated computation, reduced memory bandwidth pressure, and reduced memory consumption. However, weight updates would become too small for accumulation in late stages of training. A common practice is to keep a master copy of weights in FP32, which doubles the memory requirement. The added memory usage burdens workloads that require many weights like recommendation models, so we apply a “split” optimization for BF16 training. We split FP32 parameters into top and bottom halves. The top half is the first 16 bits, which can be viewed exactly as a BF16 number. The bottom half is the last 16 bits, which are kept preserve accuracy. When performing forward and backward propagations, the top half benefits from native BF16 support on Intel CPUs. While performing parameter updates, we concatenate the top and bottom halves to recover the parameters back to FP32, thus avoiding accuracy loss.</p>
                        <p>BF16 mixed precision training offers a significant performance boost through accelerated computation, reduced memory bandwidth pressure, and reduced memory consumption. However, weight updates would become too small for accumulation in late stages of training. A common practice is to keep a master copy of weights in FP32, which doubles the memory requirement. The added memory usage burdens workloads that require many weights like recommendation models, so we apply a “split” optimization for BF16 training. We split FP32 parameters into top and bottom halves. The top half is the first 16 bits, which can be viewed exactly as a BF16 number. The bottom half is the last 16 bits, which are kept preserve accuracy. When performing forward and backward propagations, the top half benefits from native BF16 support on Intel CPUs. While performing parameter updates, we concatenate the top and bottom halves to recover the parameters back to FP32, thus avoiding accuracy loss.</p>
                        <p>Deep learning practitioners have demonstrated the effectiveness of lower numerical precision. Using 16-bit multipliers with 32-bit accumulators improves training and inference performance without compromising accuracy. Even using 8-bit multipliers with 32-bit accumulators is effective for some inference workloads. Lower precision improves performance in two ways: The additional multiply-accumulate throughput boosts compute-bound operations, and the smaller footprint boosts memory bandwidth-bound operations by reducing memory transactions in the memory hierarchy.</p>
                        <p>Intel introduced native BF16 support in 3rd Gen Intel® Xeon® Scalable processors with BF16→ FP32 fused multiply-add (FMA) and FP32→BF16 conversion Intel® Advanced Vector Extensions-512 (Intel® AVX-512) instructions that double the theoretical compute throughput over FP32 FMAs. BF16 will be further accelerated by the Intel® Advanced Matrix Extensions (Intel® AMX) instruction set in the next generation of Intel Xeon Scalable processors.</p>
                        <p>Quantization refers to information compression in deep networks by reducing the numerical precision&nbsp;of its weights and/or activations. By converting the parameter information from FP32 to INT8, the model gets smaller and leads to significant savings in memory and compute requirements. Intel introduced the AVX-512 VNNI instruction set extension in 2nd Gen Intel Xeon Scalable processors. It gives faster computation of INT8 data and results in higher throughput. PyTorch offers a few different approaches to quantize models. (See <a href="#">Practical Quantization in PyTorch</a>.)</p>
                        <p>Graph optimizations like operator fusion maximizes the performance of the underlying kernel implementations by optimizing the overall computation and memory bandwidth. Intel Extension for PyTorch applies operator fusion passes based on the TorchScript IR, powered by the fusion ability in oneDNN and the specialized fused kernels in the extension. The whole optimization is fully transparent to users. Constant-folding is a compile-time graph optimization that replaces operators that have constant inputs with precomputed constant nodes. Convolution+BatchNorm folding for inference gives nonnegligible performance benefits for many models. Users get this benefit from the ipex.optimize frontend API. It’s worth noting that we are working with the PyTorch community to get the fusion capability better composed with PyTorch NNC (Neural Network Compiler) to get the best of both.</p>

                        <h2 class="fw-light">Examples</h2>
                        <p>Intel Extension for PyTorch can be loaded as a module for Python programs or linked as a library for C++ programs. Users can get all benefits with minimal code changes. A few examples are included below, but more can be found in our tutorials.</p>
                        <h3>BF16 Training<a class="inpage-nav-anchor" id="inpage-nav-1-undefined"></a></h3>
                        <div class="p-3" style="border: 1px solid #CCCCCC;">
                            <div class="float-end p-3">
                                <i class="fa-solid fa-copy fs-4" style="color: #0068B5;"></i>
                            </div>
                            <pre > ... <br> import torch <br> ... <br> model = Model() <br>  model = model.to(memory_format=torch.channels_last) <br>criterion = ... <br>optimizer = ... <br>model.train() <br>#################### code changes ####################
                    <br>import intel_extension_for_pytorch as ipex <br>model, optimizer = ipex.optimize(model, optimizer=optimizer, dtype=torch.bfloat16)
                       <br> ###################################################### <br>... <br>with torch.cpu.amp.autocast(): <br># Setting memory_form <br>to torch.channels_last could improve performance <br># with 4D input data. <br> data = data.to(memory_format=torch. <br>channels_last) <br>optimizer.zero_grad() <br>output = model(data) <br>loss = ... <br>loss.backward() <br>...</pre>
                        </div>
                        <h3 class="my-3">BF16 Inference<a class="inpage-nav-anchor" id="inpage-nav-1-1"></a></h3>
                        <div class="p-3" style="border: 1px solid #CCCCCC;">
                            <div class="float-end p-3">
                                <i class="fa-solid fa-copy fs-4" style="color: #0068B5;"></i>
                            </div>
                            <p><div class="toolbar"><div class="toolbar-item"><i class="fa fa-docs"></i></div></div>... <br>
                                import torch <br>
                                ... <br>
                                model = Model() <br>
                                model = model.to(memory_format=torch.channels_last)<br>
                                model.eval()<br>
                                #################### code changes ####################<br>
                                import intel_extension_for_pytorch as ipex<br>
                                model = ipex.optimize(model, dtype=torch.bfloat16)<br>
                                ######################################################<br>
                                ...<br>
                                with torch.cpu.amp.autocast(),torch.no_grad():<br>
                                    # Setting memory_format to torch.channels_last could improve performance<br>
                                    # with 4D input data.<br>
                                    data = data.to(memory_format=torch.channels_last)<br>
                                    model = torch.jit.trace(model, data)<br>
                                    model = torch.jit.freeze(model)<br>
                                with torch.no_grad():<br>
                                    output = model(data)<br>
                                ...</pre>
                        </div>
                        <h3>INT8 Inference – Calibration<a class="inpage-nav-anchor" id="inpage-nav-1-2"></a></h3>
                        <div class="p-3" style="border: 1px solid #CCCCCC;">
                            <div class="float-end p-3">
                                <i class="fa-solid fa-copy fs-4" style="color: #0068B5;"></i>
                            </div>
                            <pre><div class="toolbar"><div class="toolbar-item"><i class="fa fa-docs"></i></div></div>import os <br>
                                import torch<br>
                                model = Model()<br>
                                model.eval()<br>
                                data = torch.rand(&lt;shape&gt;)<br>
                                # Applying torch.fx.experimental.optimization.fuse against model performs<br>
                                # conv-batchnorm folding for better performance.<br>
                                import torch.fx.experimental.optimization as optimization<br>
                                model = optimization.fuse(model, inplace=True)<br>
                                #################### code changes ####################<br>
                                import intel_extension_for_pytorch as ipex<br>
                                conf = ipex.quantization.QuantConf(qscheme=torch.per_tensor_affine)<br>
                                for d in calibration_data_loader():<br>
                                    # conf will be updated with observed statistics during calibrating with the dataset<br>
                                    with ipex.quantization.calibrate(conf):<br>
                                        model(d)<br>
                                conf.save('int8_conf.json', default_recipe=True)<br>
                                with torch.no_grad():<br>
                                    model = ipex.quantization.convert(model, conf, torch.rand(&lt;shape&gt;))<br>
                                ######################################################<br>
                                model.save('quantization_model.pt')</pre><br>
                        </div>
                        <h3>INT8 Inference – Deployment</h3>
                        <div class="p-3" style="border: 1px solid #CCCCCC;">
                            <div class="float-end p-3">
                                <i class="fa-solid fa-copy fs-4" style="color: #0068B5;"></i>
                            </div>
                            <pre><div class="toolbar"><div class="toolbar-item"><i class="fa fa-docs"></i></div></div>import torch <br>
                                #################### code changes ####################<br>
                                import intel_extension_for_pytorch as ipex<br>
                                ######################################################<br>
                                model = torch.jit.load('quantization_model.pt')<br>
                                model.eval()<br>
                                data = torch.rand(&lt;shape&gt;)<br>
                                with torch.no_grad():<br>
                                    model(data)</pre>
                        </div>        
                        <h2 class="fw-light">Performance</h2>
                        <p>The potential performance improvements using Intel Extension for PyTorch are shown in <strong>Figure 2</strong> and <strong>Figure 3</strong>. Benchmarking was done on 2.3 GHz Intel Xeon Platinum 8380 processors. (See the <a href="#">measurement details</a> for more information about the hardware and software configuration.) Offline refers to running single-instance inference with large batch using all cores of a socket (<strong>Figure 2</strong>). Realtime refers to running multi-instance, single batch inference with four cores per instance.</p>
                        <div class="my-4">
                            <img src="../img/rushita_img/LLM6.png" alt="" width="100%">
                        </div>
                        <p class="utility-text-1" style="text-align:center"><strong>Figure 2. Performance improvement for offline inference using Intel® Extension for PyTorch*.</strong></p>
                        <div class="my-4">
                            <img src="../img/rushita_img/LLM7.png" alt="" width="100%">
                        </div>
                        <p class="utility-text-1" style="text-align:center"><strong>Figure 3. Performance improvement for inference using Intel® Extension for PyTorch*.</strong></p>

                        <h2 class="fw-light">Future Work</h2>
                        <p class="k_plink1 k_intext1blue1">The intention of Intel Extension for PyTorch is to quickly bring PyTorch users additional performance on Intel processors. We will upstream most of the optimizations to the mainline PyTorch while continuously experimenting with new features and optimizations for the latest Intel hardware. We encourage users to try the open-source project and provide feedback in the GitHub <a href="#">repository</a>.</p>
                        <hr>
                        <h2 class="fw-light">See Related Content</h2>
                        <h3>Technical Articles<a class="inpage-nav-anchor" id="inpage-nav-4-undefined"></a></h3>
                        <ul>
                            <li class="k_plink1 k_intext1blue1"><a href="#">Deep Learning Model Optimizations Made Easy (or at Least Easier)&nbsp;</a></li>
                            <li class="k_plink1 k_intext1blue1"><a href="#">PyTorch Inference Acceleration with Intel® Neural Compressor&nbsp;</a></li>
                            <li class="k_plink1 k_intext1blue1"><a href="#">Increase PyTorch Inference Throughput by 4x</a></li>
                        </ul>
                        <h3>On-Demand Webinars &amp; Workshops<a class="inpage-nav-anchor" id="inpage-nav-4-1"></a></h3>
                        <ul>
                            <li class="k_plink1 k_intext1blue1"><a href="#">Accelerate AI Workloads with Intel® Optimization for PyTorch</a></li>
                            <li class="k_plink1 k_intext1blue1"><a href="#">Optimize Deep-Learning Workloads Using PyTorch Optimized by Intel</a></li>
                        </ul>

                        <h2 class="fw-light">Get the Software</h2>
                        <p class="k_plink1 k_intext1blue1"><strong><a href="#">Intel® AI Analytics Toolkit</a></strong><br>
                            Accelerate end-to-end machine learning and data science pipelines with optimized deep learning frameworks and high-performing Python* libraries.<br>
                            <br>
                            <strong><a href="#">Get It Now</a></strong><br>
                            <a href="#">See All Tools</a></p>

                        
                    </div>
                </div>
            </div>
    </section>
    <section class="my-4" style="font-size: 13px;">
        <div class="k_container11">
            <div class="row">
                <div>
                    <hr class="mb-5">
                    <p>Product and Performance Information </p>
                    <div class="k_plink1 k_intext1blue1"><sup>1</sup>Refer to <a href="#">this link</a> for the latest version of Intel Extension for TensorFlow.</div>
                    <div class="k_plink1 k_intext1blue1"><sup>2</sup>Refer to <a href="#">this link</a> for the latest version of Intel® Extension for PyTorch*.</div>
                    <div class="k_plink1 k_intext1blue1"><sup>3</sup>Performance varies by use, configuration and other
                        factors. Learn more at &nbsp;<a href="#">www.Intel.com/PerformanceIndex.</a>.
                    </div>
                </div>
            </div>
        </div>
    </section>
    <div id="footer"></div>
    <script>
        // navbar include  
        fetch('../y_index/y_navbar.html')
            .then(response => response.text())
            .then(data => {
                document.getElementById('navbar').innerHTML = data;
            });
        // footer include 
        fetch('../y_index/y_footer.html')
            .then(response => response.text())
            .then(data => {
                document.getElementById('footer').innerHTML = data;
            });
    </script>
    <script src="https://cdn.jsdelivr.net/npm/@splidejs/splide@4.1.4/dist/js/splide.min.js"></script>
    <script src="../js/jquery-3.7.1.js"></script>

    <script src="../js/owl.carousel.min.js"></script>
    <script src="../js/rushita.js"></script>
</body>

</html>