<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Now Available: 12 New AI Reference Kits</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/css/bootstrap.min.css" rel="stylesheet"
        integrity="sha384-EVSTQN3/azprG1Anm3QDgpJLIm9Nao0Yz1ztcQTwFspd3yD65VohhpuuCOmLASjC" crossorigin="anonymous">

    <link href=" https://fonts.cdnfonts.com/css/intel-clear " rel="stylesheet">
    <link rel="stylesheet" href="../css/owl.carousel.min.css">
    <link rel="stylesheet" href="../css/yatri.css">
    <link rel="stylesheet" href="../css/rushita.css">
    <link rel="stylesheet" href="../css/rushita_automotive.css">

    <link rel="stylesheet" href="../css/owl.theme.default.min.css">
    <style>
        * {
            font-family: 'Intel Clear', sans-serif;
        }

        .dk_new_options_table {
            border: none !important;
            justify-content: center !important;
            text-align: center !important;
        }

        .dk_new_options_table thead tr td {
            margin-left: 0;
            max-width: fit-content;
            white-space: normal;
            overflow: auto;
            background: #fff;
            padding: 1rem;
            text-align: left;
            border-left: .125rem solid #e2e2e2;
            font-weight: 700 !important;
        }

        .dk_new_options_table thead tr:nth-child(even) td {
            background-color: #f7f7f7;
        }

        .mv_intel_amx h2 {
            font-weight: 300;
        }

        .mv_intel_amx {
            font-size: 1.25rem;
        }

        .mv_intel_amx_bg_color {
            background-color: #0068B5;
        }

        .mv_intel_amx_content {
            padding-right: 200px;
        }

        .mv_intel_amx_item {
            align-content: center;
        }

        .mv_intel_amx {
            color: #fff;
            padding: 1rem;
        }

        .mv_intel_amx h3 {
            font-weight: 300;
        }

        .mv_intel_amx p {
            margin-top: 1.5rem;
            margin-bottom: 0rem;
        }

        .mv_intel_amx_image {
            padding: 1rem;
        }

        .mv_intel_amx_image img {
            width: 100%;
            /* height: 199px; */
        }

        .mv_intel_amx_heading_text {
            font-size: 1.2rem;
            line-height: 1.15;
        }

        .VK_sidebar_active_link {
            color: #262626;
            font-weight: 700;
        }

        .VK_sidebar_active_link:hover {
            color: #262626;
        }

        .VK_print_email_font span {
            font-size: 1.375rem;
        }

        .VK_side_bar_postion_stickey {
            position: sticky;
            top: 0px;
            z-index: 3;
            background-color: #fff;
        }

        .VK_side_heading {
            font-size: 26px;
            font-weight: 400;
        }

        .VK_sidebar_dropdown P {
            margin-left: .313rem !important;
            margin-bottom: .313rem !important;
        }

        .VK_sidebar_dropdown details {
            margin-left: .313rem !important;
            margin-bottom: .313rem !important;
        }
    </style>
</head>

<body>
    <div id="navbar"></div>

    <section class="m_ai_tdrop">
        <div>
            <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false">
                Developers
            </button>
            <ul class="dropdown-menu">
                <li><a class="dropdown-item m_dropActive"
                        href="../dhruvin_developer-tools/ds_Developer-Zone.html">Overview</a></li>
                <li><a class="dropdown-item m_dropActive"
                        href="../rushita_Solutions/k_solutions_in_Viewall_SDF_Industrial-Automation_Control-flow-Enforcement_Technology.html">Topics
                        & Technologies</a></li>
                <li><a class="dropdown-item m_dropActive"
                        href="../dhruvin_developer-tools/ds_Development-Tools.html">Tools</a></li>
                <li><a class="dropdown-item m_dropActive"
                        href="../rushita_Solutions/k_solutions_in_Viewall_SDF_Industrial-Automation_Hardware-Platforms.html">Hardware
                        Platforms</a></li>
                <li><a class="dropdown-item m_dropActive"
                        href="../dhruvin_developer-tools/ds_Resource-Documentation-Center.html">Resources &
                        Documentation</a></li>
                <li><a class="dropdown-item m_dropActive" href="../dhruvin_developer-tools/ds_Learn.html">Learn</a>
                </li>
                <li><a class="dropdown-item m_dropActive"
                        href="../dhruvin_developer-tools/ds_Communities-and-Events.html">Community & Events</a></li>
                <li><a class="dropdown-item m_dropActive"
                        href="../dhruvin_developer-tools/ds_Developer-Programs.html">Developer Programs</a></li>
                <li><a class="dropdown-item m_dropActive" href="../dhruvin_developer-tools/ds_Get-Help.html">Get
                        Help</a></li>
            </ul>
        </div>


        <div class="m_ai_shlash">/</div>
        <div>
            <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false">
                Tools
            </button>
            <ul class="dropdown-menu">
                <li><a class="dropdown-item m_dropActive" href="../Product/B20_developer_catelog.html">Software
                        Catalog</a></li>
                <li><a class="dropdown-item m_dropActive" href="../Product/B17_oneapi.html">oneAPI</a></li>
                <li><a class="dropdown-item m_dropActive"
                        href="../rushita_Solutions/k_solutions_Intel-Tiber™Edge-Platform.html">Intel® Tiber™ Edge
                        Platform</a></li>
                <li><a class="dropdown-item m_dropActive" href="../Product/B10_intel_Quarts.html">FPGA</a></li>
                <li><a class="dropdown-item m_dropActive" href="../VK_developers/VK_technology_sdk.html">Intel®
                        Active Management Technology SDK</a></li>
                <li><a class="dropdown-item m_dropActive" href="../VK_developers/VK_intel_adviser.html">Intel®
                        Advisor</a></li>
                <li><a class="dropdown-item m_dropActive" href="#">Intel® AI Reference Models</a></li>
                <li><a class="dropdown-item m_dropActive"
                        href="../rushita_Solutions/k_solutions_Intel-Collaboration-Suite-for-WebRTC.html">Intel®
                        Collaboration Suite for WebRTC SDK</a>
                </li>
                <li><a class="dropdown-item m_dropActive" href="../Product/B1_19_Intel_AICloud.html">Intel® Tiber™
                        AI Cloud</a></li>
                <li><a class="dropdown-item m_dropActive"
                        href="../rushita_Solutions/k_solutions_Intel-Distribution-for-Python.html">Intel® Distribution
                        for Python*</a></li>
                <li><a class="dropdown-item m_dropActive"
                        href="../rushita_Solutions/k_solutions_Intel-Distribution-for-GDB.html">Intel® Distribution for
                        GDB*</a></li>
                <li><a class="dropdown-item m_dropActive" href="#">Intel® DPC++ Compatibility Tool</a></li>
                <li><a class="dropdown-item m_dropActive" href="#">Intel® Dynamic Application Loader</a></li>
                <li><a class="dropdown-item m_dropActive"
                        href="../rushita_Solutions/k_solutions_Topics_viewall_Cloud-Insider-Program_AI-Frameworks-and-Tools.html">Frameworks</a>
                </li>
                <li><a class="dropdown-item m_dropActive" href="#">Intel® Embree</a></li>
                <li><a class="dropdown-item m_dropActive" href="../VK_developers/VK_ai_scikit.html">Intel® Extension
                        for Scikit-learn*</a></li>
                <li><a class="dropdown-item m_dropActive"
                        href="../rushita_Solutions/k_solutions_Intel-Fortran-Compiler.html">Intel® Fortran Compiler</a>
                </li>
                <li><a class="dropdown-item m_dropActive" href="#">Intel® Graphics Performance Analyzers</a></li>
                <li><a class="dropdown-item m_dropActive"
                        href="../rushita_Solutions/k_solutions_Intel-HE-Toolkit.html">Intel® Homomorphic Encryption
                        Toolkit</a></li>
                <li><a class="dropdown-item m_dropActive"
                        href="../rushita_Solutions/k_solutions_Intel-In-Band-Manageability.html">Intel® In-Band
                        Manageability</a></li>
                <li><a class="dropdown-item m_dropActive"
                        href="../rushita_Solutions/k_solutions_Intel-Cryptography-Primitives-Library.html">Intel®
                        Integrated Performance Primitives</a></li>
                <li><a class="dropdown-item m_dropActive" href="../rushita_Solutions/">Intel® Integrated Simulation
                        Infrastructure with
                        Modeling</a></li>
                <li><a class="dropdown-item m_dropActive"
                        href="../rushita_Solutions/k_solutions_Instruction-Set-Architecture(ISA)Extensions.html">Instruction
                        Set Architecture (ISA) Extensions</a>
                </li>
                <li><a class="dropdown-item m_dropActive"
                        href="../rushita_Solutions/k_solutions_Intel-Intelligent-Storage-Acceleration-Library.html">Intel®
                        Intelligent Storage Acceleration Library
                        (Intel® ISA-L)</a></li>
                <li><a class="dropdown-item m_dropActive" href="#">Modin*</a></li>
                <li><a class="dropdown-item m_dropActive" href="#">Intel® MPI Library</a></li>
                <li><a class="dropdown-item m_dropActive" href="#">Intel® Neural Compressor</a></li>
                <li><a class="dropdown-item m_dropActive" href="#">OpenCL™ Runtime</a></li>
                <li><a class="dropdown-item m_dropActive" href="#">Intel® Open Image Denoise</a></li>
                <li><a class="dropdown-item m_dropActive" href="#">Intel® Open Path Guiding Library (Intel® Open
                        PGL)</a></li>
                <li><a class="dropdown-item m_dropActive" href="#">Pin</a></li>
                <li><a class="dropdown-item m_dropActive" href="#">Intel® Platform Analysis Library</a></li>
                <li><a class="dropdown-item m_dropActive" href="#">Intel Tools for OpenCL™ Software</a></li>
                <li><a class="dropdown-item m_dropActive" href="#">Intel Tools for OpenCL™ Applications</a></li>
                <li><a class="dropdown-item m_dropActive" href="#">Intel® OpenSWR</a></li>
                <li><a class="dropdown-item m_dropActive" href="#">OpenVINO™ Toolkit</a></li>
                <li><a class="dropdown-item m_dropActive" href="#">Intel® Open Volume Kernel Library</a></li>
                <li><a class="dropdown-item m_dropActive" href="#">Intel® Optimization for XGBoost*</a></li>
                <li><a class="dropdown-item m_dropActive" href="#">Intel® OSPRay</a></li>
                <li><a class="dropdown-item m_dropActive" href="#">Intel® OSPRay for Hydra*</a></li>
                <li><a class="dropdown-item m_dropActive" href="#">Intel® OSPRay Studio</a></li>
                <li><a class="dropdown-item m_dropActive" href="#">PyTorch* Optimizations from Intel</a></li>
                <li><a class="dropdown-item m_dropActive" href="#">TensorFlow* Optimizations from Intel</a></li>
                <li><a class="dropdown-item m_dropActive" href="#">Intel® Rendering Toolkit</a></li>
                <li><a class="dropdown-item m_dropActive" href="#">Intel® Smart Edge Open</a></li>
                <li><a class="dropdown-item m_dropActive" href="#">Intel® Software Guard Extensions</a></li>
                <li><a class="dropdown-item m_dropActive" href="#">Intel® Secure Device Onboard</a></li>
                <li><a class="dropdown-item m_dropActive" href="#">Intel® Software Development Emulator</a></li>
                <li><a class="dropdown-item m_dropActive" href="#">Intel® Trust Domain Extensions (Intel® TDX)</a>
                </li>
                <li><a class="dropdown-item m_dropActive" href="#">Intel® Quantum SDK</a></li>
                <li><a class="dropdown-item m_dropActive" href="#">Intel® Query Processing Library (Intel® QPL)</a>
                </li>
                <li><a class="dropdown-item m_dropActive" href="#">Intel® Video Processing Library</a></li>
                <li><a class="dropdown-item m_dropActive" href="#">Intel® VTune™ Profiler</a></li>
                <li><a class="dropdown-item m_dropActive" href="#">Resellers</a></li>
            </ul>
        </div>
        <div class="m_ai_shlash">/</div>
        <div>
            <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false">
                oneAPI
            </button>
            <ul class="dropdown-menu">
                <li><a class="dropdown-item m_dropActive" href="#">Overview</a></li>
                <li><a class="dropdown-item m_dropActive" href="#">Data Parallel C++/SYCL*</a></li>
                <li><a class="dropdown-item m_dropActive" href="#">Toolkits</a></li>
                <li><a class="dropdown-item m_dropActive" href="#">Tech Articles & How-Tos</a></li>
                <li><a class="dropdown-item m_dropActive" href="#">Components</a></li>
                <li><a class="dropdown-item m_dropActive" href="#">Code Samples</a></li>
                <li><a class="dropdown-item m_dropActive" href="#">Training</a></li>
                <li><a class="dropdown-item m_dropActive" href="#">Documentation</a></li>
                <li><a class="dropdown-item m_dropActive" href="#">Support</a></li>
            </ul>
        </div>
        <div class="m_ai_shlash">/</div>
        <div>
            <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false">
                Tech Articles & How-Tos
            </button>
            <ul class="dropdown-menu">
                <li><a class="dropdown-item m_dropActive" href="#">Library</a></li>
                <li><a class="dropdown-item m_dropActive" href="#">News Updates</a></li>
                <li><a class="dropdown-item m_dropActive" href="#">Webinars</a></li>
            </ul>
        </div>
        <div class="m_ai_shlash">/</div>
        <div class="m_ai_httl">Top Five Tips for LLM Fine-Tuning and Inference
        </div>
    </section>
    <section class="k_bgblue">
        <div class="k_container11">
            <div class="row text-white py-4">
                <h2 class="fw-light">Top 5 Tips and Tricks for LLM Fine-Tuning and Inference</h2>
            </div>
        </div>
    </section>
    <section class="k_space k_spacenone">
        <div class="k_container11">
            <div class="row  g-3">
                <div class="float-end k_bignone" style="padding-left:70%;">
                    <div>
                        <a href="#" class="fs-4"><i class="fa-solid fa-print mx-3 k_icon0"></i></a>
                        <a href="#" class="fs-4"> <i class="fa-regular fa-envelope k_icon0"></i></a>
                    </div>
                </div>
                <div class="col-lg-3 col-md-4 col-sm-12">
                    <div class="VK_sticky_side_bar VK_side_bar_postion_stickey">
                        <div>
                            <div class="VK_sidebar_dropdown">
                                <ul class="k_listnone k_menu ">
                                    <li class="k_text px-2"><a href="#section1">Overview</a></li>
                                    <li class="k_text"><a href="#" class="k_submenu-toggle">Frameworks</a>
                                        <ul class="k_listnone k_submenu">
                                            <li class="k_text"><a href="#"></a>Expert Tips</a>

                                            </li>
                                        </ul>
                                    </li>
                                    <li class="k_text"><a href="#" class="k_submenu-toggle">Preprocess Your Data
                                            Thoroughly</a>
                                        <ul class="k_listnone k_submenu">
                                            <li class="k_text"><a href="#"></a>Expert Tips</a>

                                            </li>
                                        </ul>
                                    </li>
                                    <li class="k_text"><a href="#" class="k_submenu-toggle">Fine-Tune Hyperparameters
                                            Systematically</a>
                                        <ul class="k_listnone k_submenu">
                                            <li class="k_text"><a href="#"></a>Expert Tips</a>

                                            </li>
                                        </ul>
                                    </li>
                                    <li class="k_text"><a href="#" class="k_submenu-toggle">Use Advanced Techniques</a>
                                        <ul class="k_listnone k_submenu">
                                            <li class="k_text"><a href="#"></a>Expert Tips</a>

                                            </li>
                                        </ul>
                                    </li>
                                    <li class="k_text"><a href="#" class="k_submenu-toggle">Optimize for Inference
                                            Speed</a>
                                        <ul class="k_listnone k_submenu">
                                            <li class="k_text"><a href="#"></a>Expert Tips</a>

                                            </li>
                                        </ul>
                                    </li>
                                    <li class="k_text"><a href="#" class="k_submenu-toggle">Scale Deployment with Robust
                                            Infrastructure</a>
                                        <ul class="k_listnone k_submenu">
                                            <li class="k_text"><a href="#"></a>Expert Tips</a>

                                            </li>
                                        </ul>
                                    </li>
                                    <li class="k_text"><a href="#" class="k_submenu-toggle">Resource Library</a>
                                        <ul class="k_listnone k_submenu">
                                            <li class="k_text"><a href="#"></a>Get Started</a></li>
                                            <li class="k_text"><a href="#"></a>Additional Resources</a></li>
                                        </ul>
                                    </li>

                                </ul>
                            </div>
                        </div>
                    </div>
                    <div>
                        <p><b>Stay in the Know on All Things CODE</b></p>
                        <p class="text-center k_btnget py-2 k_under k_btn100 my-2 kcokbtn1024" style="width: 80px;">
                            <a href="#" class="text-white">Sign Up</a>
                        </p>
                        <div class=" mt-4 ">
                          
                            <div>
                                <p class="k_plink1 k_bluehover1">Eduardo Alvarez, senior AI solutions engineer, Intel |
                                    <a href="#">LinkedIn*<br>
                                    </a>Ramya Ravi, AI software marketing engineer, Intel |&nbsp;<a
                                        href="#">LinkedIn</a></p>
                            </div>
                        </div>
                    </div>
                </div>
                <div class="col-lg-8 col-md-7 col-sm-12  k_16smpx k_width75">
                    <div class="float-end mx-3 k_smnone">
                        <div>
                            <a href="#" class="fs-4"><i class="fa-solid fa-print mx-3 k_icon0"></i></a>
                            <a href="#" class="fs-4"> <i class="fa-regular fa-envelope k_icon0"></i></a>
                        </div>
                    </div>
                    <div class=" k_smpaddnone k_smpaddnone1 pt-5">
                        <p><strong>Enhance Your AI Skills</strong></p>
                        <p class="k_plink1 k_intext1blue1">Supercharge your generative AI (GenAI) solutions with LLM
                            fine-tuning and inference. At the end of this article, make sure to review our <a
                                href="#">resource collection</a>.</p>
                        <h2 class="fw-light mt-5 ">Overview</h2>
                        <p>LLMs are at the core of GenAI, enabling us to build powerful, innovative applications.
                            However, like any advanced technology, unlocking their full potential comes with its own set
                            of challenges. Fine-tuning and deploying these models for inference can be complex. This
                            article gives you five tips to guide you through these hurdles.</p>

                        <h2 class="fw-light mt-5 ">Preprocess Your Data Thoroughly</h2>
                        <p>​Effective data preprocessing is foundational for model performance. Ensuring your dataset is
                            clean and well-labeled can significantly enhance training outcomes. Challenges include noisy
                            data, imbalanced classes, proper task-specific formatting, and nonstandard datatypes.</p>
                        <h3>Expert Tips<a class="inpage-nav-anchor" id="inpage-nav-1-undefined"></a></h3>
                        <ul>
                            <li>Determine if you will be performing instruction, chat, or open-ended text generation
                                training and fine-tuning, as this will dictate the columns and format of your dataset.
                            </li>
                            <li>Augment your data by generating synthetic data from a significantly larger LLM. For
                                example, use a 70B parameter model to generate data for fine-tuning a smaller 1B
                                parameter model.</li>
                            <li>You've heard it before: garbage in/garbage out. This is still true for language models,
                                and it can have significant impacts on the hallucination and quality of your models. Try
                                manually evaluating 10% of your data at random.</li>
                        </ul>
                        <h2 class="fw-light mt-5 ">Fine-Tune Hyperparameters Systematically</h2>
                        <p>Hyperparameter tuning is crucial for achieving optimal performance. This involves selecting
                            the right learning rate, batch size, and number of epochs, which can be daunting due to the
                            vast search space. Automating this with LLMs is challenging and typically requires access to
                            two or more accelerators to optimize.</p>
                        <h3>Expert Tips<a class="inpage-nav-anchor" id="inpage-nav-2-undefined"></a></h3>
                        <ul>
                            <li>Use grid search or random search methods to explore the hyperparameter space.</li>
                            <li class="k_plink1 k_intext1blue1">For unique LLM tasks, build a custom benchmark based on
                                your dataset by synthetically or manually creating a smaller subset of data.
                                Alternatively, use standard benchmarks from language modeling harnesses like the <a
                                    href="#">EleutherAI Language Model Evaluation Harness</a>.</li>
                            <li>Monitor training metrics closely to avoid overfitting or underfitting. Check for
                                situations where your training loss continues to decrease while validation loss
                                increases—this is a clear sign of overfitting.</li>
                        </ul>

                        <h2 class="fw-light mt-5 ">Use Advanced Techniques</h2>
                        <p>Advanced techniques like mixed precision, distributed training, and parameter-efficient
                            fine-tuning (PEFT) can significantly reduce training time and memory. These techniques are
                            well accepted and implemented by production and research teams developing GenAI
                            applications.</p>
                        <h3>Expert Tips<a class="inpage-nav-anchor" id="inpage-nav-3-undefined"></a></h3>
                        <ul>
                            <li>Regularly validate your model's performance to maintain accuracy across mixed and
                                non-mixed&nbsp;precision model training runs.</li>
                            <li class="k_plink1 k_intext1blue1">Use libraries that support mixed precision natively to
                                simplify implementation. Most notably, <a href="#">PyTorch*</a> supports automatic mixed
                                precision with very few changes to the training code.</li>
                            <li>Model sharding is more advanced and resource-efficient than traditional data parallel
                                distributed methods. It splits up the model and the data across multiple processors.
                                Popular software options include PyTorch Fully Sharded Data Parallel (FSDP) and
                                Microsoft DeepSpeed* ZeRO.</li>
                            <li>Using PEFT techniques like low-rank adaptations (LoRA) allows you to create
                                "mini-models" or adapters for various domains and tasks. LoRA also reduces the total
                                trainable parameters, thereby decreasing the memory and compute complexity of the
                                fine-tuning process. Deploying these adapters efficiently lets you support many use
                                cases without needing multiple large model files.</li>
                        </ul>
                        <h2 class="fw-light mt-5 ">Optimize for Inference Speed</h2>
                        <p>Deploying LLMs efficiently requires minimizing inference latency, which is challenging due to
                            their large size and complexity. This aspect of AI most closely impacts user experience and
                            system latency.</p>
                        <h3>Expert Tips<a class="inpage-nav-anchor" id="inpage-nav-4-undefined"></a></h3>
                        <ul>
                            <li>Use model compression techniques like low-bit quantization to compress models to 16-bit
                                and 8-bit representations.</li>
                            <li>Regularly validate your model's performance to ensure accuracy is maintained as you test
                                lower precisions quantization recipes.</li>
                            <li>Use pruning techniques to eliminate redundant weights, reducing the computational load.
                            </li>
                            <li>Consider model distillation to create a smaller, faster model that approximates the
                                original.</li>
                        </ul>

                        <h2 class="fw-light mt-5 ">Scale Deployment with Robust Infrastructure</h2>
                        <p>Deploying LLMs at scale presents challenges such as load balancing, fault tolerance, and
                            maintaining low latency. Effective infrastructure setup is key.</p>
                        <h3>Expert Tips<a class="inpage-nav-anchor" id="inpage-nav-5-undefined"></a></h3>
                        <ul>
                            <li>Use Docker* software to create consistent deployments of LLM inference environments.
                                This makes it easier to manage dependencies and configurations across different
                                deployment stages.</li>
                            <li>Use container management technologies like Kubernetes* or AI and machine learning tools
                                like Ray to orchestrate multiple instances of models deployed across a data center
                                cluster.</li>
                            <li>Employ autoscaling to handle varying loads and maintain performance under peak traffic
                                when language models receive abnormally high or low request volumes. This can help save
                                costs and ensure the deployment correctly supports the application's business
                                requirements.</li>
                        </ul>
                        <p>Fine-tuning and deploying LLMs might seem like a daunting task, but with the right
                            strategies, you can overcome any challenge that comes your way. The tips and tricks outlined
                            above can go a long way to overcoming common pitfalls.&nbsp;</p>

                        <h2 class="fw-light mt-5 ">Resource Library</h2>
                        <p>In this section, we provide expertly authored and curated content on LLM fine-tuning and
                            inference for aspiring and current AI developers. We cover techniques and tools like LoRA
                            fine-tuning of Llama* 7B, distributed training, Hugging Face* for the Optimum for Intel
                            Gaudi library, and more.</p>
                        <p><strong>What you'll learn:</strong></p>
                        <ul>
                            <li>Implement LoRA PEFT of state-of-the-art models.</li>
                            <li>Identify opportunities to use Hugging Face tools to train and perform inference with
                                LLMs.</li>
                            <li>Recognize opportunities to use distributed training techniques like PyTorch FSDP to
                                accelerate model training.</li>
                            <li>Configure an Intel® Gaudi® processor node on the Intel® Tiber™ Developer Cloud.</li>
                        </ul>
                        <h3>Get Started<a class="inpage-nav-anchor" id="inpage-nav-6-undefined"></a></h3>
                        <p class="k_plink1 k_intext1blue1"><strong>Step 1:</strong> <a href="#">Watch the video on using
                                Hugging Face on Intel Tiber Developer Cloud</a>.</p>


                        <p><strong>Step 2:</strong> Read up on the technology and run the code.</p>
                        <p class="k_plink1 k_intext1blue1">Senior AI solutions engineer Eduardo Alvarez shares this <a
                                href="#">comprehensive guide to efficiently fine-tune Llama 2 using LoRA on Intel Gaudi
                                2 processors</a>. The guide covers how to set up the development environment and details
                            steps to fine-tune and perform inference—all aimed at reducing costs and speeding up the
                            optimization of LLMs. Then go to the Intel Tiber Developer Cloud, where you'll be able to
                            use the <a href="#">free Jupyter* servers for Intel Gaudi 2 processors</a> to fine-tune your
                            LLM.</p>
                        <p class="k_plink1 k_intext1blue1"><strong class="k_plink1 k_intext1blue1">Step 3:</strong> <a
                                href="#">Learn about advanced fine-tuning techniques with PyTorch FSDP on Intel Gaudi
                                accelerators</a>.
                        <p class="k_plink1 k_intext1blue1">Senior AI solutions engineer Eduardo Alvarez shares this <a href="#">comprehensive guide to
                                efficiently fine-tune Llama 2 using LoRA on Intel Gaudi 2 processors</a>. The guide
                            covers how to set up the development environment and details steps to fine-tune and perform
                            inference—all aimed at reducing costs and speeding up the optimization of LLMs. Then go to
                            the Intel Tiber Developer Cloud, where you'll be able to use the <a href="#">free Jupyter*
                                servers for Intel Gaudi 2 processors</a> to fine-tune your LLM.
                        <p class="k_plink1 k_intext1blue1">Senior AI solutions engineer Eduardo Alvarez shares this <a href="#">comprehensive guide to
                                efficiently fine-tune Llama 2 using LoRA on Intel Gaudi 2 processors</a>. The guide
                            covers how to set up the development environment and details steps to fine-tune and perform
                            inference—all aimed at reducing costs and speeding up the optimization of LLMs. Then go to
                            the Intel Tiber Developer Cloud, where you'll be able to use the <a href="#">free Jupyter*
                                servers for Intel Gaudi 2 processors</a> to fine-tune your LLM.</p>
                        </p>
                        </p>
                        <p>Discover how to implement scalable model development using FSDP training with PyTorch on
                            Intel Gaudi accelerators. This course emphasizes Intel Gaudi software support and
                            optimizations for major frameworks, which provides developers with robust tools to develop
                            GenAI models at scale.</p>
                        <div class="my-5">
                            <img src="../img/rushita_img/LLM1.jpg" alt="" width="100%">
                            <figcaption class="text-center">Figure 1. Standard data parellel training versus advanced
                                fine-tuning techniques using FSDP</figcaption>
                        </div>
                        <p class="k_plink1 k_intext1blue1"><strong>Step 4:</strong> <a href="#">Learn more about how
                                models perform on Intel Gaudi accelerators</a>.</p>
                        <p>Get detailed performance data for Intel Gaudi AI accelerators, including latency, throughput,
                            and time-to-train metrics for top models like Llama, Mistral AI*, and Falcon. This data is
                            an essential resource for planning resource allocation and deployment, helping developers
                            optimize their AI workloads.</p>
                        <h3>Additional Resources<a class="inpage-nav-anchor" id="inpage-nav-6-1"></a></h3>
                        <p class="k_plink1 k_intext1blue1"><a href="#">Intel Gaudi Al Accelerators</a><br>
                            <a href="#">PyTorch Optimizations from Intel</a><br>
                            <a href="#">Developer Resources from Intel and Hugging Face</a><br>
                            <a href="#">Sign Up for Intel Tiber Developer Cloud</a>
                        </p>
                    </div>
                </div>
            </div>
    </section>
    <section class="my-4" style="font-size: 13px;">
        <div class="k_container11">
            <div class="row">
                <div>
                    <hr class="mb-5">
                    <p>Product and Performance Information </p>
                    <div class="k_plink1 k_intext1blue1"><sup>1</sup>Performance varies by use, configuration and other
                        factors. Learn more at &nbsp;<a href="#">www.Intel.com/PerformanceIndex.</a>.
                    </div>
                </div>
            </div>
        </div>
    </section>
    <div id="footer"></div>
    <script>
        // navbar include  
        fetch('../y_index/y_navbar.html')
            .then(response => response.text())
            .then(data => {
                document.getElementById('navbar').innerHTML = data;
            });
        // footer include 
        fetch('../y_index/y_footer.html')
            .then(response => response.text())
            .then(data => {
                document.getElementById('footer').innerHTML = data;
            });
    </script>
    <script src="https://cdn.jsdelivr.net/npm/@splidejs/splide@4.1.4/dist/js/splide.min.js"></script>
    <script src="../js/jquery-3.7.1.js"></script>

    <script src="../js/owl.carousel.min.js"></script>
    <script src="../js/rushita.js"></script>
</body>

</html>