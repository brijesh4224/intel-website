<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Advanced Matrix Extensions business</title>

    <link rel="stylesheet" href="../css/mv_ai_accelerator_matrix_extensions_business.css">

    <!-- header footer -->
    <link rel="stylesheet" href='/css/yatri.css'>
    
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/css/bootstrap.min.css" rel="stylesheet"
        integrity="sha384-EVSTQN3/azprG1Anm3QDgpJLIm9Nao0Yz1ztcQTwFspd3yD65VohhpuuCOmLASjC" crossorigin="anonymous">
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/js/bootstrap.bundle.min.js"
        integrity="sha384-MrcW6ZMFYlzcLA8Nl+NtUVF0sA7MsXsP1UyJoMp4YLEuNSfAP+JcXn/tWtIaxVXM"
        crossorigin="anonymous"></script>
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.6.0/css/all.min.css" rel="stylesheet">
</head>
<body>

    <!-- header -->
    <div id="navbar"></div>

    <!-- Intel AMX ? -->
    <section>
        <div class="mv_intel_amx_bg_color">
            <div class="container">
                <div class="row mv_intel_amx_content">
                    <div class="col-md-8 col-sm-12 mv_intel_amx_item">
                        <div class="mv_intel_amx">
                            <h3>What Is Intel<sup>®</sup> Advanced Matrix Extensions (Intel<sup>®</sup> AMX)?</h3>
                            <p>Expand, simplify, and accelerate your AI capabilities to meet compute demands for deep learning workloads with this integrated accelerator on the latest generations of Intel<sup>®</sup> Xeon<sup>®</sup> Scalable processors.<sup>1</sup></p>
                        </div>
                    </div>
                    <div class="col-md-4 col-sm-6 col-8 mv_intel_amx_item">
                        <div class="mv_intel_amx_image">
                            <img src="/img/mv_image/adobestock-620190621.jpeg.rendition.intel.web.480.270.jpg" alt="">
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="container py-5">
        <div class="row">
            <div class="d-flex justify-content-end col-xl-10 py-3 d-md-none">
                <i class="fa-solid fa-print fs-4 px-2 " style="color:#0068B5"></i>
                <i class="fa-regular fa-envelope fs-4 px-2" style="color:#0068B5"></i>
            </div>
            <div class="col-lg-3 col-md-4">
                <!-- nav -->
                <div class="VK_client_app_navigation VK_ai_navigation">
                    <div class="justify-content-center align-items-center overflow-hidden flex-nowrap mb-4">
                        <ul class="VK_ai_nav_bar list-unstyled m-0">
                            <li>
                                <a href="#ds_HPC-port" class="text-dark text-decoration-none d-block">
                                    Intel<sup>®</sup> Xeon<sup>®</sup> Scalable Processors and Intel<sup>®</sup> AMX		
                                </a>
                            </li>
                            <li>
                                <a href="#ds_HPC-tools" class="text-dark text-decoration-none d-block VK_ai_nav_link">
                                    Benefits of Intel<sup>®</sup> AMX		
                                </a>
                            </li>
                            <li>
                                <a href="#ds_HPC-get" class="text-dark text-decoration-none d-block VK_ai_nav_link">
                                    Use Cases
                                </a>
                            </li>
                            <li>
                                <a href="#ds_HPC-reso" class="text-dark text-decoration-none d-block VK_ai_nav_link">
                                    Get Started
                                </a>
                            </li>
                        </ul>
                    </div>
                </div>
                <div class="bg-light p-3">
                    <h6 style="font-weight: 700; margin-bottom: 11px;">Key Takeaways</h6>
                    <ul class="ps-3 mb-0">    
                        <li>
                            <p class="mb-0">Intel<sup>®</sup> AMX is part of the Intel<sup>®</sup> AI Engines available on the latest generations of Intel<sup>®</sup> Xeon<sup>®</sup> Scalable processors.</p>
                        </li>
                        <li>
                            <p class="mb-0">Intel<sup>®</sup> AMX accelerates deep learning training and inferencing workloads and minimizes the need for additional specialized hardware.</p>
                        </li>
                        <li>
                            <p class="mb-0">Intel<sup>®</sup> developer tools and enablement resources help make it easier to take advantage of Intel<sup>®</sup> AMX.</p>
                        </li>
                    </ul>
                </div>
            </div>
            <div class="col-lg-9 col-md-8" >
                <div class="d-md-flex justify-content-end col-xl-10 py-3 d-none">  
                    <i class="fa-solid fa-print fs-4 px-2 " style="color:#0068B5"></i>
                    <i class="fa-regular fa-envelope fs-4 px-2" style="color:#0068B5"></i>
                </div>
                <div class="col-xl-10 fs-5">
                    <div style="padding-bottom: 16px;">
                        <p>Accelerate your demanding deep learning training and inferencing workloads by taking advantage of Intel<sup>®</sup> Advanced Matrix Extensions (Intel<sup>®</sup> AMX), an integrated accelerator on the latest generations of Intel<sup>®</sup> Xeon<sup>®</sup> Scalable processors, including the upcoming <a class="b_special_a2" href="">Intel<sup>®</sup> Xeon<sup>®</sup> 6 processor</a> with P-cores. Learn how to activate Intel<sup>®</sup> AMX on your CPU with our step-by-step instruction guides so you can start optimizing your AI pipeline, improving efficiency, maximizing value, and reducing TCO at your organization.</p>
                    </div>
                    <section id="ds_HPC-port">
                        <h3 style="font-weight: 350; margin: 2.5rem 0 11px;">Intel<sup>®</sup> Xeon<sup>®</sup> Scalable Processors and Intel<sup>®</sup> Advanced Matrix Extensions</h3>
                        <p>
                            Deep learning workloads, such as those that that rely on generative AI, large language models (LLMs), and computer vision, can be incredibly compute intensive, requiring high levels of performance and, often, additional specialized hardware to ensure successful AI deployment. The associated costs of these requirements can quickly escalate, and adding discrete hardware solutions can create unnecessary layers of complexity and compatibility issues.
                        </p>
                        <p>
                            To help make your deep learning workloads more efficient and cost-effective and easier to train and deploy, Intel<sup>®</sup> AMX on Intel<sup>®</sup> Xeon<sup>®</sup> Scalable processors delivers acceleration for inferencing and training while minimizing the need for specialized hardware.
                        </p>
                        <p>
                            Intel<sup>®</sup> AMX is one of two <a class="b_special_a1" href="">Intel<sup>®</sup> AI Engines</a> integrated into 4th Gen Intel Xeon, 5th Gen Intel Xeon, and Intel<sup>®</sup> Xeon<sup>®</sup> 6 processors with P-cores, that can help you make the most of your CPU to power AI training and inferencing workloads at scale for benefits including improved efficiency, reduced inferencing, training, and deployment costs, and lower total cost of ownership (TCO). As a built-in accelerator that resides on each CPU core and placed near system memory, Intel<sup>®</sup> AMX is often less complex to use than discrete accelerators, leading to faster time to value.
                        </p>
                        <p>
                            While there are many ways organizations can support advanced AI workloads, a foundation based on Intel<sup>®</sup> Xeon<sup>®</sup> Scalable processors with powerful, integrated AI accelerators can help you achieve your training and inferencing performance objectives while reducing system complexity and deployment and operational costs for greater business return.
                        </p>
                    </section>
                    <section>
                        <h3 style="font-weight: 350; margin: 2.5rem 0 11px;">How Intel<sup>®</sup> AMX Works</h3>
                        <p>
                            Intel<sup>®</sup> AMX is a dedicated hardware block found on the Intel<sup>®</sup> Xeon<sup>®</sup> Scalable processor core that helps optimize and accelerate deep learning training and inferencing workloads that rely on matrix math.
                        </p>
                        <p class="mb-0">
                            Intel<sup>®</sup> AMX enables AI workloads to run on the CPU instead of offloading them to a discrete accelerator, providing a significant performance boost.<sup>2</sup> Its architecture supports BF16 (training/inference) and int8 (inference) data types and includes two main components:
                        </p>
                        <ul class="mb-0">
                            <li><b>Tiles</b>: These consist of eight two-dimensional registers, each 1 kilobyte in size, that store large chunks of data.</li>
                            <li><b>Tile Matrix Multiplication (TMUL)</b>: TMUL is an accelerator engine attached to the tiles that performs matrix-multiply computations for AI.</li>
                        </ul>
                        <p class="mb-0">
                            Together, these components enable Intel<sup>®</sup> AMX to store more data in each core and compute larger matrices in a single operation. Additionally, Intel<sup>®</sup> AMX is architected to be fully extensible and scalable.
                        </p>
                        <p class="mb-0">&nbsp;</p>
                    </section>
                    <section id="ds_HPC-tools">
                        <h3 style="font-weight: 350; margin: 2.5rem 0 11px;">Benefits of Intel<sup>®</sup> AMX for Better Business Outcomes</h3>
                        <p>
                            Intel<sup>®</sup> AMX enables Intel<sup>®</sup> Xeon<sup>®</sup> Scalable processors to boost the performance of deep learning training and inferencing workloads by balancing inference, the most prominent use case for a CPU in AI applications, with more capabilities for training.
                        </p>
                        <p>
                            Many Intel customers are taking advantage of Intel<sup>®</sup> AMX to enable better outcomes for their organizations. Focusing on GenAI workloads, Intel<sup>®</sup> Xeon<sup>®</sup> 6 processors with P-cores can deliver 2x higher GPT-J-6B (bf16) performance vs. 5th Gen Intel Xeon 3. On 5th Gen Intel<sup>®</sup> Xeon<sup>®</sup> processors, customers can experience up to 14x better training and inference vs. 3rd Gen Intel<sup>®</sup> Xeon<sup>®</sup> processors.<sup>4</sup>
                        </p>
                        <p class="mb-0">Primary benefits of Intel<sup>®</sup> AMX include:</p>
                        <ul class="mb-0">
                            <li>
                                <div>
                                    <b>Improved performance</b>
                                </div>
                                <div>
                                    CPU-based acceleration can improve power and resource utilization efficiencies, giving you better performance for the same price. For example, 5th Gen Intel<sup>®</sup> Xeon<sup>®</sup> Platinum 8592+ with Intel<sup>®</sup> AMX BF16 has shown up to 10.7x higher real-time speech recognition inference performance (RNN-T) and 7.9x higher performance/watt vs. 3rd Gen Intel<sup>®</sup> Xeon<sup>®</sup> processors with FP32.<sup>5</sup>
                                </div>
                            </li>
                            <li>
                                <div>
                                    <b>Reduced total cost of ownership (TCO)</b>
                                </div>
                                <div>
                                    Intel<sup>®</sup> Xeon<sup>®</sup> Scalable processors with Intel<sup>®</sup> AMX enable a range of efficiency improvements that help with decreasing costs, lowering TCO, and advancing sustainability goals. As an integrated accelerator on Intel<sup>®</sup> Xeon<sup>®</sup> Scalable processors that you may already own, Intel<sup>®</sup> AMX enables you to maximize the investments you’ve already made and get more from your CPU, removing the cost and complexity typically associated with the addition of a discrete accelerator. Intel<sup>®</sup> Xeon<sup>®</sup> Scalable processors with Intel<sup>®</sup> AMX can also provide a more cost-efficient server architecture compared to other available options, delivering both power and emission reduction benefits.
                                </div>
                                <div>
                                    In a comparison with AMD Genoa 9654 servers, 5th Gen Intel<sup>®</sup> Xeon<sup>®</sup> Platinum processors with Intel<sup>®</sup> AMX delivered up to 2.69x higher batched Natural Language Processing inference (BERT-Large) performance and 2.96x higher performance per watt.<sup>6</sup>
                                </div>
                            </li>
                            <li>
                                <div>
                                    <b>Reduced development time</b>
                                </div>
                                <div>
                                    To simplify the process of developing deep learning applications, we work closely with the open source community, including the TensorFlow and PyTorch projects, to optimize frameworks for Intel<sup>®</sup> hardware, upstreaming our newest optimizations and features so they’re immediately available to developers. This enables you to take advantage of the performance benefits of Intel<sup>®</sup> AMX with the addition of a few lines of code, reducing overall development time.
                                </div>
                                <div>
                                    We also provide access to free Intel<sup>®</sup> development tools, libraries, and resources.
                                </div>
                                <p class="mb-0">&nbsp;</p>
                            </li>
                        </ul>
                    </section>
                    <section id="ds_HPC-get">
                        <h3 style="font-weight: 350; margin: 2.5rem 0 11px;">Intel<sup>®</sup> AMX Deep learning Use Cases</h3>
                        <p>
                            Intel<sup>®</sup> AMX can be deployed in a wide range of deep learning use cases to provide a significant performance boost that results in greater end user and business value.
                        </p>
                        <ul class="mb-0">
                            <li>
                                <div>
                                    <b>Recommender systems</b>:
                                    Use Intel<sup>®</sup> AMX as a more cost-effective solution for AI recommender models that boost the responsiveness of product, content, and service recommendations for use cases, including e-commerce, social media, streaming entertainment, and personalized banking. For example, content providers often use Intel<sup>®</sup> AMX to accelerate delivery of targeted movie or book recommendations and ads or to deliver a deep learning‒based recommender system that accounts for real-time user behavior signals and context features such as time and location in near-real time. 5th Gen Intel<sup>®</sup> Xeon<sup>®</sup> processors are delivering up to 8.7x higher batch Recommendation System inference performance (DLRM) and 6.2x higher performance/watt vs. 3rd Gen Intel<sup>®</sup> Xeon<sup>®</sup> processors with FP32.<sup>7</sup>
                                </div>
                            </li>
                            <li>
                                <div>
                                    <b>Natural language processing (NLP)</b>:
                                    Accelerate text-based use cases to support and scale NLP applications, such as those used in healthcare and life sciences to extract insights from clinical notes or process large amounts of medical data to help with early detection of health issues and improve care delivery. In financial services, Intel<sup>®</sup> AMX can be used to improve online chatbot responsiveness to help connect customers with the information they need more quickly while freeing limited staff up to address more-complex requests.
                                    <p class="mb-0">
                                        Similar to the cost savings benefits for recommender systems, Intel<sup>®</sup> AMX can be a more cost-effective solution for NLP. For example, when used to deploy the BERT-Large AI Natural Language model, Intel<sup>®</sup> AMX on 4th Gen Intel<sup>®</sup> Xeon<sup>®</sup> processors provided up to 79 percent savings when compared to AMD Genoa 9354.<sup>8</sup>
                                    </p>
                                </div>
                            </li>
                            <li>
                                <div>
                                    <b>Generative AI</b>:
                                    Leverage Intel<sup>®</sup> AMX to accelerate the performance of deep learning training and inference workloads for generative AI use cases such as content generation, including images, videos, and audio, language translation, data augmentation, and summarization. For example, a performance evaluation of Intel<sup>®</sup> Xeon<sup>®</sup> Platinum 8480+ processors with Intel<sup>®</sup> AMX for BF16 data types compared to Intel<sup>®</sup> Xeon<sup>®</sup> Platinum 8380 processors for FP32 data types reduced Stable Diffusion text to image generation time to less than five seconds and fine-tuning of Stable Diffusion models to less than five minutes.<sup>9</sup>
                                </div>
                            </li>
                            <li>
                                <div>
                                    <b>Computer vision</b>:
                                    Reduce the time from video and image capture to insight and action to deliver exceptional customer experiences and help your business improve efficiency and reduce operational costs. For example, in retail stores, Intel<sup>®</sup> AMX can help minimize transaction time for customers using computer vision‒enabled frictionless checkout and support near-real-time monitoring of shelves to track inventory data and instantly notify staff when an item is out of stock. In manufacturing, accelerated analysis of video from computer vision cameras on robotic arms can help enable time and cost savings with automated defect detection capabilities.
                                </div>
                            </li>
                        </ul>
                        <p>To find additional examples of how Intel<sup>®</sup> customers are using Intel<sup>®</sup> AMX to drive better business outcomes, visit our <a class="mv_special_3" href="">customer spotlight library</a>.</p>
                        <p class="mb-0">&nbsp;</p>
                    </section>
                    <section id="ds_HPC-reso">
                        <h3 style="font-weight: 350; margin: 2.5rem 0 11px;">Get Started with Intel<sup>®</sup> AMX</h3>
                        <p>
                            We offer a wide variety of development resources to help you take advantage of the integrated Intel<sup>®</sup> AMX accelerator in your Intel<sup>®</sup> Xeon<sup>®</sup> Scalable processors.
                        </p>
                        <p class="mb-0">
                            To get started, review step-by-step instructions for boosting performance with Intel<sup>®</sup> AMX in the following guides:
                        </p>
                        <ul class="mb-0">
                            <li>
                                <div>
                                    <a class="mv_special_3" href="">Intel<sup>®</sup> AI Optimizations Quick Start Guide</a>:
                                    Provides directions for improving AI workload performance with Intel<sup>®</sup> Optimized AI Libraries and Frameworks. This guide includes step-by-step instructions for TensorFlow, XGBoost, PyTorch, and more.
                                </div>
                            </li>
                            <li>
                                <div>
                                    <a class="mv_special_3" href="">Tuning guide for improving deep learning AI performance</a>:
                                    offers recommendations for tuning processors for Intel<sup>®</sup> optimized AI toolkits to achieve the best performance possible.
                                </div>
                            </li>
                        </ul>
                        <p class="mb-0">For more in-depth technical information, tutorials, code examples, and testing modules, access:</p>
                        <ul class="mb-0">
                            <li><a class="mv_special_3" href="">Intel<sup>®</sup> AMX AI frameworks</a></li>
                            <li><a class="mv_special_3" href="">Intel<sup>®</sup> AMX AI reference kits</a></li>
                            <li><a class="mv_special_3" href="">Intel<sup>®</sup> AMX developer reference guide</a></li>
                            <li><a class="mv_special_3" href="">Intel<sup>®</sup> AMX code sample</a></li>
                        </ul>
                        <p>You can access all of our tuning guides for Intel<sup>®</sup> Xeon<sup>®</sup> Scalable processors in our <a class="mv_special_3" href="">developer software tools catalog</a>.</p>

                        <p class="mb-0">To help you streamline your AI development efforts, we offer our Intel<sup>®</sup> oneAPI Toolkits, components, and optimizations, including:</p>
                        <ul>
                            <li><a class="mv_special_3" href="">Intel<sup>®</sup> oneAPI AI Analytics Toolkit</a></li>
                            <li><a class="mv_special_3" href="">Intel<sup>®</sup> oneAPI Math Kernel Library</a></li>
                            <li><a class="mv_special_3" href="">Intel<sup>®</sup> Extension for TensorFlow</a></li>
                            <li><a class="mv_special_3" href="">PyTorch Optimizations from Intel</a></li>
                        </ul>
                    </section>
                    <section>
                        <h4 style="font-weight: 400; margin: 2.5rem 0 11px;">Experiment with Intel<sup>®</sup> AMX Today</h4>
                        <p>In addition to consulting our reference materials, you can experiment with Intel<sup>®</sup> hardware, Intel<sup>®</sup> AMX, and other integrated acceleration features using <a class="mv_special_3" href="">Intel<sup>®</sup> Developer Cloud.</a></p>
                        <p>This free online platform for learning, prototyping, testing, and running workloads also includes support for a number of Intel<sup>®</sup> software development toolkits, tools, and libraries.</p>
                    </section>
                    <div>
                        <h3 style="font-weight: 350; margin: 2.5rem 0 11px;">Expand and Enhance AI Capabilities on Your CPU with Intel<sup>®</sup> AMX</h3>
                        <p>As your organization looks for solutions to meet growing compute demands to support deep learning training and inferencing workloads, Intel<sup>®</sup> AMX can help boost performance using the Intel<sup>®</sup> hardware you may already own, without the cost and complexity that comes with additional specialized hardware and in comparably less development time, using Intel<sup>®</sup> optimizations in popular open source frameworks and access to free Intel<sup>®</sup> development tools and resources.</p>
                    </div>
                </div>
            </div>
            <p class="mb-0">&nbsp;</p>
        </div>
    </section>

    <!-- Discover More -->
    <section>
        <div class="mv_discover_more_padd">
            <div class="container">
                <div class="mv_discover_more_heading">
                    <h2 class="mb-4">Discover More</h2>
                </div>
                <div class="row">
                    <div class="col-sm-6 col-12 mv_discover_more_content">
                        <div class="mv_discover_more_item">
                            <h4><a class="b_special_a2" href="">Intel<sup>®</sup> Accelerator Engines</a></h4>
                            <p>Learn more about built-in acceleration capabilities in Intel<sup>®</sup> Xeon<sup>®</sup> Scalable processors.</p>
                            <a class="b_special_a1" href="">Browse engines</a>
                        </div>
                    </div>
                    <div class="col-sm-6 col-12 mv_discover_more_content">
                        <div class="mv_discover_more_item">
                            <h4><a class="b_special_a2" href="">Intel<sup>®</sup> Xeon<sup>®</sup> Scalable Processors</a></h4>
                            <p>Find out why these powerful processors are the right choice for your demanding workloads.</p>
                            <a class="b_special_a1" href="">Browse processors</a>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section style="border-top: 1px solid #d7d7d7;" class="container py-5">
        <h4 class="h6">Product and Performance Information</h4>
        <div class="disclaimer" style="font-size: 12px;"><sup>1</sup> Availability of accelerators varies depending on SKU. Visit the <a class="b_special_a1" href="">Intel Product Specifications page</a> for additional product details.</div>

        <div class="disclaimer" style="font-size: 12px;"><sup>2</sup> See [A16] and [A17] at <a class="b_special_a1" href="">intel.com/processorclaims</a>: 4th Gen Intel<sup>®</sup> Xeon<sup>®</sup> Scalable processors. Results may vary.</div>

        <div class="disclaimer" style="font-size: 12px;"><sup>3</sup> See [9A10] at <a class="b_special_a1" href="">intel.com/processorclaims</a>: Intel<sup>®</sup> Xeon<sup>®</sup> 6. Results may vary.</div>

        <div class="disclaimer" style="font-size: 12px;"><sup>4</sup> See [A15-A16] at <a class="b_special_a1" href="">intel.com/processorclaims</a>: 5th Gen Intel Xeon Scalable processors. Results may vary.</div>

        <div class="disclaimer" style="font-size: 12px;"><sup>5</sup> See [A23] at <a class="b_special_a1" href="">intel.com/processorclaims</a>: 5th Gen Intel Xeon processors. Results may vary.</div>

        <div class="disclaimer" style="font-size: 12px;"><sup>6</sup> See [A210] at <a class="b_special_a1" href="">intel.com/processorclaims</a>: 5th Gen Intel Xeon processors. Results may vary.</div>

        <div class="disclaimer" style="font-size: 12px;"><sup>7</sup> See [A20] at <a class="b_special_a1" href="">intel.com/processorclaims</a>: 5th Gen Intel Xeon processors. Results may vary.</div>

        <div class="disclaimer" style="font-size: 12px;"><sup>8</sup> Based on 4th Gen Intel<sup>®</sup> Xeon<sup>®</sup> processors that deliver up to 5.60x faster than the 4th Gen AMD EPYC while running a BERT-Large workload. This performance drives a fleet reduction from 50 to nine servers, which saves USD 424K kWh per year of electricity, reduces 719,546 kgCO2 over four years, and drives a cost savings of USD 1.38M. BERT-Large: 8462Y+: 1-node, 2x 4th Gen Intel<sup>®</sup> Xeon<sup>®</sup> Scalable processor 8462Y+(32c/2.8 GHz, 300W TDP) on Supermicro SYS-221H-TNR server with 1024 GB (16x64 GB/4800) DDR5 memory, microcode 0x2b000161, HT on, Turbo on, SNC off, Ubuntu 22.04.2 LTS, 5.15.0-58-generic, 1x 1.92T SAMSUNG MZQL21T9HCJR-00A07, Framework=Intel<sup>®</sup> TF 2.11.dev202242, Python 3.8, <a class="b_special_a1" href="">AI Model=BERT-Large</a>, step size=30, warmup=10; Batched results: Best scores achieved using FP32(BS=64[16 cores/instance]), BFloat16(BS=64[4 cores/instance], int8-AMX(BS=64[1 cores/instance), Real Time(BS1) results while maintaining 130ms latency; SLA: Best scores achieved using FP32 (32 cores/instance), BFloat16 (four cores/instance), int8-AMX (four cores/instance), tested by Intel, April 2023. BERT-Large: 9354: 1-node, 2x AMD EPYC processor 9354 (32c/3.25 GHz, 280W TDP) on Supermicro H13DSH server with 1536 GB (24x64 GB /4800) DDR5 memory, microcode 0xa101111, SMT on, Boost on, NPS=1, Ubuntu 22.04.2 LTS, 5.15.0-58-generic , 1x 1.92T SAMSUNG MZQL21T9HCJR-00A07, Framework=Stock TF 2.10.1, ZenDNN=v4.0, Python 3.8 , <a class="b_special_a1" href="">AI Model=BERT-Large</a>, step size=30, warmup=10; Batched results: Best scores achieved using FP32(BS=64[1 cores/instance]), tested by Intel, April 2023. Costs are based on Intel estimated and information from thinkmate.com:
        </div>
        
        <ul style="font-size: 12px;">
            <li>For a 50-server fleet of AMD EPYC 9354, estimated as of March 2023:
                <ul>
                    <li>CapEx costs: USD 1.01M</li>
                    <li>OpEx costs (four years, includes power and cooling utility costs, infrastructure, and hardware maintenance costs): USD 732.6K</li>
                    <li>Energy use in kWh (four years, per server): 43169, PUE 1.6</li>
                    <li>Other assumptions: Utility cost USD 0.1/kWh, kWh to kg CO2 factor 0.42394</li>
                </ul>
            </li>
            <li>For a nine-server fleet of 4th Gen Intel® Xeon® 8462Y as of March 2023:
                <ul>
                    <li>CapEx costs: USD 222K</li>
                    <li>OpEx costs (four years, includes power and cooling utility costs, infrastructure, and hardware maintenance costs): USD 139K</li>
                    <li>Energy use in kWh (four years, per server): 51242, PUE 1.6</li>
                    <li>Other assumptions: Utility cost USD 0.1/kWh, kWh to kg CO2 factor 0.42394</li>
                </ul>
            </li>
        </ul>

        <div class="disclaimer" style="font-size: 12px;"><sup>9</sup> 4th Gen Intel® Xeon® Scalable processor fine-tuning: Test by Intel on 12/09/2022. One to four nodes, 2S, Intel® Xeon® Platinum 8480+ 56 cores on Dennard Pass platform and software with 512 GB memory (16x32 GB DDR5 4800 MT/s [4800 MT/s]), microcode 0x90000c0, HT on, Turbo on, Rocky Linux 8.7, 4.18.0-372.32.1.el8_6.crt2.x86_64, 931.5G SSD. Multiple nodes connected with 200 Gbps OmniPath. PyTorch 1.13, IPEX 1.13, Transformers 4.24.0, Accelerate 0.14, Diffusers 0.8.0, oneDNN 2.6.0, oneCCL 2021.7.1. 4th Gen Intel® Xeon® Scalable processor inference: Test by Intel on 12/09/2022. One-node, 2S, Intel® Xeon® Platinum 8480+ 56 cores on Archer City platform and software with 1024 GB (16x64 GB DDR5 4800 MT/s [4800 MT/s]), microcode 0x2b000111, HT on, Turbo on, Ubuntu 22.04.1 LTS, 5.15.0-56-generic, 1.5 TB SSD. Multiple nodes connected with 100 Gbps Ethernet Controller I225-LM. PyTorch (commit ID: 26d1dbc) + PR 81852, Transformers 4.25.1, Accelerate 0.14, Diffusers 0.8.0, oneDNN 2.6.0. 3rd Gen Intel® Xeon® Scalable processor inference: Test by Intel on 12/09/2022. One-node, 2S, Intel® Xeon® Platinum 8380 CPU @ 2.30 GHz 40 cores on WHITLEY platform and software with 512 GB (16x32 GB DDR4 3200 MT/s [3200 MT/s]), microcode 0xd000375, HT on, Turbo on, Ubuntu, 5.15.0-56-generic, 7.0 TB SSD. Multiple nodes connected with 10 Gbps Ethernet Controller X710 for 10GBASE-T. PyTorch (commit ID: 26d1dbc), Transformers 4.25.1, Accelerate 0.14, Diffusers 0.8.0, oneDNN 2.6.0. Performance varies by use, configuration, and other factors. Learn more at intel.com/PerformanceIndex. Performance results are based on testing as of dates shown in configurations and may not reflect all publicly available updates. See backup for configuration details. No product or component can be absolutely secure.
        </div>
    </section>

    <!-- footer -->
    <div id="footer"></div>

    <!-- script header and footer -->
    <script>
        // navbar include  
        fetch('/y_index/y_navbar.html')
            .then(response => response.text())
            .then(data => {
                document.getElementById('navbar').innerHTML = data;
            });
        // footer include 
        fetch('/y_index/y_footer.html')
            .then(response => response.text())
            .then(data => {
                document.getElementById('footer').innerHTML = data;
            });
    </script>

    <!-- nav script -->
    <script>
        document.addEventListener('DOMContentLoaded', function () {
        const nav = document.querySelector('.VK_client_app_navigation');
        const navLinks = document.querySelectorAll('.VK_ai_nav_bar a');
        const sections = document.querySelectorAll('section[id]');
        let navOffset = nav.offsetTop;

            // Add smooth scrolling to all links
            navLinks.forEach(link => {
                link.addEventListener('click', function (e) {
                    e.preventDefault();
                    document.querySelector(this.getAttribute('href')).scrollIntoView({
                        behavior: 'smooth'
                    });
                });
            });

            // Sticky Navigation
            window.addEventListener('scroll', () => {
                if (window.pageYOffset >= navOffset) {
                    nav.classList.add('VK_sticky_nav_bar');
                } else {
                    nav.classList.remove('VK_sticky_nav_bar');
                }                
                // Section highlighting
                sections.forEach(section => {
                    const sectionTop = section.offsetTop - nav.clientHeight;
                    const sectionHeight = section.clientHeight;
                    console.log(sectionTop);
                    console.log(sectionHeight);
                    if (window.pageYOffset >= sectionTop && window.pageYOffset <= sectionTop + sectionHeight) {
                        navLinks.forEach(link => {
                            link.classList.remove('active');
                            if (link.getAttribute('href') === `#${section.id}`) {
                                link.classList.add('active');
                                
                                // Ensure the active link is visible in the nav bar
                                const navBar = document.querySelector('.VK_ai_nav_bar');
                                const activeLink = document.querySelector('.VK_ai_nav_bar a.active');
                                const linkRect = activeLink.getBoundingClientRect();
                                const navBarRect = navBar.getBoundingClientRect();

                                if (linkRect.left < navBarRect.left || linkRect.right > navBarRect.right) {
                                    activeLink.scrollIntoView({ inline: 'center', behavior: 'smooth' });
                                }
                            }
                        });
                    }
                });
            });
        });
    </script>

</body>
</html>