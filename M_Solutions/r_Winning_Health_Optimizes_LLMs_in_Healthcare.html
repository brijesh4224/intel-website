<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Document</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/css/bootstrap.min.css" rel="stylesheet"
        integrity="sha384-EVSTQN3/azprG1Anm3QDgpJLIm9Nao0Yz1ztcQTwFspd3yD65VohhpuuCOmLASjC" crossorigin="anonymous">

    <link href=" https://fonts.cdnfonts.com/css/intel-clear " rel="stylesheet">
    <link rel="stylesheet" href="../css/owl.carousel.min.css">

    <link rel="stylesheet" href="../css/rushita.css">
    <link rel="stylesheet" href="../css/rushita_automotive.css">
    <link rel="stylesheet" href="../css/yatri.css">
    <link rel="stylesheet" href="../css/owl.theme.default.min.css">
    <style>
        * {
            font-family: 'Intel Clear', sans-serif;
        }

        img {
            image-rendering: -webkit-optimize-contrast;
            image-rendering: crisp-edges;
            -ms-interpolation-mode: nearest-neighbor;
        }
    </style>
</head>

<body>
    <div id="navbar"></div>

    <section class="m_ai_tdrop" style="padding-top: 75px;">
        <div class="m_ai_httl">Winning Health Optimizes LLMs in Healthcare</div>
    </section>
    <section style="background-color: #0068B5;">
        <div class="k_container11">
            <div class="row k_bgpadd py-5">
                <div
                    class="col-lg-9 col-md-12 col-sm-12   mx-0 text-white align-content-center  k_tstartauto k_text2  px-0 mx-0 k_mdorder2 ">
                    <h3 class="fs-3 mb-4 fw-lighter k_marketsmall">Winning Health Optimizes LLMs in Healthcare</h3>
                    <h6>Winning Health LLM, WiNGPT, leveraging Intel® technology, meets performance demands for
                        healthcare institutions.</h6>
                </div>

            </div>
        </div>
    </section>
    <section class="k_space k_spacenone">
        <div class="k_container11">
            <div class="row my-5 g-3">
                <div class="float-end k_bignone" style="padding-left:70%;">
                    <div>
                        <a href="#" class="fs-4"><i class="fa-solid fa-print mx-3 k_icon0"></i></a>
                        <a href="#" class="fs-4"> <i class="fa-regular fa-envelope k_icon0"></i></a>

                    </div>
                </div>
                <div class="col-lg-3 col-md-4 col-sm-12">
                    <div class="k_bggrey p-3">
                        <p><b>At a glance:</b></p>
                        <ul>
                            <li>
                                <p>Winning Health Technology Group Co., Ltd. was founded to “enhance science and
                                    technology and improve people's health.”</p>
                            </li>
                            <li>
                                <p>Building on its leading medical Large Language Model (LLM) WiNGPT, Winning Health
                                    collaborated with Intel in graph optimization and weight-only quantization on 5th
                                    Gen Intel® Xeon® Scalable processors with Intel® Advanced Matrix Extensions for
                                    model inference.
                                </p>
                            </li>
                        </ul>
                        <div><button
                                style="background-color: #0068B5; color: white; border: none; padding: 0.7rem 1.5rem;">Download
                                the one-page summary</button></div>
                    </div>
                </div>
                <div class="col-lg-8 col-md-7 col-sm-12 k_20px k_16smpx k_width75">
                    <div class="float-end mx-3 k_smnone">
                        <div>
                            <a href="#" class="fs-4"><i class="fa-solid fa-print mx-3 k_icon0"></i></a>
                            <a href="#" class="fs-4"> <i class="fa-regular fa-envelope k_icon0"></i></a>

                        </div>
                    </div>
                    <div>

                        <h2 style="margin-top: 2.5rem;" class="fw-lighter">Overview</h2>
                        <p>In the current landscape of smart hospital advancement, it is widely recognized that large
                            language models (LLMs), as a groundbreaking technology, have significant potential to be
                            applied in medical settings. Applications powered by LLMs, such as medical literature
                            analysis, healthcare Q&A, medical report generation, AI-assisted imaging diagnosis,
                            pathology analysis, chronic disease monitoring and management, and medical record sorting,
                            all contribute to leveling up the efficiency and quality of medical services, reducing costs
                            for medical institutions in manpower and other resources, while improving the overall
                            experience for patients. One major obstacle in the wider use of LLMs in healthcare
                            institutions, however, is the lack of high-performance and cost-effective computing
                            platforms. Take model inference: the sheer complexity and scale of LLMs far exceed those of
                            common AI applications, posing a challenge for traditional computing platforms to adequately
                            meet their demands.
                        </p>
                        <div style="padding: 30px; display: flex; align-items: start;">
                            <i class="fa-solid fa-quote-left me-4" style="font-size: 100px; color: #E5E6E6;"></i><i>“The
                                innovation and widespread application of LLMs represent an important trend in the
                                development of smart hospitals. However, the lean operations in hospitals underscores
                                the pressing need to better unleash the potential of applying LLMs in smart healthcare
                                services with lower deployment costs. Through our collaboration with Intel, we have
                                found a CPU-based LLM inference solution that not only meets the performance
                                requirements but also offers cost advantages, helping accelerate the deployment of LLMs
                                in hospitals, while providing intelligent knowledge services across various hospital
                                scenarios.”—Zhao Daping, Vice President and CTO, Winning Health
                            </i>
                        </div>
                        <p>Building on its leading medical LLM WiNGPT, Winning Health has introduced the WiNGPT solution
                            based on <a href=""
                                style="color: #0068B5; text-decoration: underline  dashed !important;text-underline-offset: .3rem !important;">5th
                                Gen Intel® Xeon® Scalable processors1.</a> The solution effectively leverages the
                            built-in accelerators including <a href=""
                                style="color: #0068B5; text-decoration: underline  dashed !important;text-underline-offset: .3rem !important;">Intel®
                                Advanced Matrix Extensions (Intel® AMX)</a> in these
                            processors for model inference. Through collaboration with Intel in areas like graph
                            optimization and weight-only quantization, the inference performance has been increased by
                            over 3 times compared with the platform based on the 3rd <a href=""
                                style="color: #0068B5; text-decoration: underline  dashed !important;text-underline-offset: .3rem !important;">
                                Gen Intel® Xeon® Scalable
                                processors.1</a> The enhancement meets the performance demand for scenarios like
                            automated
                            medical report generation, accelerating the adoption of LLM applications in healthcare
                            institutions.</p>
                        <h2 style="margin-top: 2.5rem;" class="fw-lighter">Challenge: The Compute Conundrum in Medical
                            LLM Inference</h2>
                        <p>The extensive use of LLMs in various verticals such as healthcare is considered a milestone
                            for the real-world application of this technology. Healthcare institutions are stepping up
                            their investments and have made considerable progress in LLMs for medical diagnostics,
                            services, and management. Research forecasts that 2023 to 2027 will witness a surge in the
                            adoption of LLMs in the healthcare industry, with the market size expected to exceed 7
                            billion yuan by 2027.2</p>
                        <div style="padding: 30px; display: flex; align-items: start;">
                            <i class="fa-solid fa-quote-left me-4" style="font-size: 100px; color: #E5E6E6;"></i><i>"The
                                combination of LLMs plus healthcare opens up endless possibilities for the healthcare
                                industry. Yet, the obstacles standing between aspirations and applications aren't just
                                technical, but also the steep cost of deploying LLMs. As the latest-generation
                                processors tailored for the AI era, the 5th Gen Intel Xeon Scalable processors offer
                                more than powerful AI performance, but also cost-effectiveness and exceptional
                                flexibility in deployment, which means they can better meet the demands of LLMs applied
                                in medical scenarios and expedite the development of smart hospitals.” —Eric Tang,
                                General Manager, Software Technology Solution Group, Intel China
                            </i>
                        </div>
                        <p>LLMs are typical compute-intensive applications, and their training, fine-tuning, and
                            inference all rely on substantial computing resources, resulting in huge computing costs.
                            Among these, model inference stands out as a crucial stage in LLM deployment. When creating
                            model inference solutions, healthcare institutions are commonly confronted with the
                            following challenges:</p>
                        <ul>
                            <li>The scenarios are complex, with a high demand for real-time accuracy. This requires the
                                computing platform to be powerful enough in inference. Additionally, given the stringent
                                security requirements for medical data, healthcare institutions usually prefer the
                                platform to be deployed locally rather than on the cloud.</li>
                            <li>Hardware upgrades do not happen frequently, while LLM upgrades may require GPUs to be
                                upgraded accordingly. As a result, updated models may not be able to work on legacy
                                hardware.</li>
                            <li>The hardware requirements for the inference of Transformer-based LLMs have seen a
                                substantial rise than in the past. Both memory and time complexity scale exponentially
                                with the length of the input sequence, making it difficult for previous computing
                                resources to be fully utilized. Consequently, hardware utilization has yet to reach its
                                optimal level.</li>
                            <li>From a cost perspective, deploying servers dedicated to model inference would incur
                                higher costs and such servers would be limited in usage. Given this, many healthcare
                                institutions prefer to use CPU-based server platforms for inference to cut hardware
                                expenses with the flexibility to support various workloads.</li>
                        </ul>
                        <h2 style="margin-top: 2.5rem;" class="fw-lighter">Solution: WiNGPT Based on 5th Gen Intel®
                            Xeon® Scalable Processors
                        </h2>
                        <p>WiNGPT by Winning Health is an LLM specifically designed for the healthcare sector. Built on
                            the general-purpose LLM, WiNGPT integrates high-quality medical data, and is optimized and
                            customized for medical scenarios, allowing it to provide intelligent knowledge services
                            across different healthcare scenarios. WiNGPT is characterized by the following three
                            distinctive aspects:</p>
                        <ul>
                            <li><b>Fine-tuned and specialized:</b> WiNGPT is trained and fine-tuned for medical
                                scenarios and on high- quality data, delivering exceptional data accuracy that meets
                                diverse business requirements. </li>
                            <li><b>Low cost: </b> Via algorithm optimization, the deployment based on CPU is already
                                tested to have gained the generation efficiency close to that of GPU.
                            </li>
                            <li><b>Support customized private deployment: </b> Private deployment ensures that medical
                                data stays within healthcare institutions, preventing data leaks while offering better
                                system stability and reliability. Moreover, it allows for customized options for
                                organizations of varying needs to accommodate different budget plans.
                            </li>
                        </ul>
                        <P>To accelerate WiNGPT's inference speed, Winning Health has partnered with Intel by opting for
                            the 5th Gen Intel Xeon Scalable processors. These processors offer enhanced reliability and
                            energy efficiency, delivering significant performance gains per watt across various
                            workloads and exceptional performance in AI, data center, network and HPC, all while
                            maintaining a lower total cost of ownership (TCO). Compared with the previous generation,
                            the 5th Gen Intel Xeon Scalable processors offer increased computing power and faster memory
                            within the same range of power consumption. Additionally, they are compatible with last
                            generation’s software and platforms, significantly saving testing and validation efforts
                            when deploying new systems.</P>
                        <p>The 5th Gen Intel Xeon Scalable processors are built-in with several AI-optimized features
                            including Intel AMX, taking AI performance to the next level. Intel AMX adopts a new
                            instruction set and circuit design, significantly boosting the instructions per cycle (IPC)
                            for AI applications by enabling matrix operations. The advancement leads to a notable
                            performance improvement for both training and inference in AI workloads. </p>
                        <P>The 5th Gen Intel Xeon Scalable processor allows for:</P>
                        <ul>
                            <li>
                                Up to 21 percent overall performance gains3
                            </li>
                            <li>Up to 42 percent higher inference performance4</li>
                            <li>Up to 16 percent faster memory speed5</li>
                            <li>Up to 2.7 times larger L3 cache6</li>
                            <li>Up to 10 times higher performance per watt7</li>
                        </ul>
                        <p>In addition to the 5th Gen Intel Xeon Scalable processors, Winning Health and Intel are also
                            exploring ways to address the memory access bottleneck in LLM inference on the current
                            hardware platform. LLMs are usually considered memory-bound due to their extensive parameter
                            size, which often requires billions or even tens of billions of model weights to be loaded
                            into memory for computing. When computing is underway, vast data needs to be stored in
                            memory temporarily and read for subsequent computing. The speed of memory access—instead of
                            the computing power—has thus become the primary hindrance dragging down inference speed.</p>
                        <p>Winning Health and Intel have taken the following measures to optimize memory access and
                            beyond:</p>
                        <ul>
                            <li><b>Graph optimization:</b>Graph optimization refers to the process of merging multiple
                                operators to reduce the overhead of operator/core calls. Combining several operators
                                into a single operation saves the consumption of memory resources once required for the
                                read-ins and read-outs of different operators, thus improving the performance. In these
                                processes, Winning Health has used Intel® Extension for PyTorch to optimize the
                                algorithms, resulting in effective performance boost. With Intel® Extension for PyTorch,
                                Intel uses acceleration libraries such as oneDNN and oneCCL in the form of
                                intel-extension-for-pytorch as a plug-in to improve PyTorch performance on servers based
                                on Intel Xeon Scalable processors and <a href="">Intel® Iris® Xe graphics</a>.</li>
                        </ul>
                        <p class="mb-5 mt-5"><i>Figure 1. Intel Optimizations for PyTorch.</i></p>
                        <ul>
                            <li><b>Weight-only quantization:</b> Weight-only quantization is a type of optimization for
                                LLMs. As long as the computing accuracy is guaranteed, the parameter weights are
                                converted to INT8 data type, but restored to half-precision during computing, which
                                helps to reduce the memory space occupied by model inference, speeding up the overall
                                computing process.
                            </li>
                        </ul>
                        <p class="mb-5 mt-5"><i>Figure 2. Optimized architecture for WiNGPT.</i></p>
                        <p>Winning Health and Intel have jointly optimized WiNGPT's inference performance by improving
                            memory utilization. The two have also collaborated to fine-tune the key operator algorithms
                            for PyTorch on CPU platforms, delivering further inference acceleration for the deep
                            learning framework. </p>
                        <p>In a test-based validation environment, the inference performance of the LLaMA2 model reached
                            52ms/token. With automated medical report generation, a single output takes less than 3s.8
                        </p>
                        <p>During the test, Winning Health also compared the performance of the 5th Gen Intel® Xeon®
                            Scalable processor-based solution with that of the 3rd Gen. The result shows the latest
                            generation processors deliver over 3x performance boost over the 3rd generation.8</p>
                        <h2 style="margin-top: 2.5rem;" class="fw-lighter">Solution: HCLTech Deployment of Intel vPro
                            Platform</h2>
                        <p class="mb-5 mt-5"><i>Figure 3. Performance results of WiNGPT on different generations of
                                Intel Xeon processors.</i></p>
                        <p>As the business scenarios in which WiNGPT is used are relatively tolerant of LLM latency, the
                            robust performance of the 5th Gen Intel Xeon Scalable processor is sufficient enough to meet
                            user needs. Meanwhile, the CPU-based solution is also easily scalable for inference
                            instances and can be adapted to perform inference on a variety of platforms.</p>
                        <h2 style="margin-top: 2.5rem;" class="fw-lighter">Benefits</h2>
                        <p>WiNGPT solution based on the 5th Gen Intel Xeon Scalable processors has delivered the
                            following benefits to healthcare institutions:</p>
                        <ul>
                            <li>
                                <b>Optimized LLM performance with enhanced application experience:</b>With technical
                                optimizations by both sides, the solution has fully leveraged the AI performance
                                advantages with the 5th Gen Intel Xeon Scalable processors. It can meet the performance
                                requirements for model inference in scenarios such as medical report generation,
                                resulting in shortened generation time with guaranteed user experience.
                            </li>
                            <li>
                                <b>Improved cost-effectiveness with platform building cost kept under control:</b>The
                                solution can utilize the general-purpose servers already in use in healthcare
                                institutions for inference, eliminating the need to add dedicated inference servers,
                                which helps to reduce costs of procurement, deployment, operation, maintenance, and
                                energy consumption.
                            </li>
                            <li>
                                <b>Well-balanced between LLMs and other IT applications:</b>The fact that the solution
                                manages to use CPU for inference means healthcare
                                institutions can flexibly allocate CPU’s computing power between LLM inference and other
                                IT applications as needed, which improves the agility and flexibility of computing power
                                allocation.
                            </li>
                        </ul>
                        <h2 style="margin-top: 2.5rem;" class="fw-lighter">Looking Ahead</h2>
                        <p>The 5th Gen Intel® Xeon® Scalable CPUs provide excellent inferencing performance, especially when used in conjunction with WiNGPT, making the application of the LLM easier and more cost-effective. Both sides will continue to refine their work on LLMs to make Winning Health’s latest AI technologies accessible and beneficial to more users.</p>
                        <p><a href=""
                                style="color: #0068B5; text-decoration: underline  dashed !important;text-underline-offset: .3rem !important;">Download
                                the PDF ›</a></p>
                    </div>

                </div>
            </div>
    </section>

    <section class="k_bggrey py-5">
        <div class="k_container11">
            <div class="row mb-4">
                <div class="col-lg-9 col-md-9 col-sm-12">
                    <h1 class="fw-lighter">Explore Related Stories</h1>
                </div>
                <div class="col-lg-2 col-md-2 col-sm-12 align-content-center">
                    <p class="text-center k_btnlearn py-2 px-4  k_text m-auto k_under k_btn100"
                        style="float: inline-start; width: 200px; background-color: #0068B5;">
                        <a href="../M_Solutions/m_sol_re_Customer_Spotlight.html " class="text-white">Intel Customer
                            Spotlight <i class="fa-solid fa-arrow-right"></i></a>
                    </p>
                </div>
            </div>
            <div class="row g-3">

                <div class="col-lg-3 col-md-6 col-sm-12">
                    <img src="../img/Monika_img/nec.avif" alt="" width="100%">
                    <h4 class="mt-3 fw-lighter k_plink1 k_bluehover1"><a
                            href="../M_Solutions/r_NEC_Implements_Real-Time_Occupancy_Monitoring.html">NEC Implements
                            Real-Time Occupancy Monitoring</a></h4>
                </div>
                <div class="col-lg-3 col-md-6 col-sm-12">
                    <img src="../img/Monika_img/ibm.avif" alt="" width="100%">
                    <h4 class="mt-3 fw-lighter k_plink1 k_bluehover1"><a
                            href="../M_Solutions/r_IBM_watsonx_data_Accelerates_GenAI_Data_Analysis.html">IBM
                            watsonx.data Accelerates GenAI Data Analysis</a></h4>
                </div>
                <div class="col-lg-3 col-md-6 col-sm-12">
                    <img src="../img/Monika_img/reply.png" alt="" width="100%">
                    <h4 class="mt-3 fw-lighter k_plink1 k_bluehover1"><a
                            href="../d_partner/d_storm-reply-customer-story.html">Storm Reply Taps EC2 C7i Instances on
                            CPUs for LLM</a></h4>
                </div>
                <div class="col-lg-3 col-md-6 col-sm-12">
                    <img src="../img/Monika_img/nhn.jpg" alt="" width="100%">
                    <h4 class="mt-3 fw-lighter k_plink1 k_bluehover1"><a
                            href="../M_Solutions/r_NHN_Cloud_Offers_New_AI_Cloud_Service.html">NHN Cloud Offers New AI
                            Cloud Service</a>
                    </h4>
                </div>
            </div>
        </div>
    </section>
    <section class=" py-4 k_bgblue text-white">
        <div class="k_container11">
            <h2 class="text-white fw-lighter pb-5">Explore Related Products and Solutions</h2>
            <div class="row k_space justify-content-between">

                <div class="col-lg-6 col-md-6 col-sm-12 d-lg-flex d-block gap-3">
                    <div class=" k_order1 col-lg-5 col-md-5 col-sm-12 ">
                        <a href="#">
                            <img src="../img/rushita_img/reply4.webp" alt="" width="100%"></a>

                    </div>
                    <div class="k_wrap  flex-wrap k_order2 col-lg-7 col-md-7 col-sm-12 ">
                        <h4 class=" mb-3"><a href="#" class=" k_a_whiteunder ">Intel® Xeon® Scalable Processors</a></h4>
                        <p>Drive actionable insight, count on hardware-based security, and deploy dynamic service
                            delivery with Intel® Xeon® Scalable processors.</p>
                        <p class="k_a_whiteunder">Learn more</p>
                    </div>
                </div>
                <div class="col-lg-6 col-md-6 col-sm-12  d-lg-flex d-block gap-3">
                    <div class=" k_order1  col-lg-5 col-md-5 col-sm-12 ">
                        <a href="#">
                            <img src="../img/rushita_img/reply5.jpg" alt="" width="100%">

                    </div>
                    <div class="k_wrap  flex-wrap k_order2 col-lg-7 col-md-7 col-sm-12 ">
                        <h4 class=" mb-3"><a href="#" class=" k_a_whiteunder ">
                                Intel® Software Guard Extensions (Intel® SGX)Intel® Software Guard Extensions (Intel®
                                SGX)</a>
                        </h4>
                        Take control of protecting your data.
                        <p class="k_a_whiteunder">Learn more</p>

                    </div>
                </div>
            </div>
        </div>
    </section>
    <section style="background-color: #8F5DA2;">
        <div class="k_container11">
            <div class="row py-5 text-white g-3">
                <div class="col-lg-4 col-md-4 col-sm-12">
                    <h3 class="k_20px"><b>Customer Stories and Case Studies</b></h3>
                    <p>Explore the latest customer stories, case studies, and news releases highlighting data-centric
                        innovations.</p>
                    <ul>
                        <li class="k_a_whiteunder">
                            <a href="#">Intel Customer Spotlight</a>
                        </li>
                        <li class="k_a_whiteunder">
                            <a href="#">Intel Newsroom</a>
                        </li>
                    </ul>
                </div>
                <div class="col-lg-4 col-md-4 col-sm-12">
                    <h3 class="k_20px"><b>Data Center Workloads</b></h3>
                    <p>Learn how Intel® technologies can help provide the scalability needed for high-demand workloads
                        and applications.</p>
                    <ul>
                        <li class="k_a_whiteunder">
                            <a href="#">Advanced Analytics</a>
                        </li>
                        <li class="k_a_whiteunder">
                            <a href="#">Artificial Intelligence (AI)</a>
                        </li>
                        <li class="k_a_whiteunder">
                            <a href="#">Cloud Computing</a>
                        </li>
                        <li class="k_a_whiteunder">
                            <a href="#">High Performance Computing (HPC)</a>
                        </li>
                </div>
                <div class="col-lg-4 col-md-4 col-sm-12">
                    <h3 class="k_20px"><b>Data Center Insights</b></h3>
                    <p>Get the latest information about Intel data center performance, flexibility, and scalability.</p>
                    <ul>
                        <li class="k_a_whiteunder">
                            <a href="#">Cloud Service Provider Resources</a>
                        </li>
                        <li class="k_a_whiteunder">
                            <a href="#">Network Transformation & Communications Technology</a>
                        </li>
                </div>
            </div>
        </div>
    </section>
    <section class="k_space">
        <div class="k_container11">
            <div class="row">
                <p>Product and Performance Information</p>
                <p class="k_14px"><sup>1</sup>Data quoted from the test of Alibaba Cloud in June 2022. Test
                    configurations: 3-node clusters (g7t.32xlarge Alibaba Cloud ECS instance), 2x Intel® Xeon® Platinum
                    8369B processors, 64 cores, hyperthreading enabled, 256 GB total memory, 256 GB EPC, Ubuntu 20.04.2
                    LTS, and 5.17. 0 kernels. Test the runtime with and without Intel® SGX enabled. Intel does not
                    control or audit third-party data. You should review this content and consult other sources to
                    evaluate accuracy.</p>
            </div>
        </div>
    </section>
    <div id="footer"></div>
    <script>
        // navbar include  
        fetch('../y_index/y_navbar.html')
            .then(response => response.text())
            .then(data => {
                document.getElementById('navbar').innerHTML = data;
            });
        // footer include 
        fetch('../y_index/y_footer.html')
            .then(response => response.text())
            .then(data => {
                document.getElementById('footer').innerHTML = data;
            });
    </script>
    <script>


        document.addEventListener('DOMContentLoaded', function () {
            var video1 = document.getElementById('myVideo4');
            var playButton1 = document.getElementById('playButton4');
            var video2 = document.getElementById('myVideo5');
            var playButton2 = document.getElementById('playButton5');

            function setupVideoControl(video, playButton) {
                playButton.addEventListener('click', function () {
                    if (video.paused) {
                        video.play();
                        playButton.innerHTML = '<i class="far fa-circle-pause"></i>';
                    } else {
                        video.pause();
                        playButton.innerHTML = '<i class="far fa-circle-play"></i>';
                    }
                });

                video.addEventListener('play', function () {
                    playButton.innerHTML = '<i class="far fa-circle-pause"></i>';
                });

                video.addEventListener('pause', function () {
                    playButton.innerHTML = '<i class="far fa-circle-play"></i>';
                });

                playButton.style.display = 'block';
                playButton.style.position = 'absolute';
                playButton.style.top = '50%';
                playButton.style.left = '50%';
                playButton.style.transform = 'translate(-50%, -50%)';
                playButton.style.zIndex = '10';
            }

            setupVideoControl(video1, playButton1);
            setupVideoControl(video2, playButton2);
        });

    </script>
    <script src="https://cdn.jsdelivr.net/npm/@splidejs/splide@4.1.4/dist/js/splide.min.js"></script>
    <script src="../js/jquery-3.7.1.js"></script>
    <!-- <script src="js/jquery-3.6.4.min.js"></script> -->
    <script src="../js/owl.carousel.min.js"></script>
    <script src="../js/all.min.js"></script>
    <script src="../js/rushita.js"></script>


    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/js/bootstrap.bundle.min.js"
        integrity="sha384-MrcW6ZMFYlzcLA8Nl+NtUVF0sA7MsXsP1UyJoMp4YLEuNSfAP+JcXn/tWtIaxVXM"
        crossorigin="anonymous"></script>
    <script>
        $(document).ready(function () {
            $('.owl-carousel').owlCarousel({
                loop: true,
                margin: 20,
                nav: true,
                // dots:false,
                navText: ['Prev', 'Next'],
                navText: ['<i class="fa-solid fa-angle-left"></i>', '<i class="fa-solid fa-angle-right"></i>'],
                responsive: {
                    0: {
                        items: 1
                    },
                    600: {
                        items: 1
                    },
                    1000: {
                        items: 1
                    }
                }
            })
        });
    </script>


</body>

</html>