<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Document</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/css/bootstrap.min.css" rel="stylesheet"
        integrity="sha384-EVSTQN3/azprG1Anm3QDgpJLIm9Nao0Yz1ztcQTwFspd3yD65VohhpuuCOmLASjC" crossorigin="anonymous">

    <link href=" https://fonts.cdnfonts.com/css/intel-clear " rel="stylesheet">
    <link rel="stylesheet" href="../css/owl.carousel.min.css">

    <link rel="stylesheet" href="../css/rushita.css">
    <link rel="stylesheet" href="../css/rushita_automotive.css">
    <link rel="stylesheet" href="../css/yatri.css">
    <link rel="stylesheet" href="../css/owl.theme.default.min.css">
    <style>
        * {
            font-family: 'Intel Clear', sans-serif;
        }
    </style>
</head>

<body>
    <div id="navbar"></div>

    <section class="m_ai_tdrop" style="padding-top: 75px;">
        <div class="m_ai_httl">How to Customize Generative AI Models</div>
    </section>
    <section class="k_bgblue">
        <div class="k_container11">
            <div class="row k_bgpadd py-3">
                <div
                    class="col-lg-8 col-md-12 col-sm-12   mx-0 text-white align-content-center  k_tstartauto k_text2  px-0 mx-0 k_mdorder2 ">
                    <h3 class="fs-3 mb-2 fw-lighter k_marketsmall">Accelerate Generative AI Model Customization</h3>
                    <p class=" fs-5">Learn about the model customization techniques that can help your enterprise
                        deliver generative AI (GenAI) capabilities quickly and cost-effectively.</p>
                </div>
                <div class="col-lg-3 col-md-12 col-sm-12 k_mdimg  k_mdorder1">
                    <img src="../img/rushita_img/costmize.avif" alt="" width="100%" >
                </div>
            </div>
        </div>
    </section>
    <section class="my-3 k_spacenone">
        <div class="k_container11">
            <div class="row g-3">
                <div class="float-end k_bignone" style="padding-left:70%;">
                    <div>
                        <a href="#" class="fs-4"><i class="fa-solid fa-print mx-3 k_icon0"></i></a>
                        <a href="#" class="fs-4"> <i class="fa-regular fa-envelope k_icon0"></i></a>
                    </div>
                </div>
                <div class="col-lg-3 col-md-4 col-sm-12">
                    <!-- <div id="k_sticky-section"> -->
                    <div>
                        <div>
                            <div class="tab-container">
                                <div class="k_btn k_incorbtn1 k_text active" data-tab="key1">
                                    <a href="#key1">Understand the Value</a>
                                </div>
                                <div class="k_btn k_incorbtn1 k_text" data-tab="key2">
                                    <a href="#key2">Plan Your Initiative</a>
                                </div>
                                <div class="k_btn k_incorbtn1 k_text" data-tab="key3">
                                    <a href="#key3">Hardware Requirements</a>
                                </div>
                                <div class="k_btn k_incorbtn1 k_text" data-tab="key3">
                                    <a href="#key4">Data Security Options</a>
                                </div>
                                <div class="k_btn k_incorbtn1 k_text" data-tab="key3">
                                    <a href="#key4">Software and Development</a>
                                </div>
                                <div class="k_btn k_incorbtn1 k_text" data-tab="key3">
                                    <a href="#key4">Get Started</a>
                                </div>
                            </div>
                        </div>
                    </div>
                    <div class="k_bggrey p-3">
                        <p><b>Key Takeaways</b></p>
                        <ul>
                            <li>
                                <p>Eighty percent of enterprises will use generative AI by 2026.<sup>1</sup></p>
                            </li>
                            <li>
                                <p>Customization of off-the-shelf foundational models can help you accelerate generative
                                    AI model development.</p>
                            </li>
                            <li>
                                <p>Retrieval-augmented generation (RAG) and model fine-tuning provide two different
                                    pathways to customization.</p>
                            </li>
                            <li>
                                <p>Deploying GenAI models in a cost-effective, scalable fashion requires the right
                                    hardware and software technologies.</p>
                            </li>
                            <li class="k_plink1 k_intext1blue1">
                                <p>You can get hands-on experience with the comprehensive Intel® AI portfolio in the <a
                                        href="#">Intel® Tiber™ Developer Cloud</a>.</p>
                            </li>
                        </ul>
                    </div>
                </div>
                <div class="col-lg-8 col-md-7 col-sm-12 k_20px k_16smpx k_width75">
                    <div class="float-end mx-3 k_smnone">
                        <div>
                            <a href="#" class="fs-4 k_iconauto"><i class="fa-solid fa-print mx-3 k_icon0"></i></a>
                            <a href="#" class="fs-4"> <i class="fa-regular fa-envelope k_icon0"></i></a>
                        </div>
                    </div>
                    <div>
                        <div class="my-5 k_smpaddnone" style="padding-top: 40px !important;">
                            <h4 class="fw-lighter">
                                Scalable and seamless generative AI is a priority for enterprise organizations. However,
                                getting started and determining how to achieve customized generative AI outputs can be
                                daunting. Let’s explore how foundational models provide a streamlined path that can help
                                you get ready for deployment in less time.</h4>
                        </div>
                        <h2 class="k_26px"><strong>Navigate the Generative AI Inflection Point</strong></h2>
                        <p class="k_plink1 k_intext1blue1">Eighty percent of enterprises will use <a href="#">generative
                                AI</a>&nbsp;by 2026,<sup>1</sup> and your organization—like many others—is likely racing
                            to capture value and opportunity using this emerging technology. At the center of any AI
                            initiative is the model itself. Enterprise organizations need to quickly and cost
                            efficiently enable specific AI capabilities that are unique to their business.</p>
                        <p>Today, enterprise organizations rely on two primary methods for enabling customized
                            generative AI capabilities. They can choose to fine-tune a general-purpose foundational
                            model through further training. Or they can implement a technique known as
                            retrieval-augmented generation (RAG) that facilitates customized outputs by connecting
                            foundational models with specific datasets.</p>
                        <h4 class="my-3">Retrieval-Augmented Generation vs. Model Fine-Tuning</h4>
                        <p>RAG and fine-tuning both accelerate the journey to customized AI capabilities, but they do so
                            in different ways.</p>
                        <p>In the fine-tuning method, organizations refine off-the-shelf models using their unique
                            datasets. The foundational model provides a starting point, meaning your team doesn’t need
                            the large amounts of time and data required to build from scratch. The processing demands of
                            fine-tuning are less intense than training from scratch, so you likely won’t need heavy
                            compute (such as a GPU cluster) to fine-tune your chosen foundational model.</p>
                        <p>On the other hand, RAG connects models with relevant data from your unique, proprietary
                            databases to obtain and analyze organization-specific, up-to-the-minute information. This
                            additional context informs the final output and, like fine-tuning, leads to the highly
                            specific results that enterprise organizations need. Critically, the model is not fine-tuned
                            or further trained in the RAG paradigm. Instead, it’s connected to the required knowledge
                            bases through retrieval mechanisms.</p>
                        <p>Both approaches offer distinct advantages. Highly effective RAG-based implementations can be
                            achieved with leaner hardware than fine-tuning. RAG also reduces the risk of hallucinations,
                            can provide sources for its outputs to improve explainability, and offers security benefits
                            since sensitive information can be kept safely in private databases.</p>
                        <p>It’s important to keep in mind that these approaches can also be used together. For more
                            information on RAG, check out these guides:<br>
                            &nbsp;</p>
                        <ul>
                            <li class="k_plink1 k_intext1blue1"><a href="#">What Is RAG?</a>: Learn how RAG works and
                                explore the essential elements of a RAG implementation.</li>
                            <li class="k_plink1 k_intext1blue1"><a href="#">How to Implement RAG</a>: Get step-by-step
                                guidance on how to put the RAG approach to use, including tips for knowledge base
                                creation.</li>
                        </ul>
                        <h2 class="mt-5 k_26px"><strong>Explore Common Foundational Models</strong></h2>
                        <p>Both RAG and model fine-tuning rely on foundational models as central elements. While there
                            are an ever-growing number of off-the-shelf foundational models available to your business,
                            these six represent some of the most powerful and popular offerings in use today:<br>
                            &nbsp;</p>
                        <ul>
                            <li class="k_plink1 k_intext1blue1"><a href="#">Technology Innovation Institute (TII) Falcon
                                    LLM</a></li>
                            <li class="k_plink1 k_intext1blue1"><a href="#">MosaicML MPT</a></li>
                            <li class="k_plink1 k_intext1blue1"><a href="#">Hugging Face BLOOM</a></li>
                            <li class="k_plink1 k_intext1blue1"><a href="#">Stability AI Stable Diffusion</a></li>
                            <li class="k_plink1 k_intext1blue1"><a href="#">Databricks’ Dolly</a></li>
                            <li class="k_plink1 k_intext1blue1"><a href="#">Meta AI Llama 3</a></li>
                            <li class="k_plink1 k_intext1blue1"><a href="#">Mistral AI models</a></li>
                        </ul>
                        <p>&nbsp;<br>
                            By basing your enterprise generative AI solution on these foundational models, you can
                            significantly enhance time to value for your organization’s AI investment.</p>
                        <p class="k_plink1 k_intext1blue1">Of course, choosing a model is a complex process that depends
                            heavily on your needs and business realities. Hands-on experimentation is one of the best
                            ways to get familiar with these off-the-shelf offerings. <a href="#">All six of these models
                                are available for your team to evaluate via the Intel® Tiber™ Developer Cloud</a>.</p>
                        <h2 class="mt-5 k_26px"><strong>Hardware Recommendations</strong></h2>
                        <p>In general, customizing an off-the-shelf model requires less computational power than
                            training a model from the ground up. Depending on your needs, you may be able to run the
                            required workloads via general-purpose hardware that your organization already owns. Or you
                            may opt for specialized AI hardware to handle more-demanding workloads. In the case of RAG,
                            you’ll likely choose between hardware types based on your throughput and latency
                            requirements. Intel offers accelerated AI hardware for the entire range of customization
                            needs:<br>
                            &nbsp;</p>
                        <ul>
                            <li class="k_plink1 k_intext1blue1"><strong>Large-scale, dedicated AI fine-tuning (&lt; 1T
                                    parameters) and high-throughput, low-latency RAG:</strong> <a href="#">Intel® Gaudi®
                                    AI accelerators</a></li>
                            <li class="k_plink1 k_intext1blue1"><strong>General purpose fine-tuning (&lt; 10B
                                    parameters) and general RAG applications:</strong> <a href="#">Intel® Xeon®
                                    processors with built-in AI engines</a></li>
                        </ul>
                        <p>&nbsp;<br>
                            When deploying fine-tuned models, the latest Intel® Xeon® processors and Intel® Gaudi® AI
                            accelerators provide an optimized deployment platform that enables cost-effective inference.
                        </p>
                        <p class="k_plink1 k_intext1blue1">You and your team can test performance across the entire AI
                            pipeline on a range of hardware types via the <a href="#">Intel® Tiber™ Developer Cloud</a>.
                        </p>
                        <h2 class="mt-5 k_26px"><strong>Software Tools</strong></h2>
                        <p>Across both customization approaches, software tools and development resources play an
                            integral role in development and deployment. Without the right tools, you can face lengthy
                            development times and headaches during deployment, especially when dealing with a
                            heterogeneous mix of hardware.</p>
                        <p class="k_plink1 k_intext1blue1">To help solve these challenges, Intel offers an <a
                                href="#">end-to-end development portfolio for AI</a>. Our collection of resources and
                            tools can help you build, scale, and deploy generative AI with optimized results.</p>
                        <p class="k_plink1 k_intext1blue1">For example, <a href="#">our optimized PyTorch
                                library</a>&nbsp;enables you to take advantage of the most-up-to-date Intel® software
                            and hardware optimizations for PyTorch with just a few lines of code.</p>
                        <p class="k_plink1 k_intext1blue1">When pursuing customization through the RAG approach,
                            integrated RAG frameworks such as <a href="#">LangChain</a>, <a href="#">LLamaIndex</a>, and
                            <a href="#">IntelLab’s fastRAG</a> can streamline and accelerate your efforts. RAG
                            frameworks allow you to integrate AI toolchains across the pipeline and provide you with
                            template-based solutions for real-world use cases.</p>
                        <p class="k_plink1 k_intext1blue1">Intel offers optimizations to help maximize overall pipeline
                            performance on Intel® hardware. For example, fastRAG integrates Intel® Extension for PyTorch
                            and <a href="#">Optimum Habana</a>&nbsp;to optimize RAG applications on Intel® Xeon®
                            processors and Intel® Gaudi® AI accelerators.</p>
                        <p class="k_plink1 k_intext1blue1">Meanwhile, <a href="#">OpenVINO™ toolkit</a>&nbsp;plays an
                            integral role in deployment. It’s an open source toolkit that accelerates AI inference with
                            lower latency and higher throughput while maintaining accuracy, reducing model footprint,
                            and optimizing hardware use. The toolkit streamlines AI development and integration of deep
                            learning in generative AI—as well as computer vision and large language models.</p>
                        <p class="k_plink1 k_intext1blue1">For RAG applications, we provide several optimization
                            libraries to help you maximize LLM inference on your hardware resources. Our <a
                                href="#">Intel® oneAPI libraries</a>&nbsp;provide low-level optimizations for popular AI
                            frameworks, including PyTorch and TensorFlow, enabling you to use familiar open source tools
                            that are optimized on Intel® hardware.</p>
                        <p class="k_plink1 k_intext1blue1">You can try the Intel® software resources highlighted in this
                            article—and many others—via the <a href="#">Intel® Tiber™ Developer Cloud</a>.<br>
                            You can also consult our <a href="#">generative AI development page</a>&nbsp;for a curated
                            collection of enablement resources for your generative AI projects.</p>
                        <h2 class="mt-5"><strong>Chart a Simpler Course to Enterprise AI</strong></h2>
                        <p>As you progress your generative AI initiative from model customization and proof of concept
                            to deployment, you can optimize efficiency and accelerate innovation with tools and
                            technologies from Intel and our ecosystem of global partners.</p>
                        <p>By choosing Intel for your AI platform, you can maximize the value of the infrastructure you
                            already have while ensuring the openness and interoperability you’ll need to sustain success
                            in the future. Our investments in reliability and manageability help deliver smoother,
                            simpler AI operations across the pipeline. Our open platforms and high-performance, low-TCO
                            hardware allow for the flexible, efficient deployments you need to enable generative AI at
                            scale.</p>
                        <p>As part of the Linux Foundation Open Platform for Enterprise AI, we’re working to develop an
                            ecosystem orchestration framework that efficiently integrates generative AI technologies and
                            workflows, enables quicker adoption, and enhances business value through collaborative
                            development. Our current contributions include a set of generative AI architectures that can
                            help expedite your initiative:<br>
                            &nbsp;</p>
                        <ul>
                            <li class="k_plink1 k_intext1blue1">A <a href="#">chatbot</a>&nbsp;on Intel® Xeon Scalable
                                processors and Intel® Gaudi® AI Accelerators</li>
                            <li class="k_plink1 k_intext1blue1"><a href="#">Document summarization</a>&nbsp;using Intel®
                                Gaudi® AI Accelerators</li>
                            <li class="k_plink1 k_intext1blue1"><a href="#">Visual Question Answering</a>&nbsp;(VQA) on
                                Intel® Gaudi® AI Accelerators</li>
                            <li class="k_plink1 k_intext1blue1">A <a href="#">copilot</a>&nbsp;designed for code
                                generation in Visual Studio Code on Intel® Gaudi® AI Accelerators</li>
                        </ul>
                        <h2 class="mt-5 k_26px"><strong>Start Customizing Generative AI Models on Intel Today</strong></h2>
                        <h4><strong>Harness the Power of Generative AI</strong></h4>
                        <p class="k_plink1 k_intext1blue1">Get additional hardware and software recommendations to
                            support generative AI across the pipeline.<br>
                            <a href="#">Read now</a>
                        </p>
                        <h4><strong>Intel® AI Solutions</strong></h4>
                        <p class="k_plink1 k_intext1blue1">Explore our comprehensive AI portfolio, including solutions
                            for edge, data center, cloud, AI PCs, and end-to-end software tools.<br>
                            <a href="#">View now</a>
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </section>
    <section>
        <div class="k_container11">
            <hr class="mt-5">
        </div>
    </section>
    <section class="k_space">
        <div class="k_container11">
            <div class="row ">
                <p>Product and Performance Information</p>
                <div class="k_plink1 k_intext1blue1"><sup>1</sup>“<a href="#">Gartner Says More than 80% of Enterprises
                        Will Have Used Generative AI APIs or Deployed Generative AI-Enabled Applications by 2026,</a>”
                    Gartner, 11 Oct. 2023.</div>
            </div>
        </div>
    </section>
    <div id="footer"></div>
    <script>
        // navbar include  
        fetch('../y_index/y_navbar.html')
            .then(response => response.text())
            .then(data => {
                document.getElementById('navbar').innerHTML = data;
            });
        // footer include 
        fetch('../y_index/y_footer.html')
            .then(response => response.text())
            .then(data => {
                document.getElementById('footer').innerHTML = data;
            });
    </script>
    <script src="https://cdn.jsdelivr.net/npm/@splidejs/splide@4.1.4/dist/js/splide.min.js"></script>
    <script src="../js/jquery-3.7.1.js"></script>
    <!-- <script src="js/jquery-3.6.4.min.js"></script> -->
    <script src="../js/owl.carousel.min.js"></script>
    <script src="../js/rushita.js"></script>
    <script src="../js/monika.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/js/bootstrap.bundle.min.js"
        integrity="sha384-MrcW6ZMFYlzcLA8Nl+NtUVF0sA7MsXsP1UyJoMp4YLEuNSfAP+JcXn/tWtIaxVXM"
        crossorigin="anonymous"></script>



</body>

</html>